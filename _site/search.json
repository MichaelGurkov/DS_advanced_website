[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition\nSTL decomposition\n\nMissing data imputation"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition\nSTL decomposition\n\nMissing data imputation"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html",
    "href": "topics/ts_decomposition/class_decomp.html",
    "title": "Classical Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#trend",
    "href": "topics/ts_decomposition/class_decomp.html#trend",
    "title": "Classical Decomposition",
    "section": "Trend",
    "text": "Trend\n++ we need to decide on the window size of the moving average. It’s a good rule of thumb to set the window size to the frequency of the seasonality (i.e 12 in case of monthly data with yearly seasonality) if the seasonality is known. This allows as to “isolate” the entire seasonality cycle in one window and thus to smooth over the seasonality. If the seasonality is not know we need to visually evaluate the resulting trend. Setting the window too narrow will result in excess fluctuations of the trend line - under smoothing. Setting the window too wide will result in a flat line that will not capture the changes in the trend\n\n\ntrend_df = raw_df.copy()\n\neven_win_len = 84\n\ntrend_df[\"over_smoothing\"] = trend_df[\"sales\"].rolling(window = even_win_len).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].rolling(window = 2, center = True).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].shift(- even_win_len // 2)\n\ntrend_df[\"under_smoothing\"] = trend_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\ntrend_df[\"sales\"].plot(color = \"grey\", alpha = 0.5)\n\ntrend_df[\"under_smoothing\"].plot(color = \"steelblue\")\n\ntrend_df[\"over_smoothing\"].plot(color = \"orange\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCross validation for trend estimation"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#seasonality",
    "href": "topics/ts_decomposition/class_decomp.html#seasonality",
    "title": "Classical Decomposition",
    "section": "Seasonality",
    "text": "Seasonality\n++ explain that in order to isolate seasonality we first need to detrend the data. So we indentify the trend, than detrend (exclude the trend) by substracting (if additive) of dividing (if multiplicative) and proceed to isolate the seasonality\n\n\nseason_df = raw_df.copy()\n\nseason_df[\"trend\"] = season_df[\"sales\"].rolling(window = 12).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].rolling(window = 2, center = True).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].shift(- 12 // 2)\n\nseason_df[\"detrended_data\"] = season_df[\"sales\"] - season_df[\"trend\"]\n\n\nseason_df[\"month\"] = season_df.index.month\n\nseasonality = season_df.groupby(\"month\").mean()[\"detrended_data\"].reset_index()\n\nseasonality.columns = [\"month\",\"seasonality\"]\n\nseason_df = pd.merge(season_df.copy(),seasonality, on = \"month\", how = \"left\")\n\nseason_df[\"remainder\"] = season_df[\"detrended_data\"] - season_df[\"seasonality\"]\n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\nseason_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\nseason_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\nseason_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html",
    "href": "topics/ts_decomposition/transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nraw_df[\"passengers\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#log-transformation",
    "href": "topics/ts_decomposition/transformations.html#log-transformation",
    "title": "Transformations",
    "section": "Log transformation",
    "text": "Log transformation\n++ explain that sometimes we want to transform a features, for example in order to stabilize the variance\n\nraw_df[\"passengers_log\"] = np.log(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_log\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "href": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "title": "Transformations",
    "section": "Box Cox transformation",
    "text": "Box Cox transformation\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nlambdas_vec = [-1,-0.5,0,0.5,1,2]\n\nplt.clf()\n\nfig,ax = plt.subplots(ncols = 2, nrows = 3,figsize=[25, 15], sharex = True)\n\nax = ax.flatten()\n\nfor ix, temp_lambda in enumerate(lambdas_vec):\n  print(temp_lambda)\n  \n  bc_trans = BoxCoxTransformer(lambda_fixed = temp_lambda)\n  \n  raw_df[\"temp_box_cox\"] = bc_trans.fit_transform(raw_df[\"passengers\"])\n  \n  raw_df.plot(y = \"temp_box_cox\",ax  = ax[ix], label = f\"lambda = {temp_lambda}\")\n  \n  ax[ix].legend()\n  \n  ax[ix].set_xlabel(\"\")\n  \n\nplt.tight_layout()\n  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Guerrero methond\n++ explain briefly the Guerrero method. Emphasize that it’s advantage is the explicit focus on stabilization of variance.\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nbc_guerrero = BoxCoxTransformer(method = \"guerrero\", sp = 12)\n\nraw_df[\"passengers_bc_guerrero\"] = bc_guerrero.fit_transform(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_bc_guerrero\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#moving-averages",
    "href": "topics/ts_decomposition/transformations.html#moving-averages",
    "title": "Transformations",
    "section": "Moving averages",
    "text": "Moving averages\n++ explain the odd moving average window is symmetric (there are equal amount of point on each side of the center). The even size moving average does not have a natural center but we can change the weights to achieve a symmetric window. The weights will not be the same, on the edges the weights will be smaller. Actually in order to make an even size window symmetric we need to apply additional moving average of window 2 so the edge’s weights will be half the other (inside) weights.\n\n\nma_file_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nma_raw_df = pd.read_csv(ma_file_path,index_col = \"date\")\n\nma_raw_df.index = pd.to_datetime(ma_raw_df.index)\n\ndel ma_file_path\n\n\n\nma_df = ma_raw_df.copy()\n\nma_df[\"ma_3\"] = ma_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\nma_df[\"ma_3\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\neven_window_size = 12\n\nma_df[\"ma_2_12\"] = ma_df[\"sales\"].rolling(window = even_window_size).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].rolling(window = 2, center = True).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].shift(- even_window_size // 2)\n\n\nplt.clf()\n\nma_df[\"ma_2_12\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne7329f12e3f9c1d7266bdc4614b17630418a7520"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html",
    "href": "topics/ts_decomposition/lowess_decomp.html",
    "title": "LOWESS Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.model_selection import KFold\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "href": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "title": "LOWESS Decomposition",
    "section": "LOWESS parametres",
    "text": "LOWESS parametres\n++ explain the parameters in python implementations: frac - fraction of data for the window size (determines the smoothness) it - number of iterations for robust regression. Also explain the endog and exog parameters\n\n\ny = raw_df[\"sales\"]\n\nx = np.arange(0, len(y))\n\nts_decomp = lowess(endog = y, exog = x, frac = 0.1, it = 3)\n\nraw_df[\"trend_lowess\"] = ts_decomp[:,1]\n\n\nCross validation to select appropriate fraction parameter\n\ndef get_rmse_for_df(X,y, frac_param):\n  KFold_obj = KFold(n_splits = 5, shuffle = True, random_state = 0)\n  rmse_list = []\n  for train_index, test_index in KFold_obj.split(X,y):\n    X_train = X[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X[test_index]\n    y_test = y.iloc[test_index]\n    y_pred = lowess(endog = y_train, exog = X_train, frac = frac_param, xvals = X_test)\n    rmse = root_mean_squared_error(y_pred, y_test)\n    rmse_list.append(rmse)\n  return rmse_list\n\n\nresults = []\n\nfor temp_frac in [0.05,0.1,0.5,1]:\n  rmse_list = get_rmse_for_df(X = x,y = y,frac_param = temp_frac)\n  rmse_df = pd.DataFrame(data = rmse_list, columns = [\"rmse\"])\n  rmse_df[\"frac\"] = temp_frac\n  results.append(rmse_df)\n\n  \npd.concat(results, axis = 0)\n\n           rmse  frac\n0  22981.706955  0.05\n1  22624.297346  0.05\n2  20582.387871  0.05\n3  28572.985707  0.05\n4  21376.838789  0.05\n0  23018.284152  0.10\n1  21215.656841  0.10\n2  20279.923647  0.10\n3  25214.068880  0.10\n4  20387.775112  0.10\n0  26184.512853  0.50\n1  22354.596572  0.50\n2  23964.726098  0.50\n3  25516.588727  0.50\n4  20683.629095  0.50\n0  26343.368462  1.00\n1  23258.024086  1.00\n2  24924.462787  1.00\n3  26254.158157  1.00\n4  20870.150531  1.00"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html",
    "href": "topics/ts_decomposition/stl_decomp.html",
    "title": "STL Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#trend",
    "href": "topics/ts_decomposition/stl_decomp.html#trend",
    "title": "STL Decomposition",
    "section": "Trend",
    "text": "Trend\n++ explain the parameters of the STL implementation in statsmodels.tsa.seasonal explicitly address the seasonal, period and robust parameters. Other parameters can be left alone - the default values are typically good enough.\n\n\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid\n                 \n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\ndecompostion_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\ndecompostion_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\ndecompostion_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html",
    "href": "topics/na_imputation/na_imputation.html",
    "title": "Missing data imputation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_missing.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\nna_idx = raw_df.isnull()\n\n# url = \"https://raw.githubusercontent.com/facebook/prophet/master/examples/example_retail_sales.csv\"\n# df = pd.read_csv(url)\n# \n# df = df.iloc[0:160].copy()\n# \n# df.columns = [\"date\",\"sales\"]\n# \n# df = df.set_index(\"date\").copy()\n# \n# \n# # Insert missing data into dataframe\n# df.iloc[10:11] = np.NaN\n# df.iloc[25:28] = np.NaN\n# df.iloc[40:45] = np.NaN\n# df.iloc[70:94] = np.NaN\n# \n# \n# df.to_csv(file_path)\nprint(f\"There are {raw_df['sales'].isnull().sum()} missing values, these are {np.round(raw_df['sales'].isnull().sum() / len(raw_df)* 100,3)} percent of the data\")\n\nThere are 33 missing values, these are 20.625 percent of the data\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#forward-fill",
    "href": "topics/na_imputation/na_imputation.html#forward-fill",
    "title": "Missing data imputation",
    "section": "Forward fill",
    "text": "Forward fill\n\nffill_df = raw_df.ffill()\n\nplt.clf()\n\nax = ffill_df.plot(linestyle=\"-\", marker=\".\")\n\nffill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#backward-fill",
    "href": "topics/na_imputation/na_imputation.html#backward-fill",
    "title": "Missing data imputation",
    "section": "Backward fill",
    "text": "Backward fill\n++ explain that backward filling can introduce “data leakage” because we are carrying to the past information from the future.\n\nbfill_df = raw_df.bfill()\n\nplt.clf()\n\nax = bfill_df.plot(linestyle=\"-\", marker=\".\")\n\nbfill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "href": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "title": "Missing data imputation",
    "section": "Linear interpolation",
    "text": "Linear interpolation\n\nlin_inter = raw_df.interpolate(method = \"time\")\n\nplt.clf()\n\nax = lin_inter.plot(linestyle=\"-\", marker=\".\")\n\nlin_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "href": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "title": "Missing data imputation",
    "section": "Spline interpolation",
    "text": "Spline interpolation\n\nspline_inter = raw_df.interpolate(method = \"spline\", order = 3)\n\nplt.clf()\n\nax = spline_inter.plot(linestyle=\"-\", marker=\".\")\n\nspline_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html",
    "href": "topics/outliers/outliers.html",
    "title": "Missing data imputation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_outliers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#de-seasonalise-data",
    "href": "topics/outliers/outliers.html#de-seasonalise-data",
    "title": "Missing data imputation",
    "section": "De seasonalise data",
    "text": "De seasonalise data\n++ explain that we are deseasonalizing the data in order to prevent disturbance for outlier identification\n\n\nstl_decomp = STL(raw_df[\"sales\"], robust = True).fit()\n\nseasonal_component = stl_decomp.seasonal\n\nraw_df[\"sales_deseasonalised\"] = raw_df[\"sales\"] - seasonal_component\n\n\nplt.clf()\n\nraw_df[\"sales_deseasonalised\"].plot(marker = \".\")\n\nplt.title(\"Deseasonlalized data with outliers\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#identify-outliers",
    "href": "topics/outliers/outliers.html#identify-outliers",
    "title": "Missing data imputation",
    "section": "Identify outliers",
    "text": "Identify outliers\n\nRolling mean and standard deviation\n\n\nraw_df[[\"roll_mean\",\"roll_std\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_mean\":\"mean\", \"roll_std\":\"std\"})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper\"] = raw_df[\"roll_mean\"] + margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"lower\"] = raw_df[\"roll_mean\"] - margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"is_outlier\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRolling median and median absolute deviation\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\nraw_df[[\"roll_median\",\"roll_mad\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_median\":\"median\", \"roll_mad\":mad})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper_2\"] = raw_df[\"roll_median\"] + margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"lower_2\"] = raw_df[\"roll_median\"] - margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"is_outlier_2\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower_2\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper_2\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df[\"lower_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#impute-outliers",
    "href": "topics/outliers/outliers.html#impute-outliers",
    "title": "Missing data imputation",
    "section": "Impute outliers",
    "text": "Impute outliers\n\n\nraw_df[\"sales_na\"] = raw_df[\"sales_deseasonalised\"]\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_na\"] = np.nan\n\nraw_df[\"sales_imputed\"] = raw_df[\"sales_na\"].interpolate(method = \"time\",\n                                                         inlace = True)\n\n\nplt.clf()\n\nax = raw_df[\"sales_imputed\"].plot()\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_imputed\"].plot(ax = ax,\n                                                      alpha = 0.5,\n                                                      color = \"red\",\n                                                      marker = \"o\",\n                                                      linestyle = \"none\")\n\nplt.title(\"Imputed outliers\")\n\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "href": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "title": "Missing data imputation",
    "section": "Seasonal decomposition and interpolation",
    "text": "Seasonal decomposition and interpolation\n++ explain that we first use linear interpolation because STL can not handle missing data\n\n\nstl_inter = STL(raw_df.interpolate(method = \"time\"), seasonal = 31).fit()\n\nseasonal_component = stl_inter.seasonal\n\ndeaseasonlised_df = raw_df[\"sales\"] - seasonal_component\n\ndf_inter = deaseasonlised_df.interpolate(method = \"time\")\n\ndf_final = df_inter + seasonal_component\n\ndf_final = df_final.to_frame().rename(columns = {0:\"sales\"})\n\n\nplt.clf()\n\nax = df_final.plot(linestyle=\"-\", marker=\".\")\n\ndf_final[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/auto_correlation.html",
    "href": "topics/lagged_features/auto_correlation.html",
    "title": "Autocorrelation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.stattools import acf, pacf\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom statsmodels.tsa.seasonal import STL\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\n\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid\n\n\nTrend\n++ explain that we expect to see a high autocorrelation with a high inertia in case of trend presence.\n\nplot_acf(decompostion_df[\"trend\"], title = \"Trend Autocorrelation\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplot_pacf(decompostion_df[\"trend\"], title = \"Trend Partial Autocorrelation\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeasonality\n++ explain that we expect to see high autocorrelation at a corresponding lags if there is seasonality present\n\nplot_acf(decompostion_df[\"seasonality\"], title = \"Seasonality Autocorrelation\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplot_pacf(decompostion_df[\"seasonality\"], title = \"Seasonality Partial Autocorrelation\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRemainder (“white noise”)\n++ explain that we expect to see no autocorrelation in a white noise series. If the decomposition is good enough than the remainder should resemble white noise\n\nplot_acf(decompostion_df[\"remainder\"], title = 'Remainder (\"white noise\") Autocorrelation')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplot_pacf(decompostion_df[\"remainder\"],\ntitle = 'Remainder (\"white noise\") Partial Autocorrelation')\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/lag_plots.html",
    "href": "topics/lagged_features/lag_plots.html",
    "title": "Autoregressive processes",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nAR 1\n\n\nnum_periods = 1000\n\nar_coeff = 0.9\n\nconst_term = 0\n\ntime_index = pd.date_range(start = \"2000-01-01\", periods = num_periods,\n                           freq = \"d\")\n                           \nar1_series = np.zeros(num_periods)\n\nwhite_noise_series = np.zeros(num_periods)\n\ntrend_series = np.zeros(num_periods)\n\n\nfor t in range(1,num_periods):\n  noise = np.random.normal()\n  white_noise_series[t] = noise\n  ar1_series[t] = const_term + ar_coeff * ar1_series[t - 1] + noise\n  trend_series[t] = t + noise\n\nts_df = pd.DataFrame(data = list(zip(white_noise_series,ar1_series,trend_series)),\nindex = time_index, columns = [\"white_noise\",\"ar_1\",\"trend\"])\n\ndel white_noise_series, ar1_series, trend_series\n\ndel t, time_index, ar_coeff, const_term, noise, num_periods\n\n\n\nPandas implemenation\n\n\n\ndef plot_lags_pandas (temp_df, plot_title, ncols = 3, nrows = 3):\n\n  fig, axes = plt.subplots(ncols,nrows)\n  \n  for temp_ind, temp_ax in enumerate(axes.flatten()):\n    pd.plotting.lag_plot(temp_df, lag = temp_ind + 1, ax = temp_ax,\n                         marker = \".\", linestyle='None')\n    temp_ax.set_title(f\"Lag_{temp_ind}\")\n    \n \n  fig.suptitle(plot_title, fontsize=16)\n  \n  plt.tight_layout()\n  \n  plt.subplots_adjust(hspace=1, top=0.8)\n  \n  plt.show()\n\n\nplot_lags_pandas(ts_df[[\"white_noise\"]], plot_title = \"White noise\")\n\n\n\n\n\n\n\n\n\nplot_lags_pandas(ts_df[[\"trend\"]], plot_title = \"Trend\")\n\n\n\n\n\n\n\n\n\nplot_lags_pandas(raw_df.copy(), \"Retail sales\", nrows = 4, ncols = 3)\n\n\n\n\n\n\n\n\n\n\nManual\n\n\n\ndef plot_lags(temp_df, plot_title):\n\n  fig, axes = plt.subplots(3,3)\n  \n  for temp_ind, temp_ax in enumerate(axes.flatten()):\n    temp_ax.scatter(x = temp_df.iloc[:,0],\n                    y = temp_df.iloc[:,0].shift(temp_ind), marker = \".\")\n    temp_ax.set_title(f\"Lag_{temp_ind}\")\n  \n  fig.suptitle(plot_title, fontsize=16)\n  \n  plt.tight_layout()\n  \n  plt.subplots_adjust(hspace=1, top=0.8)\n  \n  plt.show()\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"white_noise\"]].copy(), \"White noise\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"ar_1\"]].copy(), \"Ar 1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"trend\"]].copy(), \"Trend\")"
  },
  {
    "objectID": "topics/lagged_features/ar_process.html",
    "href": "topics/lagged_features/ar_process.html",
    "title": "Autoregressive processes",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\nAR 1\n\n\nnum_periods = 1000\n\nar_coeff = 0.9\n\nconst_term = 0\n\ntime_index = pd.date_range(start = \"2000-01-01\", periods = num_periods,\n                           freq = \"d\")\n                           \ntime_series = np.zeros(num_periods)\n\nfor t in range(1,num_periods):\n  noise = np.random.normal()\n  time_series[t] = const_term + ar_coeff * time_series[t - 1] + noise\n\nar1_df = pd.DataFrame(data = time_series, index = time_index)\n\n\nplt.clf()\n\nar1_df.plot(legend = False)\n\nplt.title(\"AR 1 process\")\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/lagged_features.html",
    "href": "topics/lagged_features/lagged_features.html",
    "title": "Lagged features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom feature_engine.timeseries.forecasting import LagFeatures\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/lagged_features/lagged_features.html#lagged-features",
    "href": "topics/lagged_features/lagged_features.html#lagged-features",
    "title": "Lagged features",
    "section": "Lagged features",
    "text": "Lagged features\n\npandas implementation\n\n\nlag_df = raw_df.copy()\n\nfor temp_lag in [1,2,12]:\n  lag_df[f\"lag_{temp_lag}\"] = lag_df[\"sales\"].shift(freq = f\"{temp_lag}MS\")\n  \n\n\nplt.clf()\nlag_df.plot(alpha = 0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature_engine implementation\n\nlag_trans = LagFeatures(variables = [\"sales\"], freq = [\"1MS\",\"2MS\",\"12MS\"])\n\nlag_df_fe = lag_trans.fit_transform(raw_df.copy()) \n\nprint(lag_df_fe.head())\n\n             sales  sales_lag_1MS  sales_lag_2MS  sales_lag_12MS\ndate                                                            \n1992-01-01  146376            NaN            NaN             NaN\n1992-02-01  147079       146376.0            NaN             NaN\n1992-03-01  159336       147079.0       146376.0             NaN\n1992-04-01  163669       159336.0       147079.0             NaN\n1992-05-01  170068       163669.0       159336.0             NaN"
  }
]