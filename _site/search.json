[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html",
    "href": "topics/ts_decomposition/class_decomp.html",
    "title": "Classical Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#trend",
    "href": "topics/ts_decomposition/class_decomp.html#trend",
    "title": "Classical Decomposition",
    "section": "Trend",
    "text": "Trend\n++ we need to decide on the window size of the moving average. It’s a good rule of thumb to set the window size to the frequency of the seasonality (i.e 12 in case of monthly data with yearly seasonality) if the seasonality is known. This allows as to “isolate” the entire seasonality cycle in one window and thus to smooth over the seasonality. If the seasonality is not know we need to visually evaluate the resulting trend. Setting the window too narrow will result in excess fluctuations of the trend line - under smoothing. Setting the window too wide will result in a flat line that will not capture the changes in the trend\n\n\ntrend_df = raw_df.copy()\n\neven_win_len = 84\n\ntrend_df[\"over_smoothing\"] = trend_df[\"sales\"].rolling(window = even_win_len).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].rolling(window = 2, center = True).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].shift(- even_win_len // 2)\n\ntrend_df[\"under_smoothing\"] = trend_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\ntrend_df[\"sales\"].plot(color = \"grey\", alpha = 0.5)\n\ntrend_df[\"under_smoothing\"].plot(color = \"steelblue\")\n\ntrend_df[\"over_smoothing\"].plot(color = \"orange\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCross validation for trend estimation"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#seasonality",
    "href": "topics/ts_decomposition/class_decomp.html#seasonality",
    "title": "Classical Decomposition",
    "section": "Seasonality",
    "text": "Seasonality\n++ explain that in order to isolate seasonality we first need to detrend the data. So we indentify the trend, than detrend (exclude the trend) by substracting (if additive) of dividing (if multiplicative) and proceed to isolate the seasonality\n\n\nseason_df = raw_df.copy()\n\nseason_df[\"trend\"] = season_df[\"sales\"].rolling(window = 12).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].rolling(window = 2, center = True).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].shift(- 12 // 2)\n\nseason_df[\"detrended_data\"] = season_df[\"sales\"] - season_df[\"trend\"]\n\n\nseason_df[\"month\"] = season_df.index.month\n\nseasonality = season_df.groupby(\"month\").mean()[\"detrended_data\"].reset_index()\n\nseasonality.columns = [\"month\",\"seasonality\"]\n\nseason_df = pd.merge(season_df.copy(),seasonality, on = \"month\", how = \"left\")\n\nseason_df[\"remainder\"] = season_df[\"detrended_data\"] - season_df[\"seasonality\"]\n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\nseason_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\nseason_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\nseason_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html",
    "href": "topics/ts_decomposition/transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nraw_df[\"passengers\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#log-transformation",
    "href": "topics/ts_decomposition/transformations.html#log-transformation",
    "title": "Transformations",
    "section": "Log transformation",
    "text": "Log transformation\n++ explain that sometimes we want to transform a features, for example in order to stabilize the variance\n\nraw_df[\"passengers_log\"] = np.log(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_log\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "href": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "title": "Transformations",
    "section": "Box Cox transformation",
    "text": "Box Cox transformation\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nlambdas_vec = [-1,-0.5,0,0.5,1,2]\n\nplt.clf()\n\nfig,ax = plt.subplots(ncols = 2, nrows = 3,figsize=[25, 15], sharex = True)\n\nax = ax.flatten()\n\nfor ix, temp_lambda in enumerate(lambdas_vec):\n  print(temp_lambda)\n  \n  bc_trans = BoxCoxTransformer(lambda_fixed = temp_lambda)\n  \n  raw_df[\"temp_box_cox\"] = bc_trans.fit_transform(raw_df[\"passengers\"])\n  \n  raw_df.plot(y = \"temp_box_cox\",ax  = ax[ix], label = f\"lambda = {temp_lambda}\")\n  \n  ax[ix].legend()\n  \n  ax[ix].set_xlabel(\"\")\n  \n\nplt.tight_layout()\n  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Guerrero methond\n++ explain briefly the Guerrero method. Emphasize that it’s advantage is the explicit focus on stabilization of variance.\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nbc_guerrero = BoxCoxTransformer(method = \"guerrero\", sp = 12)\n\nraw_df[\"passengers_bc_guerrero\"] = bc_guerrero.fit_transform(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_bc_guerrero\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#moving-averages",
    "href": "topics/ts_decomposition/transformations.html#moving-averages",
    "title": "Transformations",
    "section": "Moving averages",
    "text": "Moving averages\n++ explain the odd moving average window is symmetric (there are equal amount of point on each side of the center). The even size moving average does not have a natural center but we can change the weights to achieve a symmetric window. The weights will not be the same, on the edges the weights will be smaller. Actually in order to make an even size window symmetric we need to apply additional moving average of window 2 so the edge’s weights will be half the other (inside) weights.\n\n\nma_file_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nma_raw_df = pd.read_csv(ma_file_path,index_col = \"date\")\n\nma_raw_df.index = pd.to_datetime(ma_raw_df.index)\n\ndel ma_file_path\n\n\n\nma_df = ma_raw_df.copy()\n\nma_df[\"ma_3\"] = ma_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\nma_df[\"ma_3\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\neven_window_size = 12\n\nma_df[\"ma_2_12\"] = ma_df[\"sales\"].rolling(window = even_window_size).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].rolling(window = 2, center = True).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].shift(- even_window_size // 2)\n\n\nplt.clf()\n\nma_df[\"ma_2_12\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne7329f12e3f9c1d7266bdc4614b17630418a7520"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html",
    "href": "topics/ts_decomposition/lowess_decomp.html",
    "title": "LOWESS Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.model_selection import KFold\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "href": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "title": "LOWESS Decomposition",
    "section": "LOWESS parametres",
    "text": "LOWESS parametres\n++ explain the parameters in python implementations: frac - fraction of data for the window size (determines the smoothness) it - number of iterations for robust regression. Also explain the endog and exog parameters\n\n\ny = raw_df[\"sales\"]\n\nx = np.arange(0, len(y))\n\nts_decomp = lowess(endog = y, exog = x, frac = 0.1, it = 3)\n\nraw_df[\"trend_lowess\"] = ts_decomp[:,1]\n\n\nCross validation to select appropriate fraction parameter\n\ndef get_rmse_for_df(X,y, frac_param):\n  KFold_obj = KFold(n_splits = 5, shuffle = True, random_state = 0)\n  rmse_list = []\n  for train_index, test_index in KFold_obj.split(X,y):\n    X_train = X[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X[test_index]\n    y_test = y.iloc[test_index]\n    y_pred = lowess(endog = y_train, exog = X_train, frac = frac_param, xvals = X_test)\n    rmse = root_mean_squared_error(y_pred, y_test)\n    rmse_list.append(rmse)\n  return rmse_list\n\n\nresults = []\n\nfor temp_frac in [0.05,0.1,0.5,1]:\n  rmse_list = get_rmse_for_df(X = x,y = y,frac_param = temp_frac)\n  rmse_df = pd.DataFrame(data = rmse_list, columns = [\"rmse\"])\n  rmse_df[\"frac\"] = temp_frac\n  results.append(rmse_df)\n\n  \npd.concat(results, axis = 0)\n\n           rmse  frac\n0  22981.706955  0.05\n1  22624.297346  0.05\n2  20582.387871  0.05\n3  28572.985707  0.05\n4  21376.838789  0.05\n0  23018.284152  0.10\n1  21215.656841  0.10\n2  20279.923647  0.10\n3  25214.068880  0.10\n4  20387.775112  0.10\n0  26184.512853  0.50\n1  22354.596572  0.50\n2  23964.726098  0.50\n3  25516.588727  0.50\n4  20683.629095  0.50\n0  26343.368462  1.00\n1  23258.024086  1.00\n2  24924.462787  1.00\n3  26254.158157  1.00\n4  20870.150531  1.00"
  }
]