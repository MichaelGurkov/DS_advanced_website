[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting small\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition\nSTL decomposition\n\nMissing data imputation\n\nMissing data imputation\n\nOutliers\n\nOutliers\n\nLagged features\n\nlagged_features\nlag_plots\nauto_correlation\nar_process\ndistributed_lag_features\n\nWindow features\n\nwindow_features"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting small\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition\nSTL decomposition\n\nMissing data imputation\n\nMissing data imputation\n\nOutliers\n\nOutliers\n\nLagged features\n\nlagged_features\nlag_plots\nauto_correlation\nar_process\ndistributed_lag_features\n\nWindow features\n\nwindow_features"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html",
    "href": "topics/ts_decomposition/class_decomp.html",
    "title": "Classical Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#trend",
    "href": "topics/ts_decomposition/class_decomp.html#trend",
    "title": "Classical Decomposition",
    "section": "Trend",
    "text": "Trend\n++ we need to decide on the window size of the moving average. It’s a good rule of thumb to set the window size to the frequency of the seasonality (i.e 12 in case of monthly data with yearly seasonality) if the seasonality is known. This allows as to “isolate” the entire seasonality cycle in one window and thus to smooth over the seasonality. If the seasonality is not know we need to visually evaluate the resulting trend. Setting the window too narrow will result in excess fluctuations of the trend line - under smoothing. Setting the window too wide will result in a flat line that will not capture the changes in the trend\n\n\ntrend_df = raw_df.copy()\n\neven_win_len = 84\n\ntrend_df[\"over_smoothing\"] = trend_df[\"sales\"].rolling(window = even_win_len).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].rolling(window = 2, center = True).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].shift(- even_win_len // 2)\n\ntrend_df[\"under_smoothing\"] = trend_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\ntrend_df[\"sales\"].plot(color = \"grey\", alpha = 0.5)\n\ntrend_df[\"under_smoothing\"].plot(color = \"steelblue\")\n\ntrend_df[\"over_smoothing\"].plot(color = \"orange\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCross validation for trend estimation"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#seasonality",
    "href": "topics/ts_decomposition/class_decomp.html#seasonality",
    "title": "Classical Decomposition",
    "section": "Seasonality",
    "text": "Seasonality\n++ explain that in order to isolate seasonality we first need to detrend the data. So we indentify the trend, than detrend (exclude the trend) by substracting (if additive) of dividing (if multiplicative) and proceed to isolate the seasonality\n\n\nseason_df = raw_df.copy()\n\nseason_df[\"trend\"] = season_df[\"sales\"].rolling(window = 12).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].rolling(window = 2, center = True).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].shift(- 12 // 2)\n\nseason_df[\"detrended_data\"] = season_df[\"sales\"] - season_df[\"trend\"]\n\n\nseason_df[\"month\"] = season_df.index.month\n\nseasonality = season_df.groupby(\"month\").mean()[\"detrended_data\"].reset_index()\n\nseasonality.columns = [\"month\",\"seasonality\"]\n\nseason_df = pd.merge(season_df.copy(),seasonality, on = \"month\", how = \"left\")\n\nseason_df[\"remainder\"] = season_df[\"detrended_data\"] - season_df[\"seasonality\"]\n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\nseason_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\nseason_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\nseason_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html",
    "href": "topics/ts_decomposition/transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nraw_df[\"passengers\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#log-transformation",
    "href": "topics/ts_decomposition/transformations.html#log-transformation",
    "title": "Transformations",
    "section": "Log transformation",
    "text": "Log transformation\n++ explain that sometimes we want to transform a features, for example in order to stabilize the variance\n\nraw_df[\"passengers_log\"] = np.log(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_log\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "href": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "title": "Transformations",
    "section": "Box Cox transformation",
    "text": "Box Cox transformation\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nlambdas_vec = [-1,-0.5,0,0.5,1,2]\n\nplt.clf()\n\nfig,ax = plt.subplots(ncols = 2, nrows = 3,figsize=[25, 15], sharex = True)\n\nax = ax.flatten()\n\nfor ix, temp_lambda in enumerate(lambdas_vec):\n  print(temp_lambda)\n  \n  bc_trans = BoxCoxTransformer(lambda_fixed = temp_lambda)\n  \n  raw_df[\"temp_box_cox\"] = bc_trans.fit_transform(raw_df[\"passengers\"])\n  \n  raw_df.plot(y = \"temp_box_cox\",ax  = ax[ix], label = f\"lambda = {temp_lambda}\")\n  \n  ax[ix].legend()\n  \n  ax[ix].set_xlabel(\"\")\n  \n\nplt.tight_layout()\n  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Guerrero methond\n++ explain briefly the Guerrero method. Emphasize that it’s advantage is the explicit focus on stabilization of variance.\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nbc_guerrero = BoxCoxTransformer(method = \"guerrero\", sp = 12)\n\nraw_df[\"passengers_bc_guerrero\"] = bc_guerrero.fit_transform(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_bc_guerrero\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#moving-averages",
    "href": "topics/ts_decomposition/transformations.html#moving-averages",
    "title": "Transformations",
    "section": "Moving averages",
    "text": "Moving averages\n++ explain the odd moving average window is symmetric (there are equal amount of point on each side of the center). The even size moving average does not have a natural center but we can change the weights to achieve a symmetric window. The weights will not be the same, on the edges the weights will be smaller. Actually in order to make an even size window symmetric we need to apply additional moving average of window 2 so the edge’s weights will be half the other (inside) weights.\n\n\nma_file_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nma_raw_df = pd.read_csv(ma_file_path,index_col = \"date\")\n\nma_raw_df.index = pd.to_datetime(ma_raw_df.index)\n\ndel ma_file_path\n\n\n\nma_df = ma_raw_df.copy()\n\nma_df[\"ma_3\"] = ma_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\nma_df[\"ma_3\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\neven_window_size = 12\n\nma_df[\"ma_2_12\"] = ma_df[\"sales\"].rolling(window = even_window_size).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].rolling(window = 2, center = True).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].shift(- even_window_size // 2)\n\n\nplt.clf()\n\nma_df[\"ma_2_12\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne7329f12e3f9c1d7266bdc4614b17630418a7520"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html",
    "href": "topics/ts_decomposition/lowess_decomp.html",
    "title": "LOWESS Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.model_selection import KFold\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "href": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "title": "LOWESS Decomposition",
    "section": "LOWESS parametres",
    "text": "LOWESS parametres\n++ explain the parameters in python implementations: frac - fraction of data for the window size (determines the smoothness) it - number of iterations for robust regression. Also explain the endog and exog parameters\n\n\ny = raw_df[\"sales\"]\n\nx = np.arange(0, len(y))\n\nts_decomp = lowess(endog = y, exog = x, frac = 0.1, it = 3)\n\nraw_df[\"trend_lowess\"] = ts_decomp[:,1]\n\n\nCross validation to select appropriate fraction parameter\n\ndef get_rmse_for_df(X,y, frac_param):\n  KFold_obj = KFold(n_splits = 5, shuffle = True, random_state = 0)\n  rmse_list = []\n  for train_index, test_index in KFold_obj.split(X,y):\n    X_train = X[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X[test_index]\n    y_test = y.iloc[test_index]\n    y_pred = lowess(endog = y_train, exog = X_train, frac = frac_param, xvals = X_test)\n    rmse = root_mean_squared_error(y_pred, y_test)\n    rmse_list.append(rmse)\n  return rmse_list\n\n\nresults = []\n\nfor temp_frac in [0.05,0.1,0.5,1]:\n  rmse_list = get_rmse_for_df(X = x,y = y,frac_param = temp_frac)\n  rmse_df = pd.DataFrame(data = rmse_list, columns = [\"rmse\"])\n  rmse_df[\"frac\"] = temp_frac\n  results.append(rmse_df)\n\n  \npd.concat(results, axis = 0)\n\n           rmse  frac\n0  22981.706955  0.05\n1  22624.297346  0.05\n2  20582.387871  0.05\n3  28572.985707  0.05\n4  21376.838789  0.05\n0  23018.284152  0.10\n1  21215.656841  0.10\n2  20279.923647  0.10\n3  25214.068880  0.10\n4  20387.775112  0.10\n0  26184.512853  0.50\n1  22354.596572  0.50\n2  23964.726098  0.50\n3  25516.588727  0.50\n4  20683.629095  0.50\n0  26343.368462  1.00\n1  23258.024086  1.00\n2  24924.462787  1.00\n3  26254.158157  1.00\n4  20870.150531  1.00"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html",
    "href": "topics/ts_decomposition/stl_decomp.html",
    "title": "STL Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#trend",
    "href": "topics/ts_decomposition/stl_decomp.html#trend",
    "title": "STL Decomposition",
    "section": "Trend",
    "text": "Trend\n++ explain the parameters of the STL implementation in statsmodels.tsa.seasonal explicitly address the seasonal, period and robust parameters. Other parameters can be left alone - the default values are typically good enough.\n\n\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid\n                 \n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\ndecompostion_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\ndecompostion_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\ndecompostion_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html",
    "href": "topics/na_imputation/na_imputation.html",
    "title": "Missing data imputation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_missing.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\nna_idx = raw_df.isnull()\n\n# url = \"https://raw.githubusercontent.com/facebook/prophet/master/examples/example_retail_sales.csv\"\n# df = pd.read_csv(url)\n# \n# df = df.iloc[0:160].copy()\n# \n# df.columns = [\"date\",\"sales\"]\n# \n# df = df.set_index(\"date\").copy()\n# \n# \n# # Insert missing data into dataframe\n# df.iloc[10:11] = np.NaN\n# df.iloc[25:28] = np.NaN\n# df.iloc[40:45] = np.NaN\n# df.iloc[70:94] = np.NaN\n# \n# \n# df.to_csv(file_path)\nprint(f\"There are {raw_df['sales'].isnull().sum()} missing values, these are {np.round(raw_df['sales'].isnull().sum() / len(raw_df)* 100,3)} percent of the data\")\n\nThere are 33 missing values, these are 20.625 percent of the data\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#forward-fill",
    "href": "topics/na_imputation/na_imputation.html#forward-fill",
    "title": "Missing data imputation",
    "section": "Forward fill",
    "text": "Forward fill\n\nffill_df = raw_df.ffill()\n\nplt.clf()\n\nax = ffill_df.plot(linestyle=\"-\", marker=\".\")\n\nffill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#backward-fill",
    "href": "topics/na_imputation/na_imputation.html#backward-fill",
    "title": "Missing data imputation",
    "section": "Backward fill",
    "text": "Backward fill\n++ explain that backward filling can introduce “data leakage” because we are carrying to the past information from the future.\n\nbfill_df = raw_df.bfill()\n\nplt.clf()\n\nax = bfill_df.plot(linestyle=\"-\", marker=\".\")\n\nbfill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "href": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "title": "Missing data imputation",
    "section": "Linear interpolation",
    "text": "Linear interpolation\n\nlin_inter = raw_df.interpolate(method = \"time\")\n\nplt.clf()\n\nax = lin_inter.plot(linestyle=\"-\", marker=\".\")\n\nlin_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "href": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "title": "Missing data imputation",
    "section": "Spline interpolation",
    "text": "Spline interpolation\n\nspline_inter = raw_df.interpolate(method = \"spline\", order = 3)\n\nplt.clf()\n\nax = spline_inter.plot(linestyle=\"-\", marker=\".\")\n\nspline_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html",
    "href": "topics/outliers/outliers.html",
    "title": "Missing data imputation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_outliers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#de-seasonalise-data",
    "href": "topics/outliers/outliers.html#de-seasonalise-data",
    "title": "Missing data imputation",
    "section": "De seasonalise data",
    "text": "De seasonalise data\n++ explain that we are deseasonalizing the data in order to prevent disturbance for outlier identification\n\n\nstl_decomp = STL(raw_df[\"sales\"], robust = True).fit()\n\nseasonal_component = stl_decomp.seasonal\n\nraw_df[\"sales_deseasonalised\"] = raw_df[\"sales\"] - seasonal_component\n\n\nplt.clf()\n\nraw_df[\"sales_deseasonalised\"].plot(marker = \".\")\n\nplt.title(\"Deseasonlalized data with outliers\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#identify-outliers",
    "href": "topics/outliers/outliers.html#identify-outliers",
    "title": "Missing data imputation",
    "section": "Identify outliers",
    "text": "Identify outliers\n\nRolling mean and standard deviation\n\n\nraw_df[[\"roll_mean\",\"roll_std\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_mean\":\"mean\", \"roll_std\":\"std\"})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper\"] = raw_df[\"roll_mean\"] + margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"lower\"] = raw_df[\"roll_mean\"] - margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"is_outlier\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRolling median and median absolute deviation\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\nraw_df[[\"roll_median\",\"roll_mad\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_median\":\"median\", \"roll_mad\":mad})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper_2\"] = raw_df[\"roll_median\"] + margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"lower_2\"] = raw_df[\"roll_median\"] - margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"is_outlier_2\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower_2\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper_2\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df[\"lower_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#impute-outliers",
    "href": "topics/outliers/outliers.html#impute-outliers",
    "title": "Missing data imputation",
    "section": "Impute outliers",
    "text": "Impute outliers\n\n\nraw_df[\"sales_na\"] = raw_df[\"sales_deseasonalised\"]\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_na\"] = np.nan\n\nraw_df[\"sales_imputed\"] = raw_df[\"sales_na\"].interpolate(method = \"time\",\n                                                         inlace = True)\n\n\nplt.clf()\n\nax = raw_df[\"sales_imputed\"].plot()\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_imputed\"].plot(ax = ax,\n                                                      alpha = 0.5,\n                                                      color = \"red\",\n                                                      marker = \"o\",\n                                                      linestyle = \"none\")\n\nplt.title(\"Imputed outliers\")\n\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "href": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "title": "Missing data imputation",
    "section": "Seasonal decomposition and interpolation",
    "text": "Seasonal decomposition and interpolation\n++ explain that we first use linear interpolation because STL can not handle missing data\n\n\nstl_inter = STL(raw_df.interpolate(method = \"time\"), seasonal = 31).fit()\n\nseasonal_component = stl_inter.seasonal\n\ndeaseasonlised_df = raw_df[\"sales\"] - seasonal_component\n\ndf_inter = deaseasonlised_df.interpolate(method = \"time\")\n\ndf_final = df_inter + seasonal_component\n\ndf_final = df_final.to_frame().rename(columns = {0:\"sales\"})\n\n\nplt.clf()\n\nax = df_final.plot(linestyle=\"-\", marker=\".\")\n\ndf_final[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/auto_correlation.html",
    "href": "topics/lagged_features/auto_correlation.html",
    "title": "Autocorrelation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport os \n\nfrom statsmodels.tsa.stattools import acf, pacf, ccf\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom statsmodels.tsa.seasonal import STL\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid"
  },
  {
    "objectID": "topics/lagged_features/lag_plots.html",
    "href": "topics/lagged_features/lag_plots.html",
    "title": "Autoregressive processes",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nAR 1\n\n\nnum_periods = 1000\n\nar_coeff = 0.9\n\nconst_term = 0\n\ntime_index = pd.date_range(start = \"2000-01-01\", periods = num_periods,\n                           freq = \"d\")\n                           \nar1_series = np.zeros(num_periods)\n\nwhite_noise_series = np.zeros(num_periods)\n\ntrend_series = np.zeros(num_periods)\n\n\nfor t in range(1,num_periods):\n  noise = np.random.normal()\n  white_noise_series[t] = noise\n  ar1_series[t] = const_term + ar_coeff * ar1_series[t - 1] + noise\n  trend_series[t] = t + noise\n\nts_df = pd.DataFrame(data = list(zip(white_noise_series,ar1_series,trend_series)),\nindex = time_index, columns = [\"white_noise\",\"ar_1\",\"trend\"])\n\ndel white_noise_series, ar1_series, trend_series\n\ndel t, time_index, ar_coeff, const_term, noise, num_periods\n\n\n\nPandas implemenation\n\n\n\ndef plot_lags_pandas (temp_df, plot_title, ncols = 3, nrows = 3):\n\n  fig, axes = plt.subplots(ncols,nrows)\n  \n  for temp_ind, temp_ax in enumerate(axes.flatten()):\n    pd.plotting.lag_plot(temp_df, lag = temp_ind + 1, ax = temp_ax,\n                         marker = \".\", linestyle='None')\n    temp_ax.set_title(f\"Lag_{temp_ind}\")\n    \n \n  fig.suptitle(plot_title, fontsize=16)\n  \n  plt.tight_layout()\n  \n  plt.subplots_adjust(hspace=1, top=0.8)\n  \n  plt.show()\n\n\nplot_lags_pandas(ts_df[[\"white_noise\"]], plot_title = \"White noise\")\n\n\n\n\n\n\n\n\n\nplot_lags_pandas(ts_df[[\"trend\"]], plot_title = \"Trend\")\n\n\n\n\n\n\n\n\n\nplot_lags_pandas(raw_df.copy(), \"Retail sales\", nrows = 4, ncols = 3)\n\n\n\n\n\n\n\n\n\n\nManual\n\n\n\ndef plot_lags(temp_df, plot_title):\n\n  fig, axes = plt.subplots(3,3)\n  \n  for temp_ind, temp_ax in enumerate(axes.flatten()):\n    temp_ax.scatter(x = temp_df.iloc[:,0],\n                    y = temp_df.iloc[:,0].shift(temp_ind), marker = \".\")\n    temp_ax.set_title(f\"Lag_{temp_ind}\")\n  \n  fig.suptitle(plot_title, fontsize=16)\n  \n  plt.tight_layout()\n  \n  plt.subplots_adjust(hspace=1, top=0.8)\n  \n  plt.show()\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"white_noise\"]].copy(), \"White noise\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"ar_1\"]].copy(), \"Ar 1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"trend\"]].copy(), \"Trend\")"
  },
  {
    "objectID": "topics/lagged_features/ar_process.html",
    "href": "topics/lagged_features/ar_process.html",
    "title": "Autoregressive processes",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\nAR 1\n\n\nnum_periods = 1000\n\nar_coeff = 0.9\n\nconst_term = 0\n\ntime_index = pd.date_range(start = \"2000-01-01\", periods = num_periods,\n                           freq = \"d\")\n                           \ntime_series = np.zeros(num_periods)\n\nfor t in range(1,num_periods):\n  noise = np.random.normal()\n  time_series[t] = const_term + ar_coeff * time_series[t - 1] + noise\n\nar1_df = pd.DataFrame(data = time_series, index = time_index)\n\n\nplt.clf()\n\nar1_df.plot(legend = False)\n\nplt.title(\"AR 1 process\")\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/lagged_features.html",
    "href": "topics/lagged_features/lagged_features.html",
    "title": "Lagged features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom feature_engine.timeseries.forecasting import LagFeatures\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/lagged_features/lagged_features.html#lagged-features",
    "href": "topics/lagged_features/lagged_features.html#lagged-features",
    "title": "Lagged features",
    "section": "Lagged features",
    "text": "Lagged features\n\npandas implementation\n\n\nlag_df = raw_df.copy()\n\nfor temp_lag in [1,2,12]:\n  lag_df[f\"lag_{temp_lag}\"] = lag_df[\"sales\"].shift(freq = f\"{temp_lag}MS\")\n  \n\n\nplt.clf()\nlag_df.plot(alpha = 0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature_engine implementation\n\nlag_trans = LagFeatures(variables = [\"sales\"], freq = [\"1MS\",\"2MS\",\"12MS\"])\n\nlag_df_fe = lag_trans.fit_transform(raw_df.copy()) \n\nprint(lag_df_fe.head())\n\n             sales  sales_lag_1MS  sales_lag_2MS  sales_lag_12MS\ndate                                                            \n1992-01-01  146376            NaN            NaN             NaN\n1992-02-01  147079       146376.0            NaN             NaN\n1992-03-01  159336       147079.0       146376.0             NaN\n1992-04-01  163669       159336.0       147079.0             NaN\n1992-05-01  170068       163669.0       159336.0             NaN"
  },
  {
    "objectID": "topics/lagged_features/distributed_lag_features.html",
    "href": "topics/lagged_features/distributed_lag_features.html",
    "title": "Distributed lag features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport os \n\n\nfrom statsmodels.tsa.seasonal import MSTL\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path)\n\nraw_df.index = pd.to_datetime(raw_df[\"Date_Time\"])\n\nraw_df = raw_df.drop(columns = [\"Date_Time\"]).copy()\nfig, axes = plt.subplots(nrows = 2)\n\nfor idx, ax in enumerate(axes):\n  raw_df.iloc[:,[idx]].plot(ax = ax,\n                            legend = False,\n                            title = raw_df.columns.values[idx])\n  ax.set_xlabel('')\n  ax.set_ylabel('')\n                            \nplt.tight_layout(h_pad=3)\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/distributed_lag_features.html#extract-calendar-features",
    "href": "topics/lagged_features/distributed_lag_features.html#extract-calendar-features",
    "title": "Distributed lag features",
    "section": "Extract calendar features",
    "text": "Extract calendar features\n\n\ncalendar_df = raw_df.copy()\n\ncalendar_df[\"hour\"] = calendar_df.index.hour\n\ncalendar_df[\"month\"] = calendar_df.index.month\n\ncalendar_df[\"day_of_week\"] = calendar_df.index.day_of_week\n\n\nDaily seasonality\n++ explain that we see a pattern and that means that the time of day has an effect on the expected value. That means that we should include a lag of 24 hours that will represent the value of the previous observation at the same hour.\n\nfig, axes = plt.subplots(nrows = 2)\n\nfor idx, ax in enumerate(axes):\n  calendar_df.groupby(\"hour\")[[\"CO_sensor\",\"RH\"]].mean().iloc[:,[idx]].plot(ax = ax,\n                            legend = False,\n                            title = raw_df.columns.values[idx])\n  ax.set_xlabel('')\n  ax.set_ylabel('')\n                            \nplt.tight_layout(h_pad=3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nYearly seasonality\n++ explain that we see a pattern and that means that the month has an effect on the expected value. That means that we should include a lag of 24 hours * 365 days that will represent the value of the previous observation at the same month.\n\nfig, axes = plt.subplots(nrows = 2)\n\nfor idx, ax in enumerate(axes):\n  calendar_df.groupby(\"month\")[[\"CO_sensor\",\"RH\"]].mean().iloc[:,[idx]].plot(ax = ax,\n                            legend = False,\n                            title = raw_df.columns.values[idx])\n  ax.set_xlabel('')\n  ax.set_ylabel('')\n                            \nplt.tight_layout(h_pad=3)\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/auto_correlation.html#real-world-data",
    "href": "topics/lagged_features/auto_correlation.html#real-world-data",
    "title": "Autocorrelation",
    "section": "Real world data",
    "text": "Real world data\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\npassengers_df = pd.read_csv(file_path,index_col = \"date\")\n\npassengers_df.index = pd.to_datetime(passengers_df.index)\n\nsales_df = raw_df.copy()\n\ndel file_path\n\n\ndf = pd.DataFrame(data = {\"passengers\": passengers_df[\"passengers\"].iloc[0:120],\n                          \"sales\": sales_df[\"sales\"].iloc[0:120]})"
  },
  {
    "objectID": "topics/intro/eda.html",
    "href": "topics/intro/eda.html",
    "title": "Exploratory Data Analysis - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/intro/eda.html#introduction-to-exploratory-data-analysis-eda",
    "href": "topics/intro/eda.html#introduction-to-exploratory-data-analysis-eda",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Introduction to Exploratory Data Analysis (EDA)",
    "text": "Introduction to Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) is a crucial step in any data science or time series analysis process. It involves visually and statistically summarizing the key characteristics of a dataset to gain insights into its structure, underlying patterns, and potential issues such as missing data or outliers. In this short tutorial, we will focus on performing EDA on time series data. Specifically, we will identify and handle missing values and explore seasonality—one of the most common characteristics in time series data. Seasonality refers to patterns that repeat at regular intervals, such as daily, weekly, or yearly trends, and detecting it is essential for accurate forecasting."
  },
  {
    "objectID": "topics/intro/eda.html#data-loading",
    "href": "topics/intro/eda.html#data-loading",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/intro/eda.html#data-visualization",
    "href": "topics/intro/eda.html#data-visualization",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nfor temp_col in air_quality_df.columns.values:\n  air_quality_df[temp_col].plot(figsize = (20,6))\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we plot each of the columns in the dataset (CO_sensor and RH) to visually explore the time series data. These plots provide a general understanding of how pollutant levels (CO_sensor) and humidity (RH) fluctuate over time. Larger trends, spikes, or patterns such as seasonality may already be visible, and such visualizations are a useful first step before deeper analysis."
  },
  {
    "objectID": "topics/intro/eda.html#handling-missing-values",
    "href": "topics/intro/eda.html#handling-missing-values",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nIn time series analysis, it is common to encounter missing data due to sensor malfunctions or recording errors. Properly handling missing data is crucial because forecasting models often assume data points are spaced at regular intervals. To address this, we first ensure the data is uniformly spaced by converting the Date_Time index to an hourly frequency. This allows us to easily detect missing data points and fill the gaps accordingly.\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n\nIn this block, we address missing values by using forward-filling (ffill()), which propagates the last valid observation forward until a new non-missing value is encountered. This technique works well when the missing values are sparse or when we expect the data to remain stable over short intervals. We create new columns, such as CO_sensor_imputed and RH_imputed, to store the imputed values, allowing us to compare them with the original data and ensure that no critical information is lost."
  },
  {
    "objectID": "topics/intro/eda.html#visualizing-imputed-values",
    "href": "topics/intro/eda.html#visualizing-imputed-values",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Visualizing Imputed Values",
    "text": "Visualizing Imputed Values\nTo ensure that our imputation strategy has been applied correctly, we will visually compare the original data with the imputed values. This comparison will highlight where missing values were filled and help us verify that the imputation did not introduce any distortions into the time series.\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  \n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  \n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(\n    ax = ax,legend = False,marker = \".\", color = \"red\", linestyle='None')\n    \n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  \n  plt.tick_params(axis='both', which='major', labelsize=16)\n  \n  plt.xlabel('')  # Disable x-axis label\n  \n  plt.ylabel('')  # Disable y-axis label\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this step, we overlay the imputed values onto the original time series. The original data is represented by a continuous line, while missing values that were filled via forward-fill are marked with red dots. This side-by-side comparison helps us visually assess the areas where imputation occurred and check whether it was applied appropriately without affecting the overall trend of the data."
  },
  {
    "objectID": "topics/intro/eda.html#seasonality",
    "href": "topics/intro/eda.html#seasonality",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality is a recurring pattern in data that occurs at regular intervals, often influenced by natural or social processes. In the context of air quality, pollutant levels such as carbon monoxide may follow daily or weekly cycles due to human activities like traffic or industrial operations. To detect seasonality, we group the data by the time of day (i.e., by the hour) and calculate the average pollutant levels across all observations for each hour. This provides a clear picture of how pollutant concentrations vary throughout the day.\n\nhours_con = impute_df.groupby(impute_df.index.time)[\n  \"CO_sensor\"].mean().reset_index()\n\nhours_con.plot(x = 'index', y = 'CO_sensor', legend = False)\nplt.title(\"Pollutant concentration over day time\")\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this final step, we plot the average CO_sensor concentration over the course of a typical day to visualize the intra-day seasonality. This chart allows us to observe how carbon monoxide levels fluctuate during different times of the day, potentially reflecting periods of higher traffic or other factors that influence air quality. Understanding these seasonal patterns is key for making accurate predictions and taking appropriate action in environmental monitoring."
  },
  {
    "objectID": "topics/intro/feature_egineering.html",
    "href": "topics/intro/feature_egineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nFeature engineering is the process of transforming and creating new input variables, or features, from raw data to improve the performance of predictive models. It involves converting raw data into meaningful inputs that capture underlying patterns or relationships useful for forecasting. In the context of time series forecasting, the choice of features can significantly impact the model’s ability to make accurate predictions. For example, simply using the raw values of a time series may not be enough for the model to capture complex temporal dynamics such as trends or seasonality. Therefore, transforming features or creating new ones like time-based features, lag features, or cyclical patterns is crucial. This tutorial will demonstrate several types of feature transformations, including time-related features, lag features, window (rolling) features, and periodic features. This is only a short demonstration, each type will be covered in detail in its respective section."
  },
  {
    "objectID": "topics/intro/feature_egineering.html#data-loading",
    "href": "topics/intro/feature_egineering.html#data-loading",
    "title": "Feature Engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/intro/feature_egineering.html#time-related-features",
    "href": "topics/intro/feature_egineering.html#time-related-features",
    "title": "Feature Engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#lag-features",
    "href": "topics/intro/feature_egineering.html#lag-features",
    "title": "Feature Engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#window-features",
    "href": "topics/intro/feature_egineering.html#window-features",
    "title": "Feature Engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n  (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n  (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n  ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#periodic-features",
    "href": "topics/intro/feature_egineering.html#periodic-features",
    "title": "Feature Engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclical_df = cyclical.fit_transform(calendar_df)\n\ncyclical_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#save-processed-data",
    "href": "topics/intro/feature_egineering.html#save-processed-data",
    "title": "Feature Engineering",
    "section": "Save processed data",
    "text": "Save processed data\nIn the final step of preprocessing, we must address the missing values that have been introduced during feature engineering, particularly in the creation of lag and window features. These missing values arise because, for example, a 24-hour lag feature requires data from 24 hours prior, which is unavailable for the first 24 observations. Similarly, window features, such as rolling averages, rely on past data over a specified period, leading to NA values at the start of the series where insufficient prior data exists.\nThere are generally two approaches to handle these missing values: imputation or deletion. Imputation involves replacing missing values with substitutes, such as the mean or median of the available data. However, given that the number of missing values is relatively small in this case, and since dropping rows with missing data simplifies the process, we will opt to drop the rows containing missing values. This ensures that the final dataset is complete and ready for modeling without introducing any potential bias from imputation.\nOnce the missing values are handled, the processed dataset, which includes the original data along with the engineered features (such as lag, window, and cyclical features), is saved to a CSV file for further use.\n\n\nprocessed_df = pd.concat([air_quality_df, calendar_df,lag_features_df, cyclical_df], axis = 1)\n\nprocessed_df.dropna(inplace = True)\n\nprocessed_df.to_csv(os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \"\\\\data\\\\air_quality_processed_df.csv\")"
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html",
    "href": "topics/forecasting/forecasting_pipeline.html",
    "title": "Forecasting Pipeline",
    "section": "",
    "text": "import pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nOur goal in this pipeline is to transform the data by applying a series of feature engineering techniques to prepare it for time series forecasting. We will perform the following tasks:\nThe feature engineering will be performed using the feature_engine library, which offers an easy-to-use interface to build these transformations. Our goal is to encapsulate all operations in one pipeline for easier reproducibility and maintainability."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#date-time-features",
    "href": "topics/forecasting/forecasting_pipeline.html#date-time-features",
    "title": "Forecasting Pipeline",
    "section": "Date time features",
    "text": "Date time features\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nprocessed_df = date_time_feat.fit_transform(raw_df.copy())\n\n\nprocessed_df.head()\n\n                     CO_sensor    RH  month  ...  day_of_month  hour  weekend\nDate_Time                                    ...                             \n2004-04-04 00:00:00     1224.0  56.5      4  ...             4     0        1\n2004-04-04 01:00:00     1215.0  59.2      4  ...             4     1        1\n2004-04-04 02:00:00     1115.0  62.4      4  ...             4     2        1\n2004-04-04 03:00:00     1124.0  65.0      4  ...             4     3        1\n2004-04-04 04:00:00     1028.0  65.3      4  ...             4     4        1\n\n[5 rows x 8 columns]\n\n\nIn this step, we are extracting date and time features from the index, such as the month, day of the week, and hour of the day. This helps us leverage the temporal structure of the data in subsequent modeling steps. For example, the day of the week or the hour could influence air quality, so extracting such features allows us to include this information in the model."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#lag-features",
    "href": "topics/forecasting/forecasting_pipeline.html#lag-features",
    "title": "Forecasting Pipeline",
    "section": "Lag features",
    "text": "Lag features\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\nprocessed_df = lag_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"lag\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_lag_1h  RH_lag_1h  CO_sensor_lag_24h  RH_lag_24h\nDate_Time                                                                      \n2004-04-04 00:00:00               NaN        NaN                NaN         NaN\n2004-04-04 01:00:00            1224.0       56.5                NaN         NaN\n2004-04-04 02:00:00            1215.0       59.2                NaN         NaN\n2004-04-04 03:00:00            1115.0       62.4                NaN         NaN\n2004-04-04 04:00:00            1124.0       65.0                NaN         NaN\n\n\nHere, we create lag features for the CO_sensor and RH (Relative Humidity) variables. Lagging is a powerful technique in time series forecasting as it allows us to capture information from previous time steps. In this case, we are creating two types of lags: one for 1 hour prior and another for 24 hours prior, which will help the model learn patterns that evolve over both short and longer time scales."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#window-features",
    "href": "topics/forecasting/forecasting_pipeline.html#window-features",
    "title": "Forecasting Pipeline",
    "section": "Window features",
    "text": "Window features\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\nprocessed_df = window_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"win\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_window_3h_mean  RH_window_3h_mean\nDate_Time                                                       \n2004-04-04 00:00:00                       NaN                NaN\n2004-04-04 01:00:00               1224.000000          56.500000\n2004-04-04 02:00:00               1219.500000          57.850000\n2004-04-04 03:00:00               1184.666667          59.366667\n2004-04-04 04:00:00               1151.333333          62.200000\n\n\nIn this step, we generate window features. These features capture rolling window statistics over a 3-hour window for the CO_sensor and RH variables, calculated at 1-hour intervals. This provides insight into the short-term trends or fluctuations in the data, as moving averages or other summary statistics over the window can help smooth out noisy data and emphasize underlying patterns."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#cyclical-features",
    "href": "topics/forecasting/forecasting_pipeline.html#cyclical-features",
    "title": "Forecasting Pipeline",
    "section": "Cyclical features",
    "text": "Cyclical features\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\nprocessed_df = cyclical_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"month\" or \"hour\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor    RH  month  ...  month_cos  hour_sin  hour_cos\nDate_Time                                    ...                               \n2004-04-04 00:00:00     1224.0  56.5      4  ...       -0.5  0.000000  1.000000\n2004-04-04 01:00:00     1215.0  59.2      4  ...       -0.5  0.269797  0.962917\n2004-04-04 02:00:00     1115.0  62.4      4  ...       -0.5  0.519584  0.854419\n2004-04-04 03:00:00     1124.0  65.0      4  ...       -0.5  0.730836  0.682553\n2004-04-04 04:00:00     1028.0  65.3      4  ...       -0.5  0.887885  0.460065\n\n[5 rows x 18 columns]\n\n\nCertain features, like month and hour, exhibit cyclical behavior (e.g., after December comes January, and after 23:00 comes 00:00). By converting these features into cyclical (sin and cos) representations, we ensure that the model properly understands these cyclic relationships. This prevents the model from interpreting consecutive values as linearly distant when they are, in fact, close (e.g., December and January)."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#missing-values-and-data-leakage-treatment",
    "href": "topics/forecasting/forecasting_pipeline.html#missing-values-and-data-leakage-treatment",
    "title": "Forecasting Pipeline",
    "section": "Missing values and data leakage treatment",
    "text": "Missing values and data leakage treatment\n\n\nna_drop = DropMissingData()\n\nprocessed_df = na_drop.fit_transform(processed_df.copy())\n\nAfter feature engineering, we may have introduced missing values, especially with techniques like lagging and windowing, which require previous data points. Therefore, we use DropMissingData to remove rows with missing values, ensuring a clean dataset for subsequent modeling.\nAt this stage, we have created a data frame of explanatory variables (X_mat). It is crucial to avoid data leakage (or “look-ahead bias”) in time series forecasting. This occurs when information from the future is unintentionally used to predict past events. To prevent this, we need to remove the original features (like CO_sensor and RH) after extracting all the required information through our feature engineering steps.\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\nprocessed_df = drop_feat.fit_transform(processed_df.copy())\n\nprocessed_df.head()\n\n                     month  week  day_of_week  ...  month_cos  hour_sin  hour_cos\nDate_Time                                      ...                               \n2004-04-05 00:00:00      4    15            0  ...       -0.5  0.000000  1.000000\n2004-04-05 01:00:00      4    15            0  ...       -0.5  0.269797  0.962917\n2004-04-05 02:00:00      4    15            0  ...       -0.5  0.519584  0.854419\n2004-04-05 03:00:00      4    15            0  ...       -0.5  0.730836  0.682553\n2004-04-05 04:00:00      4    15            0  ...       -0.5  0.887885  0.460065\n\n[5 rows x 16 columns]\n\n\nFinally, we drop the original features (CO_sensor and RH) from the dataset. These original columns have already contributed their information through lagged, windowed, and cyclical features, so retaining them would lead to redundancy or potential data leakage."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#pipeline",
    "href": "topics/forecasting/forecasting_pipeline.html#pipeline",
    "title": "Forecasting Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\nWe now pack all the steps into a single pipeline. Pipelines allow us to apply a sequence of transformations to the data in a well-structured and reproducible way. This makes the data preparation process more efficient and less error-prone, especially when scaling up or iterating over different models or datasets.\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\npipe_processed_df = trans_pipe.fit_transform(raw_df.copy())\n\nprint(f\"The processed df is equal to pipe_processed_df : {pipe_processed_df.equals(processed_df)}\")\n\nThe processed df is equal to pipe_processed_df : True\n\n\nHere, we’ve consolidated the entire feature engineering process into a single Pipeline object, which includes:\n\nDate and time feature extraction\nLag features\nWindow features\nCyclical features\nDropping missing values\nRemoving original features\n\nThis pipeline can be applied to any new dataset that follows a similar structure, ensuring that the feature engineering process is both scalable and consistent across different time periods or datasets. Additionally, this approach enhances model reproducibility and ease of deployment. After fitting and transforming the raw dataset through the pipeline, we confirm that the output matches the manually processed DataFrame."
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html",
    "href": "topics/forecasting/forecasting_one_step.html",
    "title": "Forecasting one period (step) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast the next period (step) using a pipeline approach. The pipeline encapsulates all preprocessing operations such as feature engineering, imputation, and feature selection into a single streamlined process. This allows for consistency and efficiency when handling time series data. It is crucial to ensure that the explanatory features (the X matrix) and the target feature (the y vector) are properly aligned in time. Misalignment can lead to look-ahead bias, where future information is inappropriately used in the model training phase, resulting in overoptimistic performance estimates. Careful attention is also required to avoid data leakage, ensuring that the model does not have access to information from the future when forecasting.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#pipeline",
    "href": "topics/forecasting/forecasting_one_step.html#pipeline",
    "title": "Forecasting one period (step) ahead",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_one_step.html#train-and-test-split",
    "title": "Forecasting one period (step) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#preprocess-data",
    "href": "topics/forecasting/forecasting_one_step.html#preprocess-data",
    "title": "Forecasting one period (step) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.fit_transform(X_test.copy())\n\nDuring preprocessing, we apply transformations that handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the training features but remain in the target vector. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the training and test target vectors based on the updated index of the processed feature sets.\n\nprint(y_train.shape)\n\n(7654,)\n\ny_train = y_train.loc[X_train_processed.index]\n\nprint(y_train.shape)\n\n(7426,)\n\n\ny_test = y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#prediction",
    "href": "topics/forecasting/forecasting_one_step.html#prediction",
    "title": "Forecasting one period (step) ahead",
    "section": "Prediction",
    "text": "Prediction\n\nlasso_model = Lasso()\n\nlasso_model.fit(X_train_processed, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_vec = lasso_model.predict(X_test_processed)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#evaluation",
    "href": "topics/forecasting/forecasting_one_step.html#evaluation",
    "title": "Forecasting one period (step) ahead",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\nrmse = np.round(root_mean_squared_error(y_test, predictions_vec), 4)\n\nprint(f\"The root mean squared error on the test set is {rmse}\")\n\nThe root mean squared error on the test set is 86.7041"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html",
    "href": "topics/forecasting/forecasting_multiple_steps.html",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast multiple period (step) using a pipeline approach.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#pipeline",
    "href": "topics/forecasting/forecasting_multiple_steps.html#pipeline",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_multiple_steps.html#train-and-test-split",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]\n\nSince we want to make predictions for multiple periods now instead of one series (vector) of target feature we will have multiple series (matrix of vectors), one for each forecast horizon. This will resolve some missing data in the target matrix because for each forecasting time point we need the next (future) 24 time points. For our last data point we don’t have future values at all, for the one before the last we only have one and so on. We handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the target matrix but remain in the train matrix. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the on the updated index.\n\nforecast_horizon = 24\n\nY_train = pd.DataFrame(index = X_train.index)\n\nY_test = pd.DataFrame(index = X_test.index)\n\nfor temp_h in range(forecast_horizon):\n  Y_train[f\"h_{temp_h}\"] = X_train[\"CO_sensor\"].shift(-temp_h, freq = \"h\")\n  Y_test[f\"h_{temp_h}\"] = X_test[\"CO_sensor\"].shift(-temp_h, freq = \"h\")\n\n\nprint(Y_train.iloc[0:5,0:5])\n\n                        h_0     h_1     h_2     h_3     h_4\nDate_Time                                                  \n2004-04-04 00:00:00  1224.0  1215.0  1115.0  1124.0  1028.0\n2004-04-04 01:00:00  1215.0  1115.0  1124.0  1028.0  1010.0\n2004-04-04 02:00:00  1115.0  1124.0  1028.0  1010.0  1074.0\n2004-04-04 03:00:00  1124.0  1028.0  1010.0  1074.0  1034.0\n2004-04-04 04:00:00  1028.0  1010.0  1074.0  1034.0  1130.0\n\n\n\n\nY_train = Y_train.dropna().copy()\n\nY_test = Y_test.dropna().copy()\n\nX_train = X_train.loc[Y_train.index]\n\nX_test = X_test.loc[Y_test.index]"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#preprocess-data",
    "href": "topics/forecasting/forecasting_multiple_steps.html#preprocess-data",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.fit_transform(X_test.copy())\n\nY_train = Y_train.loc[X_train_processed.index]\n\nY_test = Y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#prediction",
    "href": "topics/forecasting/forecasting_multiple_steps.html#prediction",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Prediction",
    "text": "Prediction\n\nlasso_model = MultiOutputRegressor(Lasso())\n\nlasso_model.fit(X_train_processed, Y_train)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_mat = lasso_model.predict(X_test_processed)\n\npredictions_mat = pd.DataFrame(predictions_mat)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#evaluation",
    "href": "topics/forecasting/forecasting_multiple_steps.html#evaluation",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\n\nrmse_df = []\n\nfor temp_hor in range(Y_test.shape[1]):\n  y_test = Y_test.iloc[:,temp_hor]\n  pred_vec = predictions_mat.iloc[:,temp_hor]\n  rmse = np.round(root_mean_squared_error(y_test, pred_vec), 4)\n  rmse_df.append(pd.DataFrame(columns = [\"horizon\",\"rmse\"],\n                              data = [[temp_hor,rmse]]))\n\nrmse_df = pd.concat(rmse_df, axis = 0)\n\n\nrmse_df.plot(x = \"horizon\", y = \"rmse\")\n\nplt.show()"
  },
  {
    "objectID": "topics/window_features/window_features.html",
    "href": "topics/window_features/window_features.html",
    "title": "Window features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport os \n\n\nfrom feature_engine.timeseries.forecasting import WindowFeatures, ExpandingWindowFeatures\n\nfrom sktime.transformations.series.summarize import WindowSummarizer\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_electricity.csv\"\n\nraw_df = pd.read_csv(file_path)\n\nraw_df.index = pd.to_datetime(raw_df[\"date_time\"])\n\nraw_df = raw_df.drop(columns = [\"date_time\"]).copy()\n\nraw_df = raw_df.loc[\"2010\":].copy()\n\n\nraw_df[\"demand\"].plot()\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nraw_df.loc[\"2014\":,\"demand\"].plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nRolling windows\n++ explain that the data set seems to have seasonality on different (daily, weekly, yearly) time scales and we’ll use window features to extract and capture this information\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\n\nwin_df_pandas = raw_df.rolling(window = 24).agg([\"mean\",\"std\",mad]).shift(freq = \"1h\").copy()\n\n\n\nwin_fe_trans = WindowFeatures(variables = [\"demand\",\"temperature\"],\n                           functions = [\"mean\",\"std\"],\n                           window = [24, 24 * 7, 24 * 365],\n                           freq = \"1h\")\n                           \nwin_df_fe = win_fe_trans.fit_transform(raw_df.copy())\n\n\nplt.clf()\n\ncol_names = win_df_fe.filter(regex = \"demand_window_[0-9]+(_mean)+\").columns\n\nwin_df_fe[col_names].plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpanding windows\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\n\nwin_df_pandas_exp = raw_df.loc[\"2015\":].expanding().agg([\"mean\",\"std\",mad]).shift(freq = \"1h\").copy()\n\n\n\nwin_fe_trans_exp = ExpandingWindowFeatures(variables = [\"demand\",\"temperature\"],\n                           functions = [\"mean\",\"std\"],\n                           freq = \"1h\")\n                           \nwin_df_fe_exp = win_fe_trans_exp.fit_transform(raw_df.loc[\"2015\":].copy())\n\n\n\nExponential weights\n\n\ndef exp_weights(series_len, alpha):\n  weights_vec = np.ones(series_len)\n  for temp_ind in range(1,series_len):\n    weights_vec[series_len - temp_ind - 1] = (1 - alpha) * weights_vec[series_len - temp_ind]\n  return weights_vec\n\ndef exp_weighted_mean(x, alpha = 0.05):\n  weights_vec = exp_weights(len(x), alpha = alpha)\n  result = (weights_vec * x).sum() / weights_vec.sum()\n  return result\n\n\n\nweight_df = raw_df.loc[\"2015\":,[\"demand\"]].copy()\n\nmean_df = (weight_df\n                    .rolling(window = 24 * 7)\n                    .agg([\"mean\", exp_weighted_mean])\n                    .shift(freq = \"1h\").copy())\n                    \nmean_df.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in mean_df.columns]\n                    \nweight_df = weight_df.join(mean_df)\n\nweight_df.dropna(inplace = True)\n\n\nplt.clf()\n\nweight_df.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom feature_engine.timeseries.forecasting import LagFeatures, WindowFeatures, ExpandingWindowFeatures\nfrom feature_engine.imputation import DropMissingData\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Lag features\nlag_transformer = LagFeatures(variables=[\"demand\", \"temperature\"],\n                              periods=[1, 2, 3, 24, 24 * 7])\n                              \n\n# Window features\nwindow_transformer = WindowFeatures(\n    variables=[\"demand\", \"temperature\"],\n    functions=[\"mean\", \"std\", \"kurt\", \"skew\"],\n    window=[24, 24 * 7, 24 * 7 * 4, 24 * 7 * 4 * 12],\n    periods=1,\n)\n\n\n# Expanding features\nexpanding_window_transformer = ExpandingWindowFeatures(\n    variables=[\"demand\"], \n    functions=[\"mean\", \"std\", \"kurt\", \"skew\"]\n)\n\n\n# Drop missing data introduced by window and lag features\nimputer = DropMissingData()\n\ntrans_pipe = Pipeline(\n    [\n        (\"lag\", lag_transformer),\n        (\"rolling\", window_transformer),\n        (\"expanding\", expanding_window_transformer),\n        (\"drop_missing\", imputer)\n    ]\n)\n\n\n\nprocessed_df = trans_pipe.fit_transform(raw_df.copy())\n\n\nfrom sklearn.linear_model import Lasso\n\nX = processed_df.drop(columns = [\"demand\"]).copy()\n\ny = processed_df[\"demand\"]\n\nX_mat = StandardScaler().fit_transform(X)\n\nlasso_model = Lasso(alpha = 1)\n\nlasso_model.fit(X,y)\n\nLasso(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso(alpha=1) \n\n\n\n\nselected_features = pd.DataFrame({\"feature_names\":X.columns,\n                                  \"values\":lasso_model.coef_})\n                                  \nselected_features = selected_features.sort_values(\"values\",\n                                                  key = abs,\n                                                  ascending = False).iloc[0:10].copy()\n\n\nplt.clf()\n\nplt.figure(figsize=(15, 6))\n\nselected_features.sort_values(\"values\").plot(kind = \"barh\", x = \"feature_names\",\n                       y = \"values\", legend = False)\n\nplt.title(\"Selected features\")\n\nplt.ylabel(\"\")\n\nplt.subplots_adjust(left=0.7)\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/distributed_lag_features.html#detrend-and-deseasonalize",
    "href": "topics/lagged_features/distributed_lag_features.html#detrend-and-deseasonalize",
    "title": "Distributed lag features",
    "section": "Detrend and deseasonalize",
    "text": "Detrend and deseasonalize\n\n\nresid_df = raw_df.copy()\n\nfor temp_col in resid_df.columns:\n  mstl_decomp = MSTL(endog = resid_df[temp_col], periods = [24, 7*24]).fit()\n  resid_df[temp_col + \"_resid\"] = mstl_decomp.resid\n\n\nfig, axes = plt.subplots(nrows = 2)\n\nresid_df[\"CO_sensor_resid\"].plot(ax = axes[0], title = \"CO_sensor_resid\", xlabel = '')\n\nresid_df[\"RH_resid\"].plot(ax = axes[1], title = \"RH_resid\", xlabel = '')\n\nplt.tight_layout(h_pad=3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(ncols = 2)\n\nplot_acf(resid_df[\"CO_sensor_resid\"].copy(),\nax  = axes[0], lags = 100, marker = \".\")\n\nplot_pacf(resid_df[\"CO_sensor_resid\"].copy(),\nax  = axes[1], lags = 25, marker = \".\")\n\nfig.suptitle(\"CO_sensor residual ACF and PACF\", fontsize=16)\n\nplt.tight_layout(w_pad = 3)\n\nplt.show()"
  },
  {
    "objectID": "topics/trend_features/trend_features.html",
    "href": "topics/trend_features/trend_features.html",
    "title": "Trend features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.linear_model import LinearRegression\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/trend_features/trend_features.html#split-data",
    "href": "topics/trend_features/trend_features.html#split-data",
    "title": "Trend features",
    "section": "Split data",
    "text": "Split data\n\n\n\nraw_df[\"trend\"] = (raw_df.index - raw_df.index.min()).days.astype(float)\n\nsplit_date = pd.to_datetime(\"1960-01-01\")\n\n\n\nX_train = raw_df[[\"trend\"]].loc[raw_df.index &lt;= split_date]\n\nX_test = raw_df[[\"trend\"]].loc[raw_df.index &gt; split_date]\n\ny_train = raw_df[\"passengers\"].loc[raw_df.index &lt;= split_date]\n\ny_test = raw_df[\"passengers\"].loc[raw_df.index &gt; split_date]"
  },
  {
    "objectID": "topics/trend_features/trend_features.html#linear-trend",
    "href": "topics/trend_features/trend_features.html#linear-trend",
    "title": "Trend features",
    "section": "Linear trend",
    "text": "Linear trend\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\ny_pred_train = lin_reg.predict(X_train)\n\ny_pred_test = lin_reg.predict(X_test)\n\n\n\ndef plot_pred(raw_df, y_pred_train, y_pred_test):\n  \n  plot_df = raw_df.copy()\n  \n  plot_df[\"pred\"] = np.concatenate([y_pred_train, y_pred_test])\n  \n  plot_df[\"type\"] = np.concatenate([[\"train\"] * len(y_pred_train),\n                                    [\"test\"] * len(y_pred_test)])\n  \n  plot_df = plot_df.reset_index().copy()\n                                    \n  plot_df = plot_df.melt(id_vars = [\"type\", \"date\"],\n                         value_vars = [\"passengers\",\"pred\"],\n                         var_name = \"line_type\").copy()\n                         \n  plt.clf()\n                         \n  sns.lineplot(data=plot_df, x=\"date\",\n               y=\"value\", hue=\"type\", style=\"line_type\", legend = False)\n  \n  \n\n  plt.show()\n\n\nplot_pred(raw_df = raw_df.copy(), y_pred_train = y_pred_train,\n          y_pred_test = y_pred_test)"
  }
]