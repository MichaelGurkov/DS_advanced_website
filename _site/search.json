[
  {
    "objectID": "topics/time_series_decomposition.html",
    "href": "topics/time_series_decomposition.html",
    "title": "Time series decomposition",
    "section": "",
    "text": "import pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nimport os\nsales_df = pd.read_csv(os.path.expanduser(\"~/Documents\") + \n\"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\")\n\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%m/%d/%Y\")\n\nsales_df[\"sales\"] = sales_df[\"sales\"] / 1000\nplt.figure(figsize=(10, 4))\n\nsns.lineplot(x=\"date\", y=\"sales\",marker = \".\", data=sales_df)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "topics/time_series_decomposition.html#trend",
    "href": "topics/time_series_decomposition.html#trend",
    "title": "Time series decomposition",
    "section": "Trend",
    "text": "Trend\n\nMoving averages\n\n\n# Auxiliary functions\n\ndef plot_combined_trend_and_series(data,**kwargs):\n    # Create a plot with two line plots\n    ax = sns.lineplot(x='date', y='sales', data=data, color=\"lightgray\")\n    \n    sns.lineplot(x='date', y='ma_value', data=data, color=\"steelblue\", ax=ax)\n    \n    return ax\n\ndef plot_panel(wide_df):\n  \n  long_df = wide_df.melt(id_vars=['date', 'sales'],\n                         var_name='ma_type',\n                         value_name='ma_value')\n                         \n  panel_grid = sns.FacetGrid(long_df, col=\"ma_type\",\n                             col_wrap=2,height = 5,aspect = 1.5)\n                             \n  panel_grid.map_dataframe(plot_combined_trend_and_series)\n  \n  plt.show()\n\n\nodd_ma_df = sales_df.copy()\n\nfor win_len in [3,5,7,9]:\n  temp_name = f\"ma_{win_len}\"\n  odd_ma_df[temp_name] = odd_ma_df[\"sales\"].rolling(window = win_len,\n                                                    center = True).mean()\n                                                    \nplot_panel(odd_ma_df)\n\n\n\n\n\n\n\n\n\neven_ma_df = sales_df.copy()\n\nfor win_len in [4,6,8,12]:\n  temp_name = f\"ma_{win_len}\"\n  even_ma_df[temp_name] = even_ma_df[\"sales\"].rolling(window = win_len).mean()\n  even_ma_df[temp_name] = even_ma_df[temp_name].rolling(window = 2).mean()\n  even_ma_df[temp_name] = even_ma_df[temp_name].shift(-win_len//2)\n  \nplot_panel(even_ma_df)\n\n\n\n\n\n\n\n\nImportant - add an explanation of odd ma (pandas give correct result) and even ma (pandas give incorrect result, need to apply another 2 MA and center)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\n\n\nTime Series Decomposition"
  },
  {
    "objectID": "topics/tabularizing_time_series/ts_to_table.html",
    "href": "topics/tabularizing_time_series/ts_to_table.html",
    "title": "Tabularizing time series data",
    "section": "",
    "text": "import pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\n\nmulti_items_df = pd.read_csv(os.path.expanduser(\"~/Documents\") + \n\"\\\\DS_advanced_website\\\\data\\\\multi_item_sales.csv\")\n\nmulti_items_df[\"date\"] = pd.to_datetime(multi_items_df[\"date\"],\n                                        format = \"%d/%m/%Y\")\n\n\n\n# Auxiliary functions\n\ndef plot_series(data,**kwargs):\n    # Create a plot with two line plots\n    ax = sns.lineplot(x='date', y='sales', data=data)\n    \n    return ax\n\ndef plot_panel(long_df):\n  \n  panel_grid = sns.FacetGrid(long_df, col=\"item_category\",\n                             col_wrap=1,height = 2, aspect = 3)\n                             \n  panel_grid.map_dataframe(plot_series)\n  \n  plt.show()\n\n\nIntro to timeseries\n\nDefinition and example\nUnivariate and multivariate time series\nMultiple time series\nRegular vs irregular time series\nStationary vs non-stationary\n\nTime series data format has (at least) two mandatory components: 1. The data - a column of values 2. The time index - an additional column or (more commonly) as index of the pandas Series or Data Frame\n++ explain wide format (in this example a date column and 4 columns one for sales of each item: ‘beverages’, ‘bakery’, ‘cleaning’, ‘dairy’ )\n\nmulti_items_df.head()\n\n        date  beverages   bakery  cleaning  dairy\n0 2015-01-02        794  285.628       501    426\n1 2015-01-03        938  289.563       470    568\n2 2015-01-04        574  151.744       312    362\n3 2015-01-05       1299  457.543      1047    814\n4 2015-01-06       1028  405.280       831    679\n\n# Create a figure and axis\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(12, 8), sharex=True)\n\n# Plot each series against the date\nfor temp_ind in range(1,multi_items_df.shape[1]):\n  \n  temp_col = multi_items_df.columns.values[temp_ind]\n  \n  multi_items_df.plot(x='date', y=temp_col,ax=axes[temp_ind-1],\n                      title=temp_col.title(), legend = False)\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n++ explain long format (in this example a date column, an item category column and a sales values column)\n\nlong_format = multi_items_df.melt(id_vars = \"date\",\n                                  value_vars = ['beverages', 'bakery',\n                                                'cleaning', 'dairy'],\n                                  var_name = \"item_category\",\n                                  value_name = \"sales\").copy()\n\nlong_format.head()\n\n        date item_category   sales\n0 2015-01-02     beverages   794.0\n1 2015-01-03     beverages   938.0\n2 2015-01-04     beverages   574.0\n3 2015-01-05     beverages  1299.0\n4 2015-01-06     beverages  1028.0\n\n\n\nplot_panel(long_format)"
  },
  {
    "objectID": "topics/time_series_decomposition.html#seasonality",
    "href": "topics/time_series_decomposition.html#seasonality",
    "title": "Time series decomposition",
    "section": "Seasonality",
    "text": "Seasonality"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html",
    "href": "topics/tabularizing_time_series/forecasting_demo.html",
    "title": "Forecasting demonstration - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#seasonality",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#seasonality",
    "title": "Forecasting demonstration - EDA",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to recurring patterns or cycles in data that occur at regular intervals, such as daily or yearly trends. By grouping the data by time of day (i.e., the hour) and averaging across all observations for that time, we can observe how pollutant concentrations (like CO_sensor) vary over the course of a typical day. This allows us to detect intra-day seasonality, which is particularly relevant for environmental and pollution data that may exhibit daily cycles due to human activity or environmental factors.\n\nhours_con = impute_df.groupby(impute_df.index.time)[\n  \"CO_sensor\"].mean().reset_index()\n\nhours_con.plot(x = 'index', y = 'CO_sensor', legend = False)\nplt.title(\"Pollutant concentration over day time\")\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo_updated.html",
    "href": "topics/tabularizing_time_series/forecasting_demo_updated.html",
    "title": "Forecasting demonstration",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo_updated.html#missing-values-handling",
    "href": "topics/tabularizing_time_series/forecasting_demo_updated.html#missing-values-handling",
    "title": "Forecasting demonstration",
    "section": "Missing values handling",
    "text": "Missing values handling\nTo handle missing data in a time series, we first ensure that our data is uniformly spaced by converting the Date_Time index to an hourly frequency using the .asfreq(\"1h\") method. This step introduces explicit gaps for any missing data points, making them easier to detect. This is important in time series analysis because irregular time steps can mislead algorithms, and most forecasting models require data to be at regular intervals.\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n\nIn this block, we handle missing values by forward-filling the gaps (.ffill()), which propagates the last observed value forward until a new valid observation is encountered. This is a common imputation technique for time series data, especially in scenarios where missing values are sparse or values don’t drastically change within short time frames.\nWe create new columns (e.g., CO_sensor_imputed, RH_imputed) that store the imputed data, while retaining the original columns for comparison and visualization of the missing values.\nNext, we will overlay the imputed values onto the original time series to visually highlight where data was missing and how it was filled.\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  \n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  \n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(\n    ax = ax,legend = False,marker = \".\", color = \"red\", linestyle='None')\n    \n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  \n  plt.tick_params(axis='both', which='major', labelsize=16)\n  \n  plt.xlabel('')  # Disable x-axis label\n  \n  plt.ylabel('')  # Disable y-axis label\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe purpose of this code block is to visualize the missing data points and the corresponding imputed values. The original data is plotted as a line, and any missing data points that were imputed are marked with red dots. This method of visual comparison allows us to easily inspect where the forward fill occurred and ensures that the imputation technique was applied correctly without distorting the overall time series pattern."
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo_updated.html#seasonality",
    "href": "topics/tabularizing_time_series/forecasting_demo_updated.html#seasonality",
    "title": "Forecasting demonstration",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to recurring patterns or cycles in data that occur at regular intervals, such as daily or yearly trends. By grouping the data by time of day (i.e., the hour) and averaging across all observations for that time, we can observe how pollutant concentrations (like CO_sensor) vary over the course of a typical day. This allows us to detect intra-day seasonality, which is particularly relevant for environmental and pollution data that may exhibit daily cycles due to human activity or environmental factors.\n\nhours_con = impute_df.groupby(impute_df.index.time)[\n  \"CO_sensor\"].mean().reset_index()\n\nhours_con.plot(x = 'index', y = 'CO_sensor', legend = False)\nplt.title(\"Pollutant concentration over day time\")\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#missing-values-handling",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#missing-values-handling",
    "title": "Forecasting demonstration",
    "section": "Missing values handling",
    "text": "Missing values handling\n++ we’ll highlight the missing values by explicitly converting the frequency of the index to hourly frequency. That will introduce missing points\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n  \n\n++ explain that we’ll overlay imputed over missing values in order to highlight the missing values present in the data\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(ax = ax,\n  legend = False, marker = \".\", color = \"red\", linestyle='None')\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo_updated.html#data-loading",
    "href": "topics/tabularizing_time_series/forecasting_demo_updated.html#data-loading",
    "title": "Forecasting demonstration",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis.\n\nfor temp_col in air_quality_df.columns.values:\n  air_quality_df[temp_col].plot(figsize = (20,6))\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we plot each of the columns in the dataset (CO_sensor and RH) to visually explore the time series data. These plots provide a general understanding of how pollutant levels (CO_sensor) and humidity (RH) fluctuate over time. Larger trends, spikes, or patterns such as seasonality may already be visible, and such visualizations are a useful first step before deeper analysis."
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html",
    "title": "Forecasting demonstration - feature engineering",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#data-loading",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#data-loading",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#time-related-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#time-related-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#lag-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#lag-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#data-loading",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#data-loading",
    "title": "Forecasting demonstration - EDA",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis.\n\nfor temp_col in air_quality_df.columns.values:\n  air_quality_df[temp_col].plot(figsize = (20,6))\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we plot each of the columns in the dataset (CO_sensor and RH) to visually explore the time series data. These plots provide a general understanding of how pollutant levels (CO_sensor) and humidity (RH) fluctuate over time. Larger trends, spikes, or patterns such as seasonality may already be visible, and such visualizations are a useful first step before deeper analysis."
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#missing-values",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#missing-values",
    "title": "Forecasting demonstration - EDA",
    "section": "Missing values",
    "text": "Missing values\nTo handle missing data in a time series, we first ensure that our data is uniformly spaced by converting the Date_Time index to an hourly frequency using the .asfreq(\"1h\") method. This step introduces explicit gaps for any missing data points, making them easier to detect. This is important in time series analysis because irregular time steps can mislead algorithms, and most forecasting models require data to be at regular intervals.\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n\nIn this block, we handle missing values by forward-filling the gaps (.ffill()), which propagates the last observed value forward until a new valid observation is encountered. This is a common imputation technique for time series data, especially in scenarios where missing values are sparse or values don’t drastically change within short time frames.\nWe create new columns (e.g., CO_sensor_imputed, RH_imputed) that store the imputed data, while retaining the original columns for comparison and visualization of the missing values.\nNext, we will overlay the imputed values onto the original time series to visually highlight where data was missing and how it was filled.\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  \n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  \n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(\n    ax = ax,legend = False,marker = \".\", color = \"red\", linestyle='None')\n    \n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  \n  plt.tick_params(axis='both', which='major', labelsize=16)\n  \n  plt.xlabel('')  # Disable x-axis label\n  \n  plt.ylabel('')  # Disable y-axis label\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe purpose of this code block is to visualize the missing data points and the corresponding imputed values. The original data is plotted as a line, and any missing data points that were imputed are marked with red dots. This method of visual comparison allows us to easily inspect where the forward fill occurred and ensures that the imputation technique was applied correctly without distorting the overall time series pattern."
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#window-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#window-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n  (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n  (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n  ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "topics/tabularizing_time_series/temp.html",
    "href": "topics/tabularizing_time_series/temp.html",
    "title": "Forecasting demonstration - feature engineering",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/tabularizing_time_series/temp.html#data-loading",
    "href": "topics/tabularizing_time_series/temp.html#data-loading",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/tabularizing_time_series/temp.html#time-related-features",
    "href": "topics/tabularizing_time_series/temp.html#time-related-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "topics/tabularizing_time_series/temp.html#lag-features",
    "href": "topics/tabularizing_time_series/temp.html#lag-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "topics/tabularizing_time_series/temp.html#window-features",
    "href": "topics/tabularizing_time_series/temp.html#window-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n                                                                                                                 (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n                                                                                                                 (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n                                                                                                                 ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "topics/tabularizing_time_series/temp.html#periodic-features",
    "href": "topics/tabularizing_time_series/temp.html#periodic-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclycal_df = cyclical.fit_transform(calendar_df)\n\ncyclycal_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#periodic-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#periodic-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclical_df = cyclical.fit_transform(calendar_df)\n\ncyclical_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecaster_demo.html",
    "href": "topics/tabularizing_time_series/forecaster_demo.html",
    "title": "Forecasting demonstration - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nThis is a demonstration of forecasting comparing two models. The data is the preprocessed data set after feature engineering"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecaster_demo.html#data-loading",
    "href": "topics/tabularizing_time_series/forecaster_demo.html#data-loading",
    "title": "Forecasting demonstration - EDA",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \\\n\"\\\\data\\\\air_quality_processed_df.csv\"\n\nair_quality_processed_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_processed_df.index = pd.to_datetime(air_quality_processed_df.index)"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecaster_demo.html#compare-models",
    "href": "topics/tabularizing_time_series/forecaster_demo.html#compare-models",
    "title": "Forecasting demonstration - EDA",
    "section": "Compare models",
    "text": "Compare models\n\n\ny_vec = air_quality_processed_df[\"CO_sensor\"]\n\nX_mat = air_quality_processed_df.drop(\"CO_sensor\", axis = 1)\n\nX_mat_train = X_mat.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\ny_vec_train = y_vec.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\nX_mat_test = X_mat.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\ny_vec_test = y_vec.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\n\nNaive model\n++ explain that as naive model we often take the last known value (the previous hour value in our case)\n\n\nnaive_forecast = air_quality_processed_df.loc[air_quality_processed_df.index &gt; pd.to_datetime(\"2005-03-04\")][\"CO_sensor_lag_1\"]\n\n\n\nLinear regression\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_mat_train, y_vec_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nlin_reg_forecast = lin_reg.predict(X_mat_test)\n\n\n\nRandom forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrand_forest = RandomForestRegressor(\n    n_estimators=50,\n    max_depth=3,\n    random_state=0,\n)\n\nrand_forest.fit(X_mat_train, y_vec_train)\n\nRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0) \n\n\nrand_forest_forecast = lin_reg.predict(X_mat_test)\n\n\n\nEvaluate models\n\nfrom sklearn.metrics import root_mean_squared_error\n\n\nprint(f\"Naive forecat error is {root_mean_squared_error(naive_forecast, y_vec_test)}\")\n\nNaive forecat error is 104.08288851736968\n\nprint(f\"Linear regression error is {root_mean_squared_error(lin_reg_forecast, y_vec_test)}\")\n\nLinear regression error is 86.8976149413457\n\nprint(f\"Random forest error is {root_mean_squared_error(rand_forest_forecast, y_vec_test)}\")\n\nRandom forest error is 86.8976149413457"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#save-processed-data",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#save-processed-data",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Save processed data",
    "text": "Save processed data\nIn the final step of preprocessing, we must address the missing values that have been introduced during feature engineering, particularly in the creation of lag and window features. These missing values arise because, for example, a 24-hour lag feature requires data from 24 hours prior, which is unavailable for the first 24 observations. Similarly, window features, such as rolling averages, rely on past data over a specified period, leading to NA values at the start of the series where insufficient prior data exists.\nThere are generally two approaches to handle these missing values: imputation or deletion. Imputation involves replacing missing values with substitutes, such as the mean or median of the available data. However, given that the number of missing values is relatively small in this case, and since dropping rows with missing data simplifies the process, we will opt to drop the rows containing missing values. This ensures that the final dataset is complete and ready for modeling without introducing any potential bias from imputation.\nOnce the missing values are handled, the processed dataset, which includes the original data along with the engineered features (such as lag, window, and cyclical features), is saved to a CSV file for further use.\n\n\nprocessed_df = pd.concat([air_quality_df, calendar_df,lag_features_df, cyclical_df], axis = 1)\n\nprocessed_df.dropna(inplace = True)\n\nprocessed_df.to_csv(os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \"\\\\data\\\\air_quality_processed_df.csv\")"
  },
  {
    "objectID": "temp.html",
    "href": "temp.html",
    "title": "Forecasting one period (step) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast the next period (step) using a pipeline approach. The pipeline encapsulates all preprocessing operations such as feature engineering, imputation, and feature selection into a single streamlined process. This allows for consistency and efficiency when handling time series data. It is crucial to ensure that the explanatory features (the X matrix) and the target feature (the y vector) are properly aligned in time. Misalignment can lead to look-ahead bias, where future information is inappropriately used in the model training phase, resulting in overoptimistic performance estimates. Careful attention is also required to avoid data leakage, ensuring that the model does not have access to information from the future when forecasting.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "temp.html#data-loading",
    "href": "temp.html#data-loading",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "temp.html#time-related-features",
    "href": "temp.html#time-related-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "temp.html#lag-features",
    "href": "temp.html#lag-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "temp.html#window-features",
    "href": "temp.html#window-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n  (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n  (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n  ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "temp.html#periodic-features",
    "href": "temp.html#periodic-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclical_df = cyclical.fit_transform(calendar_df)\n\ncyclical_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "temp.html#save-processed-data",
    "href": "temp.html#save-processed-data",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Save processed data",
    "text": "Save processed data\nIn the final step of preprocessing, we must address the missing values that have been introduced during feature engineering, particularly in the creation of lag and window features. These missing values arise because, for example, a 24-hour lag feature requires data from 24 hours prior, which is unavailable for the first 24 observations. Similarly, window features, such as rolling averages, rely on past data over a specified period, leading to NA values at the start of the series where insufficient prior data exists.\nThere are generally two approaches to handle these missing values: imputation or deletion. Imputation involves replacing missing values with substitutes, such as the mean or median of the available data. However, given that the number of missing values is relatively small in this case, and since dropping rows with missing data simplifies the process, we will opt to drop the rows containing missing values. This ensures that the final dataset is complete and ready for modeling without introducing any potential bias from imputation.\nOnce the missing values are handled, the processed dataset, which includes the original data along with the engineered features (such as lag, window, and cyclical features), is saved to a CSV file for further use.\n\n\nprocessed_df = pd.concat([air_quality_df, calendar_df,lag_features_df, cyclical_df], axis = 1)\n\nprocessed_df.dropna(inplace = True)\n\nprocessed_df.to_csv(os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \"\\\\data\\\\air_quality_processed_df.csv\")"
  },
  {
    "objectID": "topics/intro/eda.html",
    "href": "topics/intro/eda.html",
    "title": "Exploratory Data Analysis - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/intro/eda.html#introduction-to-exploratory-data-analysis-eda",
    "href": "topics/intro/eda.html#introduction-to-exploratory-data-analysis-eda",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Introduction to Exploratory Data Analysis (EDA)",
    "text": "Introduction to Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) is a crucial step in any data science or time series analysis process. It involves visually and statistically summarizing the key characteristics of a dataset to gain insights into its structure, underlying patterns, and potential issues such as missing data or outliers. In this short tutorial, we will focus on performing EDA on time series data. Specifically, we will identify and handle missing values and explore seasonality—one of the most common characteristics in time series data. Seasonality refers to patterns that repeat at regular intervals, such as daily, weekly, or yearly trends, and detecting it is essential for accurate forecasting."
  },
  {
    "objectID": "topics/intro/eda.html#data-loading",
    "href": "topics/intro/eda.html#data-loading",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/intro/eda.html#data-visualization",
    "href": "topics/intro/eda.html#data-visualization",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nfor temp_col in air_quality_df.columns.values:\n  air_quality_df[temp_col].plot(figsize = (20,6))\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we plot each of the columns in the dataset (CO_sensor and RH) to visually explore the time series data. These plots provide a general understanding of how pollutant levels (CO_sensor) and humidity (RH) fluctuate over time. Larger trends, spikes, or patterns such as seasonality may already be visible, and such visualizations are a useful first step before deeper analysis."
  },
  {
    "objectID": "topics/intro/eda.html#handling-missing-values",
    "href": "topics/intro/eda.html#handling-missing-values",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nIn time series analysis, it is common to encounter missing data due to sensor malfunctions or recording errors. Properly handling missing data is crucial because forecasting models often assume data points are spaced at regular intervals. To address this, we first ensure the data is uniformly spaced by converting the Date_Time index to an hourly frequency. This allows us to easily detect missing data points and fill the gaps accordingly.\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n\nIn this block, we address missing values by using forward-filling (ffill()), which propagates the last valid observation forward until a new non-missing value is encountered. This technique works well when the missing values are sparse or when we expect the data to remain stable over short intervals. We create new columns, such as CO_sensor_imputed and RH_imputed, to store the imputed values, allowing us to compare them with the original data and ensure that no critical information is lost."
  },
  {
    "objectID": "topics/intro/eda.html#visualizing-imputed-values",
    "href": "topics/intro/eda.html#visualizing-imputed-values",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Visualizing Imputed Values",
    "text": "Visualizing Imputed Values\nTo ensure that our imputation strategy has been applied correctly, we will visually compare the original data with the imputed values. This comparison will highlight where missing values were filled and help us verify that the imputation did not introduce any distortions into the time series.\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  \n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  \n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(\n    ax = ax,legend = False,marker = \".\", color = \"red\", linestyle='None')\n    \n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  \n  plt.tick_params(axis='both', which='major', labelsize=16)\n  \n  plt.xlabel('')  # Disable x-axis label\n  \n  plt.ylabel('')  # Disable y-axis label\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this step, we overlay the imputed values onto the original time series. The original data is represented by a continuous line, while missing values that were filled via forward-fill are marked with red dots. This side-by-side comparison helps us visually assess the areas where imputation occurred and check whether it was applied appropriately without affecting the overall trend of the data."
  },
  {
    "objectID": "topics/intro/eda.html#seasonality",
    "href": "topics/intro/eda.html#seasonality",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality is a recurring pattern in data that occurs at regular intervals, often influenced by natural or social processes. In the context of air quality, pollutant levels such as carbon monoxide may follow daily or weekly cycles due to human activities like traffic or industrial operations. To detect seasonality, we group the data by the time of day (i.e., by the hour) and calculate the average pollutant levels across all observations for each hour. This provides a clear picture of how pollutant concentrations vary throughout the day.\n\nhours_con = impute_df.groupby(impute_df.index.time)[\n  \"CO_sensor\"].mean().reset_index()\n\nhours_con.plot(x = 'index', y = 'CO_sensor', legend = False)\nplt.title(\"Pollutant concentration over day time\")\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this final step, we plot the average CO_sensor concentration over the course of a typical day to visualize the intra-day seasonality. This chart allows us to observe how carbon monoxide levels fluctuate during different times of the day, potentially reflecting periods of higher traffic or other factors that influence air quality. Understanding these seasonal patterns is key for making accurate predictions and taking appropriate action in environmental monitoring."
  },
  {
    "objectID": "topics/intro/feature_egineering.html",
    "href": "topics/intro/feature_egineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nFeature engineering is the process of transforming and creating new input variables, or features, from raw data to improve the performance of predictive models. It involves converting raw data into meaningful inputs that capture underlying patterns or relationships useful for forecasting. In the context of time series forecasting, the choice of features can significantly impact the model’s ability to make accurate predictions. For example, simply using the raw values of a time series may not be enough for the model to capture complex temporal dynamics such as trends or seasonality. Therefore, transforming features or creating new ones like time-based features, lag features, or cyclical patterns is crucial. This tutorial will demonstrate several types of feature transformations, including time-related features, lag features, window (rolling) features, and periodic features. This is only a short demonstration, each type will be covered in detail in its respective section."
  },
  {
    "objectID": "topics/intro/feature_egineering.html#data-loading",
    "href": "topics/intro/feature_egineering.html#data-loading",
    "title": "Feature Engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/intro/feature_egineering.html#time-related-features",
    "href": "topics/intro/feature_egineering.html#time-related-features",
    "title": "Feature Engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#lag-features",
    "href": "topics/intro/feature_egineering.html#lag-features",
    "title": "Feature Engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#window-features",
    "href": "topics/intro/feature_egineering.html#window-features",
    "title": "Feature Engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n  (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n  (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n  ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#periodic-features",
    "href": "topics/intro/feature_egineering.html#periodic-features",
    "title": "Feature Engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclical_df = cyclical.fit_transform(calendar_df)\n\ncyclical_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#save-processed-data",
    "href": "topics/intro/feature_egineering.html#save-processed-data",
    "title": "Feature Engineering",
    "section": "Save processed data",
    "text": "Save processed data\nIn the final step of preprocessing, we must address the missing values that have been introduced during feature engineering, particularly in the creation of lag and window features. These missing values arise because, for example, a 24-hour lag feature requires data from 24 hours prior, which is unavailable for the first 24 observations. Similarly, window features, such as rolling averages, rely on past data over a specified period, leading to NA values at the start of the series where insufficient prior data exists.\nThere are generally two approaches to handle these missing values: imputation or deletion. Imputation involves replacing missing values with substitutes, such as the mean or median of the available data. However, given that the number of missing values is relatively small in this case, and since dropping rows with missing data simplifies the process, we will opt to drop the rows containing missing values. This ensures that the final dataset is complete and ready for modeling without introducing any potential bias from imputation.\nOnce the missing values are handled, the processed dataset, which includes the original data along with the engineered features (such as lag, window, and cyclical features), is saved to a CSV file for further use.\n\n\nprocessed_df = pd.concat([air_quality_df, calendar_df,lag_features_df, cyclical_df], axis = 1)\n\nprocessed_df.dropna(inplace = True)\n\nprocessed_df.to_csv(os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \"\\\\data\\\\air_quality_processed_df.csv\")"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html",
    "href": "topics/intro/forecaster_demo.html",
    "title": "Forecasting demonstration",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nIn this section, we provide a brief demonstration of a forecasting problem where we compare the performance of two models, a Linear Regression model and a Random Forest Regressor, along with a naive benchmark model. The naive model uses the last known value as the forecast for the next time step. This demonstration focuses on predicting air quality (specifically CO sensor data) based on historical features. The purpose of this exercise is to showcase how different models perform in terms of forecast accuracy, evaluated using Root Mean Squared Error (RMSE), a common metric for assessing regression models."
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#data-loading",
    "href": "topics/intro/forecaster_demo.html#data-loading",
    "title": "Forecasting demonstration",
    "section": "Data loading",
    "text": "Data loading\nWe start by loading a preprocessed dataset that has already undergone feature engineering. The dataset contains time series data related to air quality, indexed by date and time. The file is loaded as a Pandas DataFrame and is indexed with the Date_Time column.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \\\n\"\\\\data\\\\air_quality_processed_df.csv\"\n\nair_quality_processed_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_processed_df.index = pd.to_datetime(air_quality_processed_df.index)"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#compare-models",
    "href": "topics/intro/forecaster_demo.html#compare-models",
    "title": "Forecasting demonstration",
    "section": "Compare models",
    "text": "Compare models\nNext, we split the dataset into training and testing sets. The training set contains data up to a specific cutoff date (2005-03-04), and the testing set contains data after that date. The target variable (the one we aim to predict) is CO_sensor, which measures the concentration of carbon monoxide detected by the sensor. The feature matrix X_mat contains all the other predictor variables.\n\n\ny_vec = air_quality_processed_df[\"CO_sensor\"]\n\nX_mat = air_quality_processed_df.drop(\"CO_sensor\", axis = 1)\n\nX_mat_train = X_mat.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\ny_vec_train = y_vec.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\nX_mat_test = X_mat.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\ny_vec_test = y_vec.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\n\nNaive model\nThe naive model is often used as a simple benchmark in forecasting problems. It assumes that the best prediction for the next time step is simply the last observed value from the previous time step. In this case, we use the CO sensor data from the previous hour (CO_sensor_lag_1) as our naive forecast.\n\n\nnaive_forecast = air_quality_processed_df.loc[air_quality_processed_df.index &gt; pd.to_datetime(\"2005-03-04\")][\"CO_sensor_lag_1\"]\n\n\n\nLinear regression\nThe first machine learning model we use is Linear Regression. This model attempts to find a linear relationship between the predictor variables and the target variable. Once trained on the training dataset, we use it to make predictions on the test set.\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_mat_train, y_vec_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nlin_reg_forecast = lin_reg.predict(X_mat_test)\n\n\n\nRandom forest\nThe second model is a Random Forest Regressor, an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and avoid overfitting. In this case, we set the number of trees (n_estimators) to 50 and the maximum depth of each tree to 3.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrand_forest = RandomForestRegressor(\n    n_estimators=50,\n    max_depth=3,\n    random_state=0,\n)\n\nrand_forest.fit(X_mat_train, y_vec_train)\n\nRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0) \n\n\nrand_forest_forecast = rand_forest.predict(X_mat_test)\n\n\n\nEvaluate models\nTo evaluate the performance of the models, we calculate the Root Mean Squared Error (RMSE) for each forecast. RMSE is a widely used metric in forecasting problems because it provides a measure of how well a model’s predictions match the actual values, with lower values indicating better performance.\n\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef root_mean_squared_error(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nprint(f\"Naive forecast error is {root_mean_squared_error(naive_forecast, y_vec_test)}\")\n\nNaive forecast error is 104.08288851736968\n\nprint(f\"Linear regression error is {root_mean_squared_error(lin_reg_forecast, y_vec_test)}\")\n\nLinear regression error is 86.89761494134571\n\nprint(f\"Random forest error is {root_mean_squared_error(rand_forest_forecast, y_vec_test)}\")\n\nRandom forest error is 101.07981790929384\n\n\nIn summary, this demonstration illustrates how different models can be applied to a time series forecasting problem. By comparing a naive model, a linear regression model, and a random forest model, we can observe the strengths and weaknesses of each approach in terms of their forecasting accuracy, as measured by RMSE."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\n\n\nTime Series Decomposition"
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html",
    "href": "topics/forecasting/forecasting_pipine.html",
    "title": "Forecasting Pipeline",
    "section": "",
    "text": "import pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nOur goal in this pipeline is to transform the data by applying a series of feature engineering techniques to prepare it for time series forecasting. We will perform the following tasks:\nThe feature engineering will be performed using the feature_engine library, which offers an easy-to-use interface to build these transformations. Our goal is to encapsulate all operations in one pipeline for easier reproducibility and maintainability."
  },
  {
    "objectID": "temp.html#pipeline",
    "href": "temp.html#pipeline",
    "title": "Forecasting Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\nWe now pack all the steps into a single pipeline. Pipelines allow us to apply a sequence of transformations to the data in a well-structured and reproducible way. This makes the data preparation process more efficient and less error-prone, especially when scaling up or iterating over different models or datasets.\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\npipe_processed_df = trans_pipe.fit_transform(raw_df.copy())\n\nprint(f\"The processed df is equal to pipe_processed_df : {pipe_processed_df.equals(processed_df)}\")\n\nThe processed df is equal to pipe_processed_df : True\n\n\nHere, we’ve consolidated the entire feature engineering process into a single Pipeline object, which includes:\n\nDate and time feature extraction\nLag features\nWindow features\nCyclical features\nDropping missing values\nRemoving original features\n\nThis pipeline can be applied to any new dataset that follows a similar structure, ensuring that the feature engineering process is both scalable and consistent across different time periods or datasets. Additionally, this approach enhances model reproducibility and ease of deployment. After fitting and transforming the raw dataset through the pipeline, we confirm that the output matches the manually processed DataFrame."
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html",
    "href": "topics/forecasting/forecasting_one_step.html",
    "title": "Forecasting one period (step) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast the next period (step) using a pipeline approach. The pipeline encapsulates all preprocessing operations such as feature engineering, imputation, and feature selection into a single streamlined process. This allows for consistency and efficiency when handling time series data. It is crucial to ensure that the explanatory features (the X matrix) and the target feature (the y vector) are properly aligned in time. Misalignment can lead to look-ahead bias, where future information is inappropriately used in the model training phase, resulting in overoptimistic performance estimates. Careful attention is also required to avoid data leakage, ensuring that the model does not have access to information from the future when forecasting.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_one_step.html#train-and-test-split",
    "title": "Forecasting one period (step) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]"
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html#date-time-features",
    "href": "topics/forecasting/forecasting_pipine.html#date-time-features",
    "title": "Forecasting Pipeline",
    "section": "Date time features",
    "text": "Date time features\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nprocessed_df = date_time_feat.fit_transform(raw_df.copy())\n\n\nprocessed_df.head()\n\n                     CO_sensor    RH  month  ...  day_of_month  hour  weekend\nDate_Time                                    ...                             \n2004-04-04 00:00:00     1224.0  56.5      4  ...             4     0        1\n2004-04-04 01:00:00     1215.0  59.2      4  ...             4     1        1\n2004-04-04 02:00:00     1115.0  62.4      4  ...             4     2        1\n2004-04-04 03:00:00     1124.0  65.0      4  ...             4     3        1\n2004-04-04 04:00:00     1028.0  65.3      4  ...             4     4        1\n\n[5 rows x 8 columns]\n\n\nIn this step, we are extracting date and time features from the index, such as the month, day of the week, and hour of the day. This helps us leverage the temporal structure of the data in subsequent modeling steps. For example, the day of the week or the hour could influence air quality, so extracting such features allows us to include this information in the model."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html#lag-features",
    "href": "topics/forecasting/forecasting_pipine.html#lag-features",
    "title": "Forecasting Pipeline",
    "section": "Lag features",
    "text": "Lag features\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\nprocessed_df = lag_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"lag\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_lag_1h  RH_lag_1h  CO_sensor_lag_24h  RH_lag_24h\nDate_Time                                                                      \n2004-04-04 00:00:00               NaN        NaN                NaN         NaN\n2004-04-04 01:00:00            1224.0       56.5                NaN         NaN\n2004-04-04 02:00:00            1215.0       59.2                NaN         NaN\n2004-04-04 03:00:00            1115.0       62.4                NaN         NaN\n2004-04-04 04:00:00            1124.0       65.0                NaN         NaN\n\n\nHere, we create lag features for the CO_sensor and RH (Relative Humidity) variables. Lagging is a powerful technique in time series forecasting as it allows us to capture information from previous time steps. In this case, we are creating two types of lags: one for 1 hour prior and another for 24 hours prior, which will help the model learn patterns that evolve over both short and longer time scales."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html#window-features",
    "href": "topics/forecasting/forecasting_pipine.html#window-features",
    "title": "Forecasting Pipeline",
    "section": "Window features",
    "text": "Window features\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\nprocessed_df = window_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"win\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_window_3h_mean  RH_window_3h_mean\nDate_Time                                                       \n2004-04-04 00:00:00                       NaN                NaN\n2004-04-04 01:00:00               1224.000000          56.500000\n2004-04-04 02:00:00               1219.500000          57.850000\n2004-04-04 03:00:00               1184.666667          59.366667\n2004-04-04 04:00:00               1151.333333          62.200000\n\n\nIn this step, we generate window features. These features capture rolling window statistics over a 3-hour window for the CO_sensor and RH variables, calculated at 1-hour intervals. This provides insight into the short-term trends or fluctuations in the data, as moving averages or other summary statistics over the window can help smooth out noisy data and emphasize underlying patterns."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html#cyclical-features",
    "href": "topics/forecasting/forecasting_pipine.html#cyclical-features",
    "title": "Forecasting Pipeline",
    "section": "Cyclical features",
    "text": "Cyclical features\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\nprocessed_df = cyclical_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"month\" or \"hour\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor    RH  month  ...  month_cos  hour_sin  hour_cos\nDate_Time                                    ...                               \n2004-04-04 00:00:00     1224.0  56.5      4  ...       -0.5  0.000000  1.000000\n2004-04-04 01:00:00     1215.0  59.2      4  ...       -0.5  0.269797  0.962917\n2004-04-04 02:00:00     1115.0  62.4      4  ...       -0.5  0.519584  0.854419\n2004-04-04 03:00:00     1124.0  65.0      4  ...       -0.5  0.730836  0.682553\n2004-04-04 04:00:00     1028.0  65.3      4  ...       -0.5  0.887885  0.460065\n\n[5 rows x 18 columns]\n\n\nCertain features, like month and hour, exhibit cyclical behavior (e.g., after December comes January, and after 23:00 comes 00:00). By converting these features into cyclical (sin and cos) representations, we ensure that the model properly understands these cyclic relationships. This prevents the model from interpreting consecutive values as linearly distant when they are, in fact, close (e.g., December and January)."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html#missing-values-and-data-leakage-treatment",
    "href": "topics/forecasting/forecasting_pipine.html#missing-values-and-data-leakage-treatment",
    "title": "Forecasting Pipeline",
    "section": "Missing values and data leakage treatment",
    "text": "Missing values and data leakage treatment\n\n\nna_drop = DropMissingData()\n\nprocessed_df = na_drop.fit_transform(processed_df.copy())\n\nAfter feature engineering, we may have introduced missing values, especially with techniques like lagging and windowing, which require previous data points. Therefore, we use DropMissingData to remove rows with missing values, ensuring a clean dataset for subsequent modeling.\nAt this stage, we have created a data frame of explanatory variables (X_mat). It is crucial to avoid data leakage (or “look-ahead bias”) in time series forecasting. This occurs when information from the future is unintentionally used to predict past events. To prevent this, we need to remove the original features (like CO_sensor and RH) after extracting all the required information through our feature engineering steps.\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\nprocessed_df = drop_feat.fit_transform(processed_df.copy())\n\nprocessed_df.head()\n\n                     month  week  day_of_week  ...  month_cos  hour_sin  hour_cos\nDate_Time                                      ...                               \n2004-04-05 00:00:00      4    15            0  ...       -0.5  0.000000  1.000000\n2004-04-05 01:00:00      4    15            0  ...       -0.5  0.269797  0.962917\n2004-04-05 02:00:00      4    15            0  ...       -0.5  0.519584  0.854419\n2004-04-05 03:00:00      4    15            0  ...       -0.5  0.730836  0.682553\n2004-04-05 04:00:00      4    15            0  ...       -0.5  0.887885  0.460065\n\n[5 rows x 16 columns]\n\n\nFinally, we drop the original features (CO_sensor and RH) from the dataset. These original columns have already contributed their information through lagged, windowed, and cyclical features, so retaining them would lead to redundancy or potential data leakage."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipine.html#pipeline",
    "href": "topics/forecasting/forecasting_pipine.html#pipeline",
    "title": "Forecasting Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\nWe now pack all the steps into a single pipeline. Pipelines allow us to apply a sequence of transformations to the data in a well-structured and reproducible way. This makes the data preparation process more efficient and less error-prone, especially when scaling up or iterating over different models or datasets.\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\npipe_processed_df = trans_pipe.fit_transform(raw_df.copy())\n\nprint(f\"The processed df is equal to pipe_processed_df : {pipe_processed_df.equals(processed_df)}\")\n\nThe processed df is equal to pipe_processed_df : True\n\n\nHere, we’ve consolidated the entire feature engineering process into a single Pipeline object, which includes:\n\nDate and time feature extraction\nLag features\nWindow features\nCyclical features\nDropping missing values\nRemoving original features\n\nThis pipeline can be applied to any new dataset that follows a similar structure, ensuring that the feature engineering process is both scalable and consistent across different time periods or datasets. Additionally, this approach enhances model reproducibility and ease of deployment. After fitting and transforming the raw dataset through the pipeline, we confirm that the output matches the manually processed DataFrame."
  },
  {
    "objectID": "temp.html#train-and-test-split",
    "href": "temp.html#train-and-test-split",
    "title": "Forecasting one period (step) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]"
  },
  {
    "objectID": "temp.html#preprocess-data",
    "href": "temp.html#preprocess-data",
    "title": "Forecasting one period (step) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.fit_transform(X_test.copy())\n\nDuring preprocessing, we apply transformations that handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the training features but remain in the target vector. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the training and test target vectors based on the updated index of the processed feature sets.\n\nprint(y_train.shape)\n\n(7654,)\n\ny_train = y_train.loc[X_train_processed.index]\n\nprint(y_train.shape)\n\n(7426,)\n\n\ny_test = y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data.\n\nlasso_model = Lasso()\n\nlasso_model.fit(X_train_processed, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_vec = lasso_model.predict(X_test_processed)\n\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\nrmse = np.round(root_mean_squared_error(y_test, predictions_vec), 4)\n\nprint(f\"The root mean squared error on the test set is {rmse}\")\n\nThe root mean squared error on the test set is 86.7041"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#pipeline",
    "href": "topics/forecasting/forecasting_one_step.html#pipeline",
    "title": "Forecasting one period (step) ahead",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#preprocess-data",
    "href": "topics/forecasting/forecasting_one_step.html#preprocess-data",
    "title": "Forecasting one period (step) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.fit_transform(X_test.copy())\n\nDuring preprocessing, we apply transformations that handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the training features but remain in the target vector. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the training and test target vectors based on the updated index of the processed feature sets.\n\nprint(y_train.shape)\n\n(7654,)\n\ny_train = y_train.loc[X_train_processed.index]\n\nprint(y_train.shape)\n\n(7426,)\n\n\ny_test = y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#prediction",
    "href": "topics/forecasting/forecasting_one_step.html#prediction",
    "title": "Forecasting one period (step) ahead",
    "section": "Prediction",
    "text": "Prediction\n\nlasso_model = Lasso()\n\nlasso_model.fit(X_train_processed, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_vec = lasso_model.predict(X_test_processed)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#evaluation",
    "href": "topics/forecasting/forecasting_one_step.html#evaluation",
    "title": "Forecasting one period (step) ahead",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\nrmse = np.round(root_mean_squared_error(y_test, predictions_vec), 4)\n\nprint(f\"The root mean squared error on the test set is {rmse}\")\n\nThe root mean squared error on the test set is 86.7041"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\n++ Here, we want to demonstrate how to forecast multiple period (step) using a recursive approach.The recursive approach entails forecasting step by step: forecasting the next step, updating the data required to produce (engineer) predictve features and than making the next forecast based on the updated data.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#pipeline",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#pipeline",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"12h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_original_features\",drop_feat),\n   (\"drop_missing_values\",na_drop)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_original_features\",drop_feat),\n   (\"drop_missing_values\",na_drop)\n  ]\n  \n)\ndel date_time_feat, lag_feat, window_feat, cyclical_feat, drop_feat,na_drop"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#train-and-test-split",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 12 hours, so we need to ensure that the test set includes the first forecasting point and at least 12 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(12)]\n\nY_train = raw_df.loc[raw_df.index &lt; split_point,[\"CO_sensor\", \"RH\"]]\n\nY_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(12),\n                   [\"CO_sensor\", \"RH\"]]"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#preprocess-data",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#preprocess-data",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nY_train = Y_train.loc[X_train_processed.index]\n\n# X_test_processed = trans_pipe.fit_transform(X_test.copy())\n# \n# Y_test = Y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#prediction",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#prediction",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Prediction",
    "text": "Prediction\n++ explain the for illustration sake we will produce the first two forecast (first and second forecasting points) “manually”. Then we will automate the process of producing the next forecast by updating the input data (using current forecast) and passing the updated data to the model\n\nlasso_model = MultiOutputRegressor(Lasso())\n\nlasso_model.fit(X_train_processed, Y_train)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\n\nFirst point\n\nfirst_forecast_point = pd.Timestamp(\"2005-03-04\")\n\nfirst_input_data = X_test.loc[\n  (X_test.index &gt;= first_forecast_point - pd.offsets.Hour(12)) &\n  (X_test.index &lt; first_forecast_point)\n  ]\n                        \nforecast_row = pd.DataFrame(data = np.nan,\n                            index = [first_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])\n                            \nfirst_input_data = pd.concat([first_input_data.copy(),\n                              forecast_row], axis = 0)\n\nprint(first_input_data)\n\n                     CO_sensor    RH\n2005-03-03 12:00:00     1129.0  78.6\n2005-03-03 13:00:00     1092.0  69.2\n2005-03-03 14:00:00     1096.0  66.0\n2005-03-03 15:00:00     1108.0  70.1\n2005-03-03 16:00:00     1124.0  72.1\n2005-03-03 17:00:00     1216.0  75.7\n2005-03-03 18:00:00     1437.0  80.4\n2005-03-03 19:00:00     1473.0  82.4\n2005-03-03 20:00:00     1396.0  84.0\n2005-03-03 21:00:00     1285.0  83.6\n2005-03-03 22:00:00     1206.0  82.5\n2005-03-03 23:00:00     1179.0  82.0\n2005-03-04 00:00:00        NaN   NaN\n\n\n\nfirst_input_data_processed = trans_pipe.transform(first_input_data)\n\nprint(first_input_data.index.min(),first_input_data.index.max())\n\n2005-03-03 12:00:00 2005-03-04 00:00:00\n\nprint(first_input_data_processed.index.min(),\n      first_input_data_processed.index.max())\n\n2005-03-04 00:00:00 2005-03-04 00:00:00\n\n      \n\nfirst_point_pred = lasso_model.predict(first_input_data_processed)\n\npredictions_df = pd.DataFrame(data = first_point_pred,\n                                index = [first_forecast_point],\n                                columns = [\"CO_sensor\",\"RH\"])\n\n\n\nSecond point\n\n\nsecond_forecast_point = first_forecast_point + pd.offsets.Hour(1)\n\nsecond_input_data = first_input_data.iloc[1:].copy()\n\nsecond_input_data.loc[first_forecast_point] = first_point_pred\n\nsecond_forecast_row = pd.DataFrame(data = np.nan,\n                            index = [second_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])\n\nsecond_input_data = pd.concat([second_input_data.copy(),\n                              second_forecast_row], axis = 0)                      \n\n\n\nsecond_input_data_processed = trans_pipe.transform(second_input_data)\n\nsecond_point_pred = lasso_model.predict(second_input_data_processed)\n\npredictions_df = pd.concat([predictions_df.copy(),\n                            pd.DataFrame(data = second_point_pred,\n                            index = [second_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])])\n\n\nprint(predictions_df)\n\n                       CO_sensor         RH\n2005-03-04 00:00:00  1115.696769  80.345937\n2005-03-04 01:00:00  1060.033356  78.441773"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#evaluation",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#evaluation",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\nrmse_df = []\n\nfor temp_hor in range(Y_test.shape[1]):\n  y_test = Y_test.iloc[:,temp_hor]\n  pred_vec = predictions_mat.iloc[:,temp_hor]\n  rmse = np.round(root_mean_squared_error(y_test, pred_vec), 4)\n  rmse_df.append(pd.DataFrame(columns = [\"horizon\",\"rmse\"],\n                              data = [[temp_hor,rmse]]))\n\nrmse_df = pd.concat(rmse_df, axis = 0)\n\n\nrmse_df.plot(x = \"horizon\", y = \"rmse\")\n\nplt.show()"
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html",
    "href": "topics/forecasting/forecasting_pipeline.html",
    "title": "Forecasting Pipeline",
    "section": "",
    "text": "import pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nOur goal in this pipeline is to transform the data by applying a series of feature engineering techniques to prepare it for time series forecasting. We will perform the following tasks:\nThe feature engineering will be performed using the feature_engine library, which offers an easy-to-use interface to build these transformations. Our goal is to encapsulate all operations in one pipeline for easier reproducibility and maintainability."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#date-time-features",
    "href": "topics/forecasting/forecasting_pipeline.html#date-time-features",
    "title": "Forecasting Pipeline",
    "section": "Date time features",
    "text": "Date time features\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nprocessed_df = date_time_feat.fit_transform(raw_df.copy())\n\n\nprocessed_df.head()\n\n                     CO_sensor    RH  month  ...  day_of_month  hour  weekend\nDate_Time                                    ...                             \n2004-04-04 00:00:00     1224.0  56.5      4  ...             4     0        1\n2004-04-04 01:00:00     1215.0  59.2      4  ...             4     1        1\n2004-04-04 02:00:00     1115.0  62.4      4  ...             4     2        1\n2004-04-04 03:00:00     1124.0  65.0      4  ...             4     3        1\n2004-04-04 04:00:00     1028.0  65.3      4  ...             4     4        1\n\n[5 rows x 8 columns]\n\n\nIn this step, we are extracting date and time features from the index, such as the month, day of the week, and hour of the day. This helps us leverage the temporal structure of the data in subsequent modeling steps. For example, the day of the week or the hour could influence air quality, so extracting such features allows us to include this information in the model."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#lag-features",
    "href": "topics/forecasting/forecasting_pipeline.html#lag-features",
    "title": "Forecasting Pipeline",
    "section": "Lag features",
    "text": "Lag features\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\nprocessed_df = lag_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"lag\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_lag_1h  RH_lag_1h  CO_sensor_lag_24h  RH_lag_24h\nDate_Time                                                                      \n2004-04-04 00:00:00               NaN        NaN                NaN         NaN\n2004-04-04 01:00:00            1224.0       56.5                NaN         NaN\n2004-04-04 02:00:00            1215.0       59.2                NaN         NaN\n2004-04-04 03:00:00            1115.0       62.4                NaN         NaN\n2004-04-04 04:00:00            1124.0       65.0                NaN         NaN\n\n\nHere, we create lag features for the CO_sensor and RH (Relative Humidity) variables. Lagging is a powerful technique in time series forecasting as it allows us to capture information from previous time steps. In this case, we are creating two types of lags: one for 1 hour prior and another for 24 hours prior, which will help the model learn patterns that evolve over both short and longer time scales."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#window-features",
    "href": "topics/forecasting/forecasting_pipeline.html#window-features",
    "title": "Forecasting Pipeline",
    "section": "Window features",
    "text": "Window features\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\nprocessed_df = window_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"win\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_window_3h_mean  RH_window_3h_mean\nDate_Time                                                       \n2004-04-04 00:00:00                       NaN                NaN\n2004-04-04 01:00:00               1224.000000          56.500000\n2004-04-04 02:00:00               1219.500000          57.850000\n2004-04-04 03:00:00               1184.666667          59.366667\n2004-04-04 04:00:00               1151.333333          62.200000\n\n\nIn this step, we generate window features. These features capture rolling window statistics over a 3-hour window for the CO_sensor and RH variables, calculated at 1-hour intervals. This provides insight into the short-term trends or fluctuations in the data, as moving averages or other summary statistics over the window can help smooth out noisy data and emphasize underlying patterns."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#cyclical-features",
    "href": "topics/forecasting/forecasting_pipeline.html#cyclical-features",
    "title": "Forecasting Pipeline",
    "section": "Cyclical features",
    "text": "Cyclical features\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\nprocessed_df = cyclical_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"month\" or \"hour\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor    RH  month  ...  month_cos  hour_sin  hour_cos\nDate_Time                                    ...                               \n2004-04-04 00:00:00     1224.0  56.5      4  ...       -0.5  0.000000  1.000000\n2004-04-04 01:00:00     1215.0  59.2      4  ...       -0.5  0.269797  0.962917\n2004-04-04 02:00:00     1115.0  62.4      4  ...       -0.5  0.519584  0.854419\n2004-04-04 03:00:00     1124.0  65.0      4  ...       -0.5  0.730836  0.682553\n2004-04-04 04:00:00     1028.0  65.3      4  ...       -0.5  0.887885  0.460065\n\n[5 rows x 18 columns]\n\n\nCertain features, like month and hour, exhibit cyclical behavior (e.g., after December comes January, and after 23:00 comes 00:00). By converting these features into cyclical (sin and cos) representations, we ensure that the model properly understands these cyclic relationships. This prevents the model from interpreting consecutive values as linearly distant when they are, in fact, close (e.g., December and January)."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#missing-values-and-data-leakage-treatment",
    "href": "topics/forecasting/forecasting_pipeline.html#missing-values-and-data-leakage-treatment",
    "title": "Forecasting Pipeline",
    "section": "Missing values and data leakage treatment",
    "text": "Missing values and data leakage treatment\n\n\nna_drop = DropMissingData()\n\nprocessed_df = na_drop.fit_transform(processed_df.copy())\n\nAfter feature engineering, we may have introduced missing values, especially with techniques like lagging and windowing, which require previous data points. Therefore, we use DropMissingData to remove rows with missing values, ensuring a clean dataset for subsequent modeling.\nAt this stage, we have created a data frame of explanatory variables (X_mat). It is crucial to avoid data leakage (or “look-ahead bias”) in time series forecasting. This occurs when information from the future is unintentionally used to predict past events. To prevent this, we need to remove the original features (like CO_sensor and RH) after extracting all the required information through our feature engineering steps.\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\nprocessed_df = drop_feat.fit_transform(processed_df.copy())\n\nprocessed_df.head()\n\n                     month  week  day_of_week  ...  month_cos  hour_sin  hour_cos\nDate_Time                                      ...                               \n2004-04-05 00:00:00      4    15            0  ...       -0.5  0.000000  1.000000\n2004-04-05 01:00:00      4    15            0  ...       -0.5  0.269797  0.962917\n2004-04-05 02:00:00      4    15            0  ...       -0.5  0.519584  0.854419\n2004-04-05 03:00:00      4    15            0  ...       -0.5  0.730836  0.682553\n2004-04-05 04:00:00      4    15            0  ...       -0.5  0.887885  0.460065\n\n[5 rows x 16 columns]\n\n\nFinally, we drop the original features (CO_sensor and RH) from the dataset. These original columns have already contributed their information through lagged, windowed, and cyclical features, so retaining them would lead to redundancy or potential data leakage."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#pipeline",
    "href": "topics/forecasting/forecasting_pipeline.html#pipeline",
    "title": "Forecasting Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\nWe now pack all the steps into a single pipeline. Pipelines allow us to apply a sequence of transformations to the data in a well-structured and reproducible way. This makes the data preparation process more efficient and less error-prone, especially when scaling up or iterating over different models or datasets.\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\npipe_processed_df = trans_pipe.fit_transform(raw_df.copy())\n\nprint(f\"The processed df is equal to pipe_processed_df : {pipe_processed_df.equals(processed_df)}\")\n\nThe processed df is equal to pipe_processed_df : True\n\n\nHere, we’ve consolidated the entire feature engineering process into a single Pipeline object, which includes:\n\nDate and time feature extraction\nLag features\nWindow features\nCyclical features\nDropping missing values\nRemoving original features\n\nThis pipeline can be applied to any new dataset that follows a similar structure, ensuring that the feature engineering process is both scalable and consistent across different time periods or datasets. Additionally, this approach enhances model reproducibility and ease of deployment. After fitting and transforming the raw dataset through the pipeline, we confirm that the output matches the manually processed DataFrame."
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html",
    "href": "topics/forecasting/forecasting_multiple_steps.html",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast multiple period (step) using a pipeline approach.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#pipeline",
    "href": "topics/forecasting/forecasting_multiple_steps.html#pipeline",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_multiple_steps.html#train-and-test-split",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]\n\nSince we want to make predictions for multiple periods now instead of one series (vector) of target feature we will have multiple series (matrix of vectors), one for each forecast horizon. This will resolve some missing data in the target matrix because for each forecasting time point we need the next (future) 24 time points. For our last data point we don’t have future values at all, for the one before the last we only have one and so on. We handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the target matrix but remain in the train matrix. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the on the updated index.\n\nforecast_horizon = 24\n\nY_train = pd.DataFrame(index = X_train.index)\n\nY_test = pd.DataFrame(index = X_test.index)\n\nfor temp_h in range(forecast_horizon):\n  Y_train[f\"h_{temp_h}\"] = X_train[\"CO_sensor\"].shift(-temp_h, freq = \"h\")\n  Y_test[f\"h_{temp_h}\"] = X_test[\"CO_sensor\"].shift(-temp_h, freq = \"h\")\n\n\nprint(Y_train.iloc[0:5,0:5])\n\n                        h_0     h_1     h_2     h_3     h_4\nDate_Time                                                  \n2004-04-04 00:00:00  1224.0  1215.0  1115.0  1124.0  1028.0\n2004-04-04 01:00:00  1215.0  1115.0  1124.0  1028.0  1010.0\n2004-04-04 02:00:00  1115.0  1124.0  1028.0  1010.0  1074.0\n2004-04-04 03:00:00  1124.0  1028.0  1010.0  1074.0  1034.0\n2004-04-04 04:00:00  1028.0  1010.0  1074.0  1034.0  1130.0\n\n\n\n\nY_train = Y_train.dropna().copy()\n\nY_test = Y_test.dropna().copy()\n\nX_train = X_train.loc[Y_train.index]\n\nX_test = X_test.loc[Y_test.index]"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#preprocess-data",
    "href": "topics/forecasting/forecasting_multiple_steps.html#preprocess-data",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.fit_transform(X_test.copy())\n\nY_train = Y_train.loc[X_train_processed.index]\n\nY_test = Y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#prediction",
    "href": "topics/forecasting/forecasting_multiple_steps.html#prediction",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Prediction",
    "text": "Prediction\n\nlasso_model = MultiOutputRegressor(Lasso())\n\nlasso_model.fit(X_train_processed, Y_train)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_mat = lasso_model.predict(X_test_processed)\n\npredictions_mat = pd.DataFrame(predictions_mat)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#evaluation",
    "href": "topics/forecasting/forecasting_multiple_steps.html#evaluation",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\n\nrmse_df = []\n\nfor temp_hor in range(Y_test.shape[1]):\n  y_test = Y_test.iloc[:,temp_hor]\n  pred_vec = predictions_mat.iloc[:,temp_hor]\n  rmse = np.round(root_mean_squared_error(y_test, pred_vec), 4)\n  rmse_df.append(pd.DataFrame(columns = [\"horizon\",\"rmse\"],\n                              data = [[temp_hor,rmse]]))\n\nrmse_df = pd.concat(rmse_df, axis = 0)\n\n\nrmse_df.plot(x = \"horizon\", y = \"rmse\")\n\nplt.show()"
  }
]