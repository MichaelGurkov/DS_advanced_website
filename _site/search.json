[
  {
    "objectID": "topics/window_features/window_features.html",
    "href": "topics/window_features/window_features.html",
    "title": "Window features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport os \n\n\nfrom feature_engine.timeseries.forecasting import WindowFeatures, ExpandingWindowFeatures\n\nfrom sktime.transformations.series.summarize import WindowSummarizer\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_electricity.csv\"\n\nraw_df = pd.read_csv(file_path)\n\nraw_df.index = pd.to_datetime(raw_df[\"date_time\"])\n\nraw_df = raw_df.drop(columns = [\"date_time\"]).copy()\n\nraw_df = raw_df.loc[\"2010\":].copy()\n\n\nraw_df[\"demand\"].plot()\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nraw_df.loc[\"2014\":,\"demand\"].plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nRolling windows\n++ explain that the data set seems to have seasonality on different (daily, weekly, yearly) time scales and we’ll use window features to extract and capture this information\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\n\nwin_df_pandas = raw_df.rolling(window = 24).agg([\"mean\",\"std\",mad]).shift(freq = \"1h\").copy()\n\n\n\nwin_fe_trans = WindowFeatures(variables = [\"demand\",\"temperature\"],\n                           functions = [\"mean\",\"std\"],\n                           window = [24, 24 * 7, 24 * 365],\n                           freq = \"1h\")\n                           \nwin_df_fe = win_fe_trans.fit_transform(raw_df.copy())\n\n\nplt.clf()\n\ncol_names = win_df_fe.filter(regex = \"demand_window_[0-9]+(_mean)+\").columns\n\nwin_df_fe[col_names].plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpanding windows\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\n\nwin_df_pandas_exp = raw_df.loc[\"2015\":].expanding().agg([\"mean\",\"std\",mad]).shift(freq = \"1h\").copy()\n\n\n\nwin_fe_trans_exp = ExpandingWindowFeatures(variables = [\"demand\",\"temperature\"],\n                           functions = [\"mean\",\"std\"],\n                           freq = \"1h\")\n                           \nwin_df_fe_exp = win_fe_trans_exp.fit_transform(raw_df.loc[\"2015\":].copy())\n\n\n\nExponential weights\n\n\ndef exp_weights(series_len, alpha):\n  weights_vec = np.ones(series_len)\n  for temp_ind in range(1,series_len):\n    weights_vec[series_len - temp_ind - 1] = (1 - alpha) * weights_vec[series_len - temp_ind]\n  return weights_vec\n\ndef exp_weighted_mean(x, alpha = 0.05):\n  weights_vec = exp_weights(len(x), alpha = alpha)\n  result = (weights_vec * x).sum() / weights_vec.sum()\n  return result\n\n\n\nweight_df = raw_df.loc[\"2015\":,[\"demand\"]].copy()\n\nmean_df = (weight_df\n                    .rolling(window = 24 * 7)\n                    .agg([\"mean\", exp_weighted_mean])\n                    .shift(freq = \"1h\").copy())\n                    \nmean_df.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in mean_df.columns]\n                    \nweight_df = weight_df.join(mean_df)\n\nweight_df.dropna(inplace = True)\n\n\nplt.clf()\n\nweight_df.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom feature_engine.timeseries.forecasting import LagFeatures, WindowFeatures, ExpandingWindowFeatures\nfrom feature_engine.imputation import DropMissingData\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Lag features\nlag_transformer = LagFeatures(variables=[\"demand\", \"temperature\"],\n                              periods=[1, 2, 3, 24, 24 * 7])\n                              \n\n# Window features\nwindow_transformer = WindowFeatures(\n    variables=[\"demand\", \"temperature\"],\n    functions=[\"mean\", \"std\", \"kurt\", \"skew\"],\n    window=[24, 24 * 7, 24 * 7 * 4, 24 * 7 * 4 * 12],\n    periods=1,\n)\n\n\n# Expanding features\nexpanding_window_transformer = ExpandingWindowFeatures(\n    variables=[\"demand\"], \n    functions=[\"mean\", \"std\", \"kurt\", \"skew\"]\n)\n\n\n# Drop missing data introduced by window and lag features\nimputer = DropMissingData()\n\ntrans_pipe = Pipeline(\n    [\n        (\"lag\", lag_transformer),\n        (\"rolling\", window_transformer),\n        (\"expanding\", expanding_window_transformer),\n        (\"drop_missing\", imputer)\n    ]\n)\n\n\n\nprocessed_df = trans_pipe.fit_transform(raw_df.copy())\n\n\nfrom sklearn.linear_model import Lasso\n\nX = processed_df.drop(columns = [\"demand\"]).copy()\n\ny = processed_df[\"demand\"]\n\nX_mat = StandardScaler().fit_transform(X)\n\nlasso_model = Lasso(alpha = 1)\n\nlasso_model.fit(X,y)\n\nLasso(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso(alpha=1) \n\n\n\n\nselected_features = pd.DataFrame({\"feature_names\":X.columns,\n                                  \"values\":lasso_model.coef_})\n                                  \nselected_features = selected_features.sort_values(\"values\",\n                                                  key = abs,\n                                                  ascending = False).iloc[0:10].copy()\n\n\nplt.clf()\n\nplt.figure(figsize=(15, 6))\n\nselected_features.sort_values(\"values\").plot(kind = \"barh\", x = \"feature_names\",\n                       y = \"values\", legend = False)\n\nplt.title(\"Selected features\")\n\nplt.ylabel(\"\")\n\nplt.subplots_adjust(left=0.7)\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html",
    "href": "topics/ts_decomposition/stl_decomp.html",
    "title": "STL Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#trend",
    "href": "topics/ts_decomposition/stl_decomp.html#trend",
    "title": "STL Decomposition",
    "section": "Trend",
    "text": "Trend\n++ explain the parameters of the STL implementation in statsmodels.tsa.seasonal explicitly address the seasonal, period and robust parameters. Other parameters can be left alone - the default values are typically good enough.\n\n\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid\n                 \n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\ndecompostion_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\ndecompostion_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\ndecompostion_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html",
    "href": "topics/ts_decomposition/class_decomp.html",
    "title": "Classical Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#trend",
    "href": "topics/ts_decomposition/class_decomp.html#trend",
    "title": "Classical Decomposition",
    "section": "Trend",
    "text": "Trend\n++ we need to decide on the window size of the moving average. It’s a good rule of thumb to set the window size to the frequency of the seasonality (i.e 12 in case of monthly data with yearly seasonality) if the seasonality is known. This allows as to “isolate” the entire seasonality cycle in one window and thus to smooth over the seasonality. If the seasonality is not know we need to visually evaluate the resulting trend. Setting the window too narrow will result in excess fluctuations of the trend line - under smoothing. Setting the window too wide will result in a flat line that will not capture the changes in the trend\n\n\ntrend_df = raw_df.copy()\n\neven_win_len = 84\n\ntrend_df[\"over_smoothing\"] = trend_df[\"sales\"].rolling(window = even_win_len).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].rolling(window = 2, center = True).mean()\n\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].shift(- even_win_len // 2)\n\ntrend_df[\"under_smoothing\"] = trend_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\ntrend_df[\"sales\"].plot(color = \"grey\", alpha = 0.5)\n\ntrend_df[\"under_smoothing\"].plot(color = \"steelblue\")\n\ntrend_df[\"over_smoothing\"].plot(color = \"orange\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCross validation for trend estimation"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#seasonality",
    "href": "topics/ts_decomposition/class_decomp.html#seasonality",
    "title": "Classical Decomposition",
    "section": "Seasonality",
    "text": "Seasonality\n++ explain that in order to isolate seasonality we first need to detrend the data. So we indentify the trend, than detrend (exclude the trend) by substracting (if additive) of dividing (if multiplicative) and proceed to isolate the seasonality\n\n\nseason_df = raw_df.copy()\n\nseason_df[\"trend\"] = season_df[\"sales\"].rolling(window = 12).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].rolling(window = 2, center = True).mean()\n\nseason_df[\"trend\"] = season_df[\"trend\"].shift(- 12 // 2)\n\nseason_df[\"detrended_data\"] = season_df[\"sales\"] - season_df[\"trend\"]\n\n\nseason_df[\"month\"] = season_df.index.month\n\nseasonality = season_df.groupby(\"month\").mean()[\"detrended_data\"].reset_index()\n\nseasonality.columns = [\"month\",\"seasonality\"]\n\nseason_df = pd.merge(season_df.copy(),seasonality, on = \"month\", how = \"left\")\n\nseason_df[\"remainder\"] = season_df[\"detrended_data\"] - season_df[\"seasonality\"]\n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\n\nseason_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\nseason_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\nseason_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/trend_features/trend_features.html",
    "href": "topics/trend_features/trend_features.html",
    "title": "Trend features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/trend_features/trend_features.html#split-data-and-auxiliary-functions",
    "href": "topics/trend_features/trend_features.html#split-data-and-auxiliary-functions",
    "title": "Trend features",
    "section": "Split data and auxiliary functions",
    "text": "Split data and auxiliary functions\n\n\n\nraw_df[\"trend\"] = (raw_df.index - raw_df.index.min()).days.astype(float)\n\nsplit_date = pd.to_datetime(\"1960-01-01\")\n\n\n\nX_train = raw_df[[\"trend\"]].loc[raw_df.index &lt;= split_date]\n\nX_test = raw_df[[\"trend\"]].loc[raw_df.index &gt; split_date]\n\ny_train = raw_df[\"passengers\"].loc[raw_df.index &lt;= split_date]\n\ny_test = raw_df[\"passengers\"].loc[raw_df.index &gt; split_date]\n\n\n\ndef plot_pred(df,pred_train, pred_test):\n  \n  plot_df = df.copy()\n  \n  plot_df[\"pred\"] = np.concatenate([pred_train, pred_test])\n  \n  plot_df[\"type\"] = np.concatenate([[\"train\"] * len(pred_train),\n                                    [\"test\"] * len(pred_test)])\n  \n  plot_df = plot_df.reset_index().copy()\n                                    \n  plot_df = plot_df.melt(id_vars = [\"type\", \"date\"],\n                         value_vars = [\"passengers\",\"pred\"],\n                         var_name = \"line_type\").copy()\n                         \n  plt.clf()\n                         \n  sns.lineplot(data=plot_df, x=\"date\",\n               y=\"value\", hue=\"type\", style=\"line_type\", legend = False)\n  \n  \n\n  plt.show()"
  },
  {
    "objectID": "topics/trend_features/trend_features.html#linear-trend",
    "href": "topics/trend_features/trend_features.html#linear-trend",
    "title": "Trend features",
    "section": "Linear trend",
    "text": "Linear trend\n\nLinear model\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\ny_pred_train_lin_reg = lin_reg.predict(X_train)\n\ny_pred_test_lin_reg = lin_reg.predict(X_test)\n\n\nplot_pred(df = raw_df.copy(), pred_train = y_pred_train_lin_reg,\n          pred_test = y_pred_test_lin_reg)\n\n\n\n\n\n\n\n\n\n\nTree model\n\ntree_model = DecisionTreeRegressor(max_depth = 1)\n\ntree_model.fit(X_train, y_train)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=1) \n\n\ny_pred_train_tree = tree_model.predict(X_train)\n\ny_pred_test_tree = tree_model.predict(X_test)\n\n\nplot_pred(df = raw_df.copy(), pred_train = y_pred_train_tree,\n          pred_test = y_pred_test_tree)\n\n\n\n\n\n\n\n\n\n\nGradient boosting\n\ngb_model = HistGradientBoostingRegressor()\n\ngb_model.fit(X_train, y_train)\n\nHistGradientBoostingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressoriFittedHistGradientBoostingRegressor() \n\n\ny_pred_train_gb = gb_model.predict(X_train)\n\ny_pred_test_gb = gb_model.predict(X_test)\n\n\nplot_pred(df = raw_df.copy(), pred_train = y_pred_train_gb,\n          pred_test = y_pred_test_gb)\n\n\n\n\n\n\n\n\n\n\nRolling forecast\n\n\nforecast_start = raw_df.index[-1] + pd.DateOffset(months = 1)\n\nforecast_df = pd.DataFrame(index = pd.date_range(start = forecast_start,\n                                                 periods = 12,\n                                                 freq = \"MS\"))"
  },
  {
    "objectID": "topics/trend_features/trend_features.html#nonlinear-trend",
    "href": "topics/trend_features/trend_features.html#nonlinear-trend",
    "title": "Trend features",
    "section": "Nonlinear trend",
    "text": "Nonlinear trend\n++explain that we add a square term in order to capture the non linear trend component. Emphasize that extrapolating non linear terms can often result overfitting and unreal forecasts\n\nX_train_square = X_train.copy()\n\nX_train_square[\"trend_sq\"] = X_train_square ** 2\n\nX_test_square = X_test.copy()\n\nX_test_square[\"trend_sq\"] = X_test_square ** 2\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_train_square, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\ny_pred_train_lin_reg_square = lin_reg.predict(X_train_square)\n\ny_pred_test_lin_reg_square = lin_reg.predict(X_test_square)\n\n\nplot_pred(df = raw_df.copy(), pred_train = y_pred_train_lin_reg_square,\n          pred_test = y_pred_test_lin_reg_square)"
  },
  {
    "objectID": "topics/trend_features/trend_features.html#regularization",
    "href": "topics/trend_features/trend_features.html#regularization",
    "title": "Trend features",
    "section": "Regularization",
    "text": "Regularization"
  },
  {
    "objectID": "topics/time_series_decomposition.html",
    "href": "topics/time_series_decomposition.html",
    "title": "Time series decomposition",
    "section": "",
    "text": "import pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nimport os\nsales_df = pd.read_csv(os.path.expanduser(\"~/Documents\") + \n\"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\")\n\nsales_df[\"date\"] = pd.to_datetime(sales_df[\"date\"], format = \"%m/%d/%Y\")\n\nsales_df[\"sales\"] = sales_df[\"sales\"] / 1000\nplt.figure(figsize=(10, 4))\n\nsns.lineplot(x=\"date\", y=\"sales\",marker = \".\", data=sales_df)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/time_series_decomposition.html#trend",
    "href": "topics/time_series_decomposition.html#trend",
    "title": "Time series decomposition",
    "section": "Trend",
    "text": "Trend\n\nMoving averages\n\n\n# Auxiliary functions\n\ndef plot_combined_trend_and_series(data,**kwargs):\n    # Create a plot with two line plots\n    ax = sns.lineplot(x='date', y='sales', data=data, color=\"lightgray\")\n    \n    sns.lineplot(x='date', y='ma_value', data=data, color=\"steelblue\", ax=ax)\n    \n    return ax\n\ndef plot_panel(wide_df):\n  \n  long_df = wide_df.melt(id_vars=['date', 'sales'],\n                         var_name='ma_type',\n                         value_name='ma_value')\n                         \n  panel_grid = sns.FacetGrid(long_df, col=\"ma_type\",\n                             col_wrap=2,height = 5,aspect = 1.5)\n                             \n  panel_grid.map_dataframe(plot_combined_trend_and_series)\n  \n  plt.show()\n\n\nodd_ma_df = sales_df.copy()\n\nfor win_len in [3,5,7,9]:\n  temp_name = f\"ma_{win_len}\"\n  odd_ma_df[temp_name] = odd_ma_df[\"sales\"].rolling(window = win_len,\n                                                    center = True).mean()\n                                                    \nplot_panel(odd_ma_df)\n\n\n\n\n\n\n\n\n\neven_ma_df = sales_df.copy()\n\nfor win_len in [4,6,8,12]:\n  temp_name = f\"ma_{win_len}\"\n  even_ma_df[temp_name] = even_ma_df[\"sales\"].rolling(window = win_len).mean()\n  even_ma_df[temp_name] = even_ma_df[temp_name].rolling(window = 2).mean()\n  even_ma_df[temp_name] = even_ma_df[temp_name].shift(-win_len//2)\n  \nplot_panel(even_ma_df)\n\n\n\n\n\n\n\n\nImportant - add an explanation of odd ma (pandas give correct result) and even ma (pandas give incorrect result, need to apply another 2 MA and center)"
  },
  {
    "objectID": "topics/time_series_decomposition.html#seasonality",
    "href": "topics/time_series_decomposition.html#seasonality",
    "title": "Time series decomposition",
    "section": "Seasonality",
    "text": "Seasonality"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecaster_demo.html",
    "href": "topics/tabularizing_time_series/forecaster_demo.html",
    "title": "Forecasting demonstration - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nThis is a demonstration of forecasting comparing two models. The data is the preprocessed data set after feature engineering"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecaster_demo.html#data-loading",
    "href": "topics/tabularizing_time_series/forecaster_demo.html#data-loading",
    "title": "Forecasting demonstration - EDA",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \\\n\"\\\\data\\\\air_quality_processed_df.csv\"\n\nair_quality_processed_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_processed_df.index = pd.to_datetime(air_quality_processed_df.index)"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecaster_demo.html#compare-models",
    "href": "topics/tabularizing_time_series/forecaster_demo.html#compare-models",
    "title": "Forecasting demonstration - EDA",
    "section": "Compare models",
    "text": "Compare models\n\n\ny_vec = air_quality_processed_df[\"CO_sensor\"]\n\nX_mat = air_quality_processed_df.drop(\"CO_sensor\", axis = 1)\n\nX_mat_train = X_mat.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\ny_vec_train = y_vec.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\nX_mat_test = X_mat.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\ny_vec_test = y_vec.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\n\nNaive model\n++ explain that as naive model we often take the last known value (the previous hour value in our case)\n\n\nnaive_forecast = air_quality_processed_df.loc[air_quality_processed_df.index &gt; pd.to_datetime(\"2005-03-04\")][\"CO_sensor_lag_1\"]\n\n\n\nLinear regression\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_mat_train, y_vec_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nlin_reg_forecast = lin_reg.predict(X_mat_test)\n\n\n\nRandom forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrand_forest = RandomForestRegressor(\n    n_estimators=50,\n    max_depth=3,\n    random_state=0,\n)\n\nrand_forest.fit(X_mat_train, y_vec_train)\n\nRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0) \n\n\nrand_forest_forecast = lin_reg.predict(X_mat_test)\n\n\n\nEvaluate models\n\nfrom sklearn.metrics import root_mean_squared_error\n\n\nprint(f\"Naive forecat error is {root_mean_squared_error(naive_forecast, y_vec_test)}\")\n\nNaive forecat error is 104.08288851736968\n\nprint(f\"Linear regression error is {root_mean_squared_error(lin_reg_forecast, y_vec_test)}\")\n\nLinear regression error is 86.89761494134571\n\nprint(f\"Random forest error is {root_mean_squared_error(rand_forest_forecast, y_vec_test)}\")\n\nRandom forest error is 86.89761494134571"
  },
  {
    "objectID": "topics/outliers/outliers.html",
    "href": "topics/outliers/outliers.html",
    "title": "Missing data imputation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_outliers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#de-seasonalise-data",
    "href": "topics/outliers/outliers.html#de-seasonalise-data",
    "title": "Missing data imputation",
    "section": "De seasonalise data",
    "text": "De seasonalise data\n++ explain that we are deseasonalizing the data in order to prevent disturbance for outlier identification\n\n\nstl_decomp = STL(raw_df[\"sales\"], robust = True).fit()\n\nseasonal_component = stl_decomp.seasonal\n\nraw_df[\"sales_deseasonalised\"] = raw_df[\"sales\"] - seasonal_component\n\n\nplt.clf()\n\nraw_df[\"sales_deseasonalised\"].plot(marker = \".\")\n\nplt.title(\"Deseasonlalized data with outliers\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#identify-outliers",
    "href": "topics/outliers/outliers.html#identify-outliers",
    "title": "Missing data imputation",
    "section": "Identify outliers",
    "text": "Identify outliers\n\nRolling mean and standard deviation\n\n\nraw_df[[\"roll_mean\",\"roll_std\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_mean\":\"mean\", \"roll_std\":\"std\"})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper\"] = raw_df[\"roll_mean\"] + margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"lower\"] = raw_df[\"roll_mean\"] - margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"is_outlier\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRolling median and median absolute deviation\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\nraw_df[[\"roll_median\",\"roll_mad\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_median\":\"median\", \"roll_mad\":mad})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper_2\"] = raw_df[\"roll_median\"] + margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"lower_2\"] = raw_df[\"roll_median\"] - margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"is_outlier_2\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower_2\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper_2\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df[\"lower_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#impute-outliers",
    "href": "topics/outliers/outliers.html#impute-outliers",
    "title": "Missing data imputation",
    "section": "Impute outliers",
    "text": "Impute outliers\n\n\nraw_df[\"sales_na\"] = raw_df[\"sales_deseasonalised\"]\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_na\"] = np.nan\n\nraw_df[\"sales_imputed\"] = raw_df[\"sales_na\"].interpolate(method = \"time\",\n                                                         inlace = True)\n\n\nplt.clf()\n\nax = raw_df[\"sales_imputed\"].plot()\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_imputed\"].plot(ax = ax,\n                                                      alpha = 0.5,\n                                                      color = \"red\",\n                                                      marker = \"o\",\n                                                      linestyle = \"none\")\n\nplt.title(\"Imputed outliers\")\n\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/lag_plots.html",
    "href": "topics/lagged_features/lag_plots.html",
    "title": "Autoregressive processes",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nAR 1\n\n\nnum_periods = 1000\n\nar_coeff = 0.9\n\nconst_term = 0\n\ntime_index = pd.date_range(start = \"2000-01-01\", periods = num_periods,\n                           freq = \"d\")\n                           \nar1_series = np.zeros(num_periods)\n\nwhite_noise_series = np.zeros(num_periods)\n\ntrend_series = np.zeros(num_periods)\n\n\nfor t in range(1,num_periods):\n  noise = np.random.normal()\n  white_noise_series[t] = noise\n  ar1_series[t] = const_term + ar_coeff * ar1_series[t - 1] + noise\n  trend_series[t] = t + noise\n\nts_df = pd.DataFrame(data = list(zip(white_noise_series,ar1_series,trend_series)),\nindex = time_index, columns = [\"white_noise\",\"ar_1\",\"trend\"])\n\ndel white_noise_series, ar1_series, trend_series\n\ndel t, time_index, ar_coeff, const_term, noise, num_periods\n\n\n\nPandas implemenation\n\n\n\ndef plot_lags_pandas (temp_df, plot_title, ncols = 3, nrows = 3):\n\n  fig, axes = plt.subplots(ncols,nrows)\n  \n  for temp_ind, temp_ax in enumerate(axes.flatten()):\n    pd.plotting.lag_plot(temp_df, lag = temp_ind + 1, ax = temp_ax,\n                         marker = \".\", linestyle='None')\n    temp_ax.set_title(f\"Lag_{temp_ind}\")\n    \n \n  fig.suptitle(plot_title, fontsize=16)\n  \n  plt.tight_layout()\n  \n  plt.subplots_adjust(hspace=1, top=0.8)\n  \n  plt.show()\n\n\nplot_lags_pandas(ts_df[[\"white_noise\"]], plot_title = \"White noise\")\n\n\n\n\n\n\n\n\n\nplot_lags_pandas(ts_df[[\"trend\"]], plot_title = \"Trend\")\n\n\n\n\n\n\n\n\n\nplot_lags_pandas(raw_df.copy(), \"Retail sales\", nrows = 4, ncols = 3)\n\n\n\n\n\n\n\n\n\n\nManual\n\n\n\ndef plot_lags(temp_df, plot_title):\n\n  fig, axes = plt.subplots(3,3)\n  \n  for temp_ind, temp_ax in enumerate(axes.flatten()):\n    temp_ax.scatter(x = temp_df.iloc[:,0],\n                    y = temp_df.iloc[:,0].shift(temp_ind), marker = \".\")\n    temp_ax.set_title(f\"Lag_{temp_ind}\")\n  \n  fig.suptitle(plot_title, fontsize=16)\n  \n  plt.tight_layout()\n  \n  plt.subplots_adjust(hspace=1, top=0.8)\n  \n  plt.show()\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"white_noise\"]].copy(), \"White noise\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"ar_1\"]].copy(), \"Ar 1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nplot_lags(ts_df[[\"trend\"]].copy(), \"Trend\")"
  },
  {
    "objectID": "topics/lagged_features/distributed_lag_features.html",
    "href": "topics/lagged_features/distributed_lag_features.html",
    "title": "Distributed lag features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport os \n\n\nfrom statsmodels.tsa.seasonal import MSTL\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path)\n\nraw_df.index = pd.to_datetime(raw_df[\"Date_Time\"])\n\nraw_df = raw_df.drop(columns = [\"Date_Time\"]).copy()\nfig, axes = plt.subplots(nrows = 2)\n\nfor idx, ax in enumerate(axes):\n  raw_df.iloc[:,[idx]].plot(ax = ax,\n                            legend = False,\n                            title = raw_df.columns.values[idx])\n  ax.set_xlabel('')\n  ax.set_ylabel('')\n                            \nplt.tight_layout(h_pad=3)\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/distributed_lag_features.html#extract-calendar-features",
    "href": "topics/lagged_features/distributed_lag_features.html#extract-calendar-features",
    "title": "Distributed lag features",
    "section": "Extract calendar features",
    "text": "Extract calendar features\n\n\ncalendar_df = raw_df.copy()\n\ncalendar_df[\"hour\"] = calendar_df.index.hour\n\ncalendar_df[\"month\"] = calendar_df.index.month\n\ncalendar_df[\"day_of_week\"] = calendar_df.index.day_of_week\n\n\nDaily seasonality\n++ explain that we see a pattern and that means that the time of day has an effect on the expected value. That means that we should include a lag of 24 hours that will represent the value of the previous observation at the same hour.\n\nfig, axes = plt.subplots(nrows = 2)\n\nfor idx, ax in enumerate(axes):\n  calendar_df.groupby(\"hour\")[[\"CO_sensor\",\"RH\"]].mean().iloc[:,[idx]].plot(ax = ax,\n                            legend = False,\n                            title = raw_df.columns.values[idx])\n  ax.set_xlabel('')\n  ax.set_ylabel('')\n                            \nplt.tight_layout(h_pad=3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nYearly seasonality\n++ explain that we see a pattern and that means that the month has an effect on the expected value. That means that we should include a lag of 24 hours * 365 days that will represent the value of the previous observation at the same month.\n\nfig, axes = plt.subplots(nrows = 2)\n\nfor idx, ax in enumerate(axes):\n  calendar_df.groupby(\"month\")[[\"CO_sensor\",\"RH\"]].mean().iloc[:,[idx]].plot(ax = ax,\n                            legend = False,\n                            title = raw_df.columns.values[idx])\n  ax.set_xlabel('')\n  ax.set_ylabel('')\n                            \nplt.tight_layout(h_pad=3)\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/distributed_lag_features.html#detrend-and-deseasonalize",
    "href": "topics/lagged_features/distributed_lag_features.html#detrend-and-deseasonalize",
    "title": "Distributed lag features",
    "section": "Detrend and deseasonalize",
    "text": "Detrend and deseasonalize\n\n\nresid_df = raw_df.copy()\n\nfor temp_col in resid_df.columns:\n  mstl_decomp = MSTL(endog = resid_df[temp_col], periods = [24, 7*24]).fit()\n  resid_df[temp_col + \"_resid\"] = mstl_decomp.resid\n\n\nfig, axes = plt.subplots(nrows = 2)\n\nresid_df[\"CO_sensor_resid\"].plot(ax = axes[0], title = \"CO_sensor_resid\", xlabel = '')\n\nresid_df[\"RH_resid\"].plot(ax = axes[1], title = \"RH_resid\", xlabel = '')\n\nplt.tight_layout(h_pad=3)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(ncols = 2)\n\nplot_acf(resid_df[\"CO_sensor_resid\"].copy(),\nax  = axes[0], lags = 100, marker = \".\")\n\nplot_pacf(resid_df[\"CO_sensor_resid\"].copy(),\nax  = axes[1], lags = 25, marker = \".\")\n\nfig.suptitle(\"CO_sensor residual ACF and PACF\", fontsize=16)\n\nplt.tight_layout(w_pad = 3)\n\nplt.show()"
  },
  {
    "objectID": "topics/lagged_features/ar_process.html",
    "href": "topics/lagged_features/ar_process.html",
    "title": "Autoregressive processes",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\nAR 1\n\n\nnum_periods = 1000\n\nar_coeff = 0.9\n\nconst_term = 0\n\ntime_index = pd.date_range(start = \"2000-01-01\", periods = num_periods,\n                           freq = \"d\")\n                           \ntime_series = np.zeros(num_periods)\n\nfor t in range(1,num_periods):\n  noise = np.random.normal()\n  time_series[t] = const_term + ar_coeff * time_series[t - 1] + noise\n\nar1_df = pd.DataFrame(data = time_series, index = time_index)\n\n\nplt.clf()\n\nar1_df.plot(legend = False)\n\nplt.title(\"AR 1 process\")\n\nplt.show()"
  },
  {
    "objectID": "topics/intro/feature_egineering.html",
    "href": "topics/intro/feature_egineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nFeature engineering is the process of transforming and creating new input variables, or features, from raw data to improve the performance of predictive models. It involves converting raw data into meaningful inputs that capture underlying patterns or relationships useful for forecasting. In the context of time series forecasting, the choice of features can significantly impact the model’s ability to make accurate predictions. For example, simply using the raw values of a time series may not be enough for the model to capture complex temporal dynamics such as trends or seasonality. Therefore, transforming features or creating new ones like time-based features, lag features, or cyclical patterns is crucial. This tutorial will demonstrate several types of feature transformations, including time-related features, lag features, window (rolling) features, and periodic features. This is only a short demonstration, each type will be covered in detail in its respective section."
  },
  {
    "objectID": "topics/intro/feature_egineering.html#data-loading",
    "href": "topics/intro/feature_egineering.html#data-loading",
    "title": "Feature Engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/intro/feature_egineering.html#time-related-features",
    "href": "topics/intro/feature_egineering.html#time-related-features",
    "title": "Feature Engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#lag-features",
    "href": "topics/intro/feature_egineering.html#lag-features",
    "title": "Feature Engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#window-features",
    "href": "topics/intro/feature_egineering.html#window-features",
    "title": "Feature Engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n  (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n  (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n  ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#periodic-features",
    "href": "topics/intro/feature_egineering.html#periodic-features",
    "title": "Feature Engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclical_df = cyclical.fit_transform(calendar_df)\n\ncyclical_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "topics/intro/feature_egineering.html#save-processed-data",
    "href": "topics/intro/feature_egineering.html#save-processed-data",
    "title": "Feature Engineering",
    "section": "Save processed data",
    "text": "Save processed data\nIn the final step of preprocessing, we must address the missing values that have been introduced during feature engineering, particularly in the creation of lag and window features. These missing values arise because, for example, a 24-hour lag feature requires data from 24 hours prior, which is unavailable for the first 24 observations. Similarly, window features, such as rolling averages, rely on past data over a specified period, leading to NA values at the start of the series where insufficient prior data exists.\nThere are generally two approaches to handle these missing values: imputation or deletion. Imputation involves replacing missing values with substitutes, such as the mean or median of the available data. However, given that the number of missing values is relatively small in this case, and since dropping rows with missing data simplifies the process, we will opt to drop the rows containing missing values. This ensures that the final dataset is complete and ready for modeling without introducing any potential bias from imputation.\nOnce the missing values are handled, the processed dataset, which includes the original data along with the engineered features (such as lag, window, and cyclical features), is saved to a CSV file for further use.\n\n\nprocessed_df = pd.concat([air_quality_df, calendar_df,lag_features_df, cyclical_df], axis = 1)\n\nprocessed_df.dropna(inplace = True)\n\nprocessed_df.to_csv(os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \"\\\\data\\\\air_quality_processed_df.csv\")"
  },
  {
    "objectID": "topics/forecasting/recursive_forecasting_small.html",
    "href": "topics/forecasting/recursive_forecasting_small.html",
    "title": "Recursive forecasting with lag and window feature",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.base import clone\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sktime.transformations.series.summarize import WindowSummarizer\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/recursive_forecasting_small.html#first-point",
    "href": "topics/forecasting/recursive_forecasting_small.html#first-point",
    "title": "Recursive forecasting with lag and window feature",
    "section": "First point",
    "text": "First point\n\n\n\nfeature_source_data = train_set.copy()\n\nfirst_forecast_date = train_set.index.max() + pd.DateOffset(days = 1)\n\nfeature_source_data.loc[first_forecast_date] = np.nan\n\nfeature_vec = basic_pipeline.transform(feature_source_data.copy())\n\nfeature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n\nfirst_pred = lin_reg.predict(feature_vec)\n\nmanual_preds = pd.DataFrame(index = [first_forecast_date],\n                            data = first_pred)"
  },
  {
    "objectID": "topics/forecasting/recursive_forecasting_small.html#second-point",
    "href": "topics/forecasting/recursive_forecasting_small.html#second-point",
    "title": "Recursive forecasting with lag and window feature",
    "section": "Second point",
    "text": "Second point\n\n\nfeature_source_data.loc[first_forecast_date] = first_pred\n\nsecond_forecast_date = first_forecast_date + pd.DateOffset(days = 1)\n\nfeature_source_data.loc[second_forecast_date] = np.nan\n\nfeature_vec = basic_pipeline.transform(feature_source_data.copy())\n\nfeature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n\nsecond_pred = lin_reg.predict(feature_vec)\n\nmanual_preds = pd.concat([manual_preds,pd.DataFrame(index = [second_forecast_date],\n                            data = second_pred)], axis = 0).copy()\n\nmanual_preds.columns = [\"passengers\"]\n\n#Automate recursive forecast\n\n\n\ndef make_recursive_forecast(forecast_horizon,X_train, y_train,\n                            model, preprocess_pipe):\n    \n    # Preprocess data and fit model\n    \n    feature_source_data, model_spec,preprocess_pipe_spec = \\\n    preprocess_data_and_fit_model(X_mat = X_train.copy(),\n                    Y_mat = y_train.copy(),\n                    model_spec = model,\n                    preprocess_pipe_spec = preprocess_pipe)\n                    \n    \n     \n                    \n    # Make next prediction\n    \n    next_pred = predict_next_point(feature_source_data = feature_source_data,\n                              model_spec = model_spec,\n                              preprocess_pipe_spec = preprocess_pipe_spec)\n                              \n    predictions_df = next_pred.copy()                          \n    \n    # Update feature_source_data and iterate\n    \n    for step in range(1, forecast_horizon):\n    \n      feature_source_data = pd.concat([feature_source_data.copy(),\n                                       next_pred], axis = 0)\n                                       \n      next_pred = predict_next_point(feature_source_data = feature_source_data,\n                                model_spec = model_spec,\n                                preprocess_pipe_spec = preprocess_pipe_spec)\n                                \n      predictions_df = pd.concat([predictions_df.copy(), next_pred],\n                                  axis = 0)\n                                  \n    \n    return predictions_df\n    \n    \n \ndef preprocess_data_and_fit_model(X_mat, Y_mat,preprocess_pipe_spec, model_spec):\n  \n  preprocess_pipe_spec.fit(X_mat.copy())\n  \n  X_mat_processed = preprocess_pipe_spec.transform(X_mat.copy())\n\n  Y_mat_processed = Y_mat.loc[X_mat_processed.index].copy()\n  \n  model_spec.fit(X_mat_processed,Y_mat_processed)\n  \n  feature_source_data = X_mat.copy()\n  \n  return([feature_source_data, model_spec,preprocess_pipe_spec])\n\n\ndef predict_next_point(feature_source_data,model_spec,preprocess_pipe_spec):\n  \n  forecast_index = feature_source_data.index.max() + pd.DateOffset(months = 1)\n  \n  forecast_index_row = pd.DataFrame(data = np.nan,\n                              index = [forecast_index],\n                              columns = [\"passengers\"])\n                              \n  feature_source_data = pd.concat([feature_source_data.copy(),\n                                forecast_index_row], axis = 0)\n                                \n  feature_vec = preprocess_pipe_spec.transform(feature_source_data)\n  \n  feature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n  \n  predictions = model_spec.predict(feature_vec)\n  \n  predictions_df = pd.DataFrame(data = predictions,\n                              index = [forecast_index],\n                              columns = [\"passengers\"])\n  \n  return predictions_df\n\n\npredictions_for_10_days = make_recursive_forecast(forecast_horizon = 10,\n                                               X_train = train_set.copy(),\n                                               y_train = train_set.copy(),\n                                               model = LinearRegression(),\n                                               preprocess_pipe = clone(basic_pipeline))\n                               \nprint(predictions_for_10_days.tail())\n\n            passengers\n1960-07-01  372.407985\n1960-08-01  365.443142\n1960-09-01  359.063200\n1960-10-01  353.223936\n1960-11-01  347.881225\n\n\n\nprint(f\"Manual predictions are equal to automatic predicitons: {manual_preds.equals(predictions_for_10_days.iloc[0:2])}\")\n\nManual predictions are equal to automatic predicitons: False"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html",
    "href": "topics/forecasting/forecasting_one_step.html",
    "title": "Forecasting one period (step) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast the next period (step) using a pipeline approach. The pipeline encapsulates all preprocessing operations such as feature engineering, imputation, and feature selection into a single streamlined process. This allows for consistency and efficiency when handling time series data. It is crucial to ensure that the explanatory features (the X matrix) and the target feature (the y vector) are properly aligned in time. Misalignment can lead to look-ahead bias, where future information is inappropriately used in the model training phase, resulting in overoptimistic performance estimates. Careful attention is also required to avoid data leakage, ensuring that the model does not have access to information from the future when forecasting.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#pipeline",
    "href": "topics/forecasting/forecasting_one_step.html#pipeline",
    "title": "Forecasting one period (step) ahead",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_one_step.html#train-and-test-split",
    "title": "Forecasting one period (step) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#preprocess-data",
    "href": "topics/forecasting/forecasting_one_step.html#preprocess-data",
    "title": "Forecasting one period (step) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.transform(X_test.copy())\n\nDuring preprocessing, we apply transformations that handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the training features but remain in the target vector. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the training and test target vectors based on the updated index of the processed feature sets.\n\nprint(y_train.shape)\n\n(7654,)\n\ny_train = y_train.loc[X_train_processed.index]\n\nprint(y_train.shape)\n\n(7426,)\n\n\ny_test = y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#prediction",
    "href": "topics/forecasting/forecasting_one_step.html#prediction",
    "title": "Forecasting one period (step) ahead",
    "section": "Prediction",
    "text": "Prediction\n\nlasso_model = Lasso()\n\nlasso_model.fit(X_train_processed, y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Lasso?Documentation for LassoiFittedLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_vec = lasso_model.predict(X_test_processed)"
  },
  {
    "objectID": "topics/forecasting/forecasting_one_step.html#evaluation",
    "href": "topics/forecasting/forecasting_one_step.html#evaluation",
    "title": "Forecasting one period (step) ahead",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\nrmse = np.round(root_mean_squared_error(y_test, predictions_vec), 4)\n\nprint(f\"The root mean squared error on the test set is {rmse}\")\n\nThe root mean squared error on the test set is 86.7063"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html",
    "href": "topics/forecasting/forecasting_multiple_steps.html",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nHere, we want to demonstrate how to forecast multiple period (step) using a pipeline approach.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#pipeline",
    "href": "topics/forecasting/forecasting_multiple_steps.html#pipeline",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Pipeline",
    "text": "Pipeline\nThe following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\nThe pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_multiple_steps.html#train-and-test-split",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Train and test split",
    "text": "Train and test split\nIn time series forecasting, it’s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is “2005-03-04”. If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24)]\n\ny_train = raw_df.loc[raw_df.index &lt; split_point,\"CO_sensor\"]\n\ny_test = raw_df.loc[raw_df.index &gt;= split_point - pd.offsets.Hour(24),\"CO_sensor\"]\n\nSince we want to make predictions for multiple periods now instead of one series (vector) of target feature we will have multiple series (matrix of vectors), one for each forecast horizon. This will resolve some missing data in the target matrix because for each forecasting time point we need the next (future) 24 time points. For our last data point we don’t have future values at all, for the one before the last we only have one and so on. We handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the target matrix but remain in the train matrix. To resolve this, we need to realign the target vector with the processed features by using .loc to filter both the on the updated index.\n\nforecast_horizon = 24\n\nY_train = pd.DataFrame(index = X_train.index)\n\nY_test = pd.DataFrame(index = X_test.index)\n\nfor temp_h in range(forecast_horizon):\n  Y_train[f\"h_{temp_h}\"] = X_train[\"CO_sensor\"].shift(-temp_h, freq = \"h\")\n  Y_test[f\"h_{temp_h}\"] = X_test[\"CO_sensor\"].shift(-temp_h, freq = \"h\")\n\n\nprint(Y_train.iloc[0:5,0:5])\n\n                        h_0     h_1     h_2     h_3     h_4\nDate_Time                                                  \n2004-04-04 00:00:00  1224.0  1215.0  1115.0  1124.0  1028.0\n2004-04-04 01:00:00  1215.0  1115.0  1124.0  1028.0  1010.0\n2004-04-04 02:00:00  1115.0  1124.0  1028.0  1010.0  1074.0\n2004-04-04 03:00:00  1124.0  1028.0  1010.0  1074.0  1034.0\n2004-04-04 04:00:00  1028.0  1010.0  1074.0  1034.0  1130.0\n\n\n\n\nY_train = Y_train.dropna().copy()\n\nY_test = Y_test.dropna().copy()\n\nX_train = X_train.loc[Y_train.index]\n\nX_test = X_test.loc[Y_test.index]"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#preprocess-data",
    "href": "topics/forecasting/forecasting_multiple_steps.html#preprocess-data",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_processed = trans_pipe.fit_transform(X_train.copy())\n\nX_test_processed = trans_pipe.fit_transform(X_test.copy())\n\nY_train = Y_train.loc[X_train_processed.index]\n\nY_test = Y_test.loc[X_test_processed.index]\n\nAfter preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data."
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#prediction",
    "href": "topics/forecasting/forecasting_multiple_steps.html#prediction",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Prediction",
    "text": "Prediction\n\nlasso_model = MultiOutputRegressor(Lasso())\n\nlasso_model.fit(X_train_processed, Y_train)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\nAfter fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.\n\n\npredictions_mat = lasso_model.predict(X_test_processed)\n\npredictions_mat = pd.DataFrame(predictions_mat)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps.html#evaluation",
    "href": "topics/forecasting/forecasting_multiple_steps.html#evaluation",
    "title": "Forecasting multiple periods (steps) ahead",
    "section": "Evaluation",
    "text": "Evaluation\nFinally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.\n\n\nrmse_df = []\n\nfor temp_hor in range(Y_test.shape[1]):\n  y_test = Y_test.iloc[:,temp_hor]\n  pred_vec = predictions_mat.iloc[:,temp_hor]\n  rmse = np.round(root_mean_squared_error(y_test, pred_vec), 4)\n  rmse_df.append(pd.DataFrame(columns = [\"horizon\",\"rmse\"],\n                              data = [[temp_hor,rmse]]))\n\nrmse_df = pd.concat(rmse_df, axis = 0)\n\n\nrmse_df.plot(x = \"horizon\", y = \"rmse\")\n\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting small\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition\nSTL decomposition\n\nMissing data imputation\n\nMissing data imputation\n\nOutliers\n\nOutliers\n\nLagged features\n\nlagged_features\nlag_plots\nauto_correlation\nar_process\ndistributed_lag_features\n\nWindow features\n\nwindow_features"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "DS_advanced_website",
    "section": "",
    "text": "Introduction\n\nFeature Engineering\nForecasting\n\nForecasting\n\nForecasting small\nForecasting pipeline\nOne step forecasting\nMultistep forecasting\n\nDirect forecasting\nRecursive forecasting\nForecasting comparison\n\n\nTime Series Decomposition\n\nTransformations\nClassical decomposition\nLOWESS decomposition\nSTL decomposition\n\nMissing data imputation\n\nMissing data imputation\n\nOutliers\n\nOutliers\n\nLagged features\n\nlagged_features\nlag_plots\nauto_correlation\nar_process\ndistributed_lag_features\n\nWindow features\n\nwindow_features"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "topics/forecasting/forecasting_comparison.html",
    "href": "topics/forecasting/forecasting_comparison.html",
    "title": "Forecasting comparison - recursive vs one step approach",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_original_features\",drop_feat),\n   (\"drop_missing_values\",na_drop)\n  ]\n  \n)\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_original_features\",drop_feat),\n   (\"drop_missing_values\",na_drop)\n  ]\n  \n)\ndel date_time_feat, lag_feat, window_feat, cyclical_feat, drop_feat,na_drop"
  },
  {
    "objectID": "topics/forecasting/forecasting_comparison.html#preprocess-data",
    "href": "topics/forecasting/forecasting_comparison.html#preprocess-data",
    "title": "Forecasting comparison - recursive vs one step approach",
    "section": "Preprocess data",
    "text": "Preprocess data\n\n\nX_train_recursive_processed = trans_pipe.fit_transform(X_train_recursive.copy())\n\nY_train_recursive = Y_train_recursive.loc[X_train_recursive_processed.index]\n\n\n\nX_train_direct_processed = trans_pipe.fit_transform(X_train_direct.copy())\n\nY_train_direct = Y_train_direct.loc[X_train_direct_processed.index]\n\n\nX_test_direct_processed = trans_pipe.fit_transform(X_test_direct.copy())\n\nY_test_direct = Y_test_direct.loc[X_test_direct_processed.index]"
  },
  {
    "objectID": "topics/forecasting/forecasting_comparison.html#direct",
    "href": "topics/forecasting/forecasting_comparison.html#direct",
    "title": "Forecasting comparison - recursive vs one step approach",
    "section": "Direct",
    "text": "Direct\n\nlasso_model_direct = MultiOutputRegressor(Lasso())\n\nlasso_model_direct.fit(X_train_direct_processed , Y_train_direct)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\n\n\npred_direct = lasso_model_direct.predict(X_test_direct_processed)\n\npred_direct = pd.DataFrame(data = pred_direct, index = X_test_direct_processed.index)\n\npred_direct = pred_direct[[23]].copy()\n\npred_direct.columns = [\"direct_forecast\"]"
  },
  {
    "objectID": "topics/forecasting/forecasting_comparison.html#recursive",
    "href": "topics/forecasting/forecasting_comparison.html#recursive",
    "title": "Forecasting comparison - recursive vs one step approach",
    "section": "Recursive",
    "text": "Recursive\n\n\ndef forecast_next_point(input_data, model, preprocess_pipe):\n  input_data_processed = preprocess_pipe.transform(input_data)\n  prediction = model.predict(input_data_processed)\n  return(prediction)\n\ndef update_input_data(input_data, last_prediction):\n  input_data.iloc[len(input_data) - 1] = last_prediction\n  input_data = input_data.iloc[1:].copy()\n  next_forecast_row = pd.DataFrame(\n                      data = np.nan,\n                      index = [input_data.index.max() + pd.offsets.Hour(1)],\n                      columns = input_data.columns.values\n                      )\n                      \n  input_data = pd.concat([input_data.copy(), next_forecast_row], axis = 0)\n  return input_data\n\ndef make_recursive_forecast(forecast_horizon,\n                            initial_input_data,model, preprocess_pipe):\n                              \n    # Initial case\n                              \n    current_data = initial_input_data.copy()\n\n    current_pred = forecast_next_point(initial_input_data,model,preprocess_pipe)\n    \n    predictions_df = pd.DataFrame(data = current_pred,\n                                  index = [initial_input_data.index.max()],\n                                  columns = initial_input_data.columns.values)\n                                  \n    # Loop through the remaining forecasting range                            \n    \n    for temp_hor in range(1, forecast_horizon):\n      \n      current_data = update_input_data(current_data.copy(), current_pred)\n      \n      current_pred = forecast_next_point(current_data, model, preprocess_pipe)\n      \n      predictions_df = pd.concat([predictions_df.copy(),\n                                  pd.DataFrame(data = current_pred,\n                                  index = [current_data.index.max()],\n                                  columns = current_data.columns.values)],\n                                  axis = 0)\n                                  \n    return predictions_df\n\n\nlasso_model_recursive = MultiOutputRegressor(Lasso())\n\nlasso_model_recursive.fit(X_train_recursive_processed, Y_train_recursive)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\n\nfirst_forecast_point = pd.Timestamp(\"2005-03-04\")\n\nfirst_input_data = X_test_recursive.loc[\n  (X_test_recursive.index &gt;= first_forecast_point - pd.offsets.Hour(24)) &\n  (X_test_recursive.index &lt; first_forecast_point)\n  ]\n                        \nforecast_row = pd.DataFrame(data = np.nan,\n                            index = [first_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])\n                            \nfirst_input_data = pd.concat([first_input_data.copy(),\n                              forecast_row], axis = 0)\n\n  \n  \npred_recursive = make_recursive_forecast(forecast_horizon = 24,\n                               initial_input_data = first_input_data,\n                               model = lasso_model_recursive,\n                               preprocess_pipe = trans_pipe)\n                               \n\npred_recursive = pred_recursive[[\"CO_sensor\"]].copy()\n\npred_recursive.columns = [\"recursive_forecast\"]"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "",
    "text": "In this tutorial, we will explore how to forecast multiple periods ahead using a recursive approach for time series data. The recursive approach involves forecasting one step at a time: after predicting the next step, we update the input data to reflect this new forecast before predicting the following step. This method is particularly useful for time series forecasting tasks where we aim to predict several future steps based on historical data. We will walk through the entire process, from feature engineering and model training to the recursive forecasting method, utilizing a practical dataset related to air quality measurements.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.base import clone\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nThe recursive approach in time series forecasting predicts each step sequentially, updating the input data with each new forecast before proceeding to the next. The idea is to first make a prediction for the next time step based on historical data, then append the predicted value to the input features. This updated data is used to forecast the subsequent time step, and the process repeats for the desired number of forecast periods.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#introduction",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#introduction",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "",
    "text": "In this tutorial, we will explore how to forecast multiple periods ahead using a recursive approach for time series data. The recursive approach involves forecasting one step at a time: after predicting the next step, we update the input data to reflect this new forecast before predicting the following step. This method is particularly useful for time series forecasting tasks where we aim to predict several future steps based on historical data. We will walk through the entire process, from feature engineering and model training to the recursive forecasting method, utilizing a practical dataset related to air quality measurements.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.base import clone\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nThe recursive approach in time series forecasting predicts each step sequentially, updating the input data with each new forecast before proceeding to the next. The idea is to first make a prediction for the next time step based on historical data, then append the predicted value to the input features. This updated data is used to forecast the subsequent time step, and the process repeats for the desired number of forecast periods.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#pipeline",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#pipeline",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Pipeline",
    "text": "Pipeline\nThe feature engineering process is key in preparing time series data for forecasting. Here, we will extract important features from the datetime index, such as month, day, and hour. We’ll also generate lag features (i.e., previous values from certain intervals) and rolling window features (i.e., averages or other statistics over a specific time window). Additionally, cyclical features like month and hour will be transformed using sine and cosine functions to capture the seasonality and periodic nature of the data. Missing data will be handled, and irrelevant features will be dropped to ensure cleaner inputs for the forecasting model.\n\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"12h\"],\n  missing_values = \"ignore\"\n)\n\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\n\nna_drop = DropMissingData()\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_original_features\",drop_feat),\n   (\"drop_missing_values\",na_drop)\n  ]\n  \n)\n\nThe transformations inside the pipeline help to automate the process of preparing the data consistently for both training and prediction. This approach ensures that we can avoid data leakage (i.e., using information from the future) and apply the same set of transformations to any new data points that come in.\n\n\ntrans_pipe = Pipeline(\n  \n  [\n    (\"date_time_features\",date_time_feat),\n    (\"lag_features\",lag_feat),\n    (\"window_features\",window_feat),\n    (\"periodic_features\",cyclical_feat),\n    (\"drop_original_features\",drop_feat),\n    (\"drop_missing_values\",na_drop)\n  ]\n  \n)\n\ndel date_time_feat, lag_feat, window_feat, cyclical_feat, drop_feat,na_drop"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#train-and-test-split",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#train-and-test-split",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Train and Test Split",
    "text": "Train and Test Split\nIn time series forecasting, it’s essential to split the data into training and test sets in a way that prevents any future information from contaminating the training set. We will split the dataset so that the test set contains data starting from “2005-03-04,”. The test set will be used for model evaluation, while the model will be trained on data before the split point.\n\n\nsplit_point = pd.Timestamp(\"2005-03-04\")\n\nX_train = raw_df.loc[raw_df.index &lt; split_point]\n\nX_test = raw_df.loc[raw_df.index &gt;= split_point]\n\nY_train = raw_df.loc[raw_df.index &lt; split_point,[\"CO_sensor\", \"RH\"]]\n\nY_test = raw_df.loc[raw_df.index &gt;= split_point,[\"CO_sensor\", \"RH\"]]"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#preprocess-data",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#preprocess-data",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Preprocess Data",
    "text": "Preprocess Data\n\ntrans_pipe.fit(X_train.copy())\n\nPipeline(steps=[('date_time_features',\n                 DatetimeFeatures(features_to_extract=['month', 'week',\n                                                       'day_of_week',\n                                                       'day_of_month', 'hour',\n                                                       'weekend'],\n                                  variables='index')),\n                ('lag_features',\n                 LagFeatures(freq=['1h', '12h'], missing_values='ignore',\n                             variables=['CO_sensor', 'RH'])),\n                ('window_features',\n                 WindowFeatures(freq='1h', missing_values='ignore',\n                                variables=['CO_sensor', 'RH'], window='3h')),\n                ('periodic_features',\n                 CyclicalFeatures(variables=['month', 'hour'])),\n                ('drop_original_features',\n                 DropFeatures(features_to_drop=['CO_sensor', 'RH'])),\n                ('drop_missing_values', DropMissingData())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('date_time_features',\n                 DatetimeFeatures(features_to_extract=['month', 'week',\n                                                       'day_of_week',\n                                                       'day_of_month', 'hour',\n                                                       'weekend'],\n                                  variables='index')),\n                ('lag_features',\n                 LagFeatures(freq=['1h', '12h'], missing_values='ignore',\n                             variables=['CO_sensor', 'RH'])),\n                ('window_features',\n                 WindowFeatures(freq='1h', missing_values='ignore',\n                                variables=['CO_sensor', 'RH'], window='3h')),\n                ('periodic_features',\n                 CyclicalFeatures(variables=['month', 'hour'])),\n                ('drop_original_features',\n                 DropFeatures(features_to_drop=['CO_sensor', 'RH'])),\n                ('drop_missing_values', DropMissingData())]) DatetimeFeaturesDatetimeFeatures(features_to_extract=['month', 'week', 'day_of_week',\n                                      'day_of_month', 'hour', 'weekend'],\n                 variables='index') LagFeaturesLagFeatures(freq=['1h', '12h'], missing_values='ignore',\n            variables=['CO_sensor', 'RH']) WindowFeaturesWindowFeatures(freq='1h', missing_values='ignore',\n               variables=['CO_sensor', 'RH'], window='3h') CyclicalFeaturesCyclicalFeatures(variables=['month', 'hour']) DropFeaturesDropFeatures(features_to_drop=['CO_sensor', 'RH']) DropMissingDataDropMissingData() \n\n\nX_train_processed = trans_pipe.transform(X_train.copy())\n\nY_train_processed = Y_train.loc[X_train_processed.index].copy()\n\n# X_test_processed = trans_pipe.fit_transform(X_test.copy())\n# \n# Y_test = Y_test.loc[X_test_processed.index]\n\nOnce the data is preprocessed and aligned correctly with the target variables, we can move on to training the model. For this, we use Lasso regression, a form of linear regression that includes a regularization term to reduce the complexity of the model and prevent overfitting. It’s well-suited for scenarios where the dataset has many features and we want to ensure that the model generalizes well to new data."
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#fit-model",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#fit-model",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Fit model",
    "text": "Fit model\n\nlasso_model = MultiOutputRegressor(Lasso())\n\nlasso_model.fit(X_train_processed, Y_train_processed)\n\nMultiOutputRegressor(estimator=Lasso())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MultiOutputRegressor?Documentation for MultiOutputRegressoriFittedMultiOutputRegressor(estimator=Lasso()) estimator: LassoLasso()  Lasso?Documentation for LassoLasso() \n\n\n\nFirst point\nWe first manually prepare the input data for the initial forecast point (“2005-03-04 00:00:00”). The input data must include all the necessary features from the previous 12 hours to compute lagged features for this forecast point. After preparing the data, we make the first prediction.\n\nfirst_forecast_point = split_point\n\nfeature_source_data = X_train.iloc[-12:].copy()\n                        \nforecast_index_row = pd.DataFrame(data = np.nan,\n                            index = [first_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])\n                            \nfeature_source_data = pd.concat([feature_source_data.copy(),\n                              forecast_index_row], axis = 0)\n\nprint(feature_source_data)\n\n                     CO_sensor    RH\n2005-03-03 12:00:00     1129.0  78.6\n2005-03-03 13:00:00     1092.0  69.2\n2005-03-03 14:00:00     1096.0  66.0\n2005-03-03 15:00:00     1108.0  70.1\n2005-03-03 16:00:00     1124.0  72.1\n2005-03-03 17:00:00     1216.0  75.7\n2005-03-03 18:00:00     1437.0  80.4\n2005-03-03 19:00:00     1473.0  82.4\n2005-03-03 20:00:00     1396.0  84.0\n2005-03-03 21:00:00     1285.0  83.6\n2005-03-03 22:00:00     1206.0  82.5\n2005-03-03 23:00:00     1179.0  82.0\n2005-03-04 00:00:00        NaN   NaN\n\n\n\nfirst_features_vec = trans_pipe.transform(feature_source_data)\n\nfirst_point_pred = lasso_model.predict(first_features_vec)\n\npredictions_df = pd.DataFrame(data = first_point_pred,\n                                index = [first_forecast_point],\n                                columns = [\"CO_sensor\",\"RH\"])\nprint(predictions_df)\n\n              CO_sensor         RH\n2005-03-04  1115.696769  80.345937\n\n\n\n\nSecond point\nNext, we update the input data by incorporating the forecast made for the first point and use it to predict the second forecast point.\n\n\nsecond_forecast_point = first_forecast_point + pd.offsets.Hour(1)\n\n# Update feature source data (exclude the first observation and append forecast)\n\nfeature_source_data = feature_source_data.iloc[1:].copy()\n\nfeature_source_data.loc[first_forecast_point] = first_point_pred\n\nsecond_forecast_row = pd.DataFrame(data = np.nan,\n                            index = [second_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])\n\nfeature_source_data = pd.concat([feature_source_data.copy(),\n                              second_forecast_row], axis = 0)                      \n\n\nsecond_features_vec = trans_pipe.transform(feature_source_data)\n\nsecond_point_pred = lasso_model.predict(second_features_vec)\n\npredictions_df = pd.concat([predictions_df.copy(),\n                            pd.DataFrame(data = second_point_pred,\n                            index = [second_forecast_point],\n                            columns = [\"CO_sensor\",\"RH\"])])\n\nprint(predictions_df)\n\n                       CO_sensor         RH\n2005-03-04 00:00:00  1115.696769  80.345937\n2005-03-04 01:00:00  1060.033356  78.441773"
  },
  {
    "objectID": "topics/forecasting/forecasting_multiple_steps_recursive.html#summary",
    "href": "topics/forecasting/forecasting_multiple_steps_recursive.html#summary",
    "title": "Forecasting multiple periods (steps) ahead - recursive approach",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we demonstrated how to use a recursive approach to forecast multiple periods ahead in time series data. By applying a pipeline for feature engineering and using Lasso regression, we built a model to predict air quality sensor readings. The recursive method allowed us to forecast step-by-step, updating the input data with each new prediction. Finally, we automated the process using a loop, making it easy to predict several steps ahead efficiently. This approach is highly adaptable to various time series forecasting problems."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html",
    "href": "topics/forecasting/forecasting_pipeline.html",
    "title": "Forecasting Pipeline",
    "section": "",
    "text": "import pandas as pd\n\nfrom feature_engine.creation import CyclicalFeatures\n\nfrom feature_engine.datetime import DatetimeFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)\n\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nraw_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nOur goal in this pipeline is to transform the data by applying a series of feature engineering techniques to prepare it for time series forecasting. We will perform the following tasks:\nThe feature engineering will be performed using the feature_engine library, which offers an easy-to-use interface to build these transformations. Our goal is to encapsulate all operations in one pipeline for easier reproducibility and maintainability."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#date-time-features",
    "href": "topics/forecasting/forecasting_pipeline.html#date-time-features",
    "title": "Forecasting Pipeline",
    "section": "Date time features",
    "text": "Date time features\n\ndate_time_feat = DatetimeFeatures(\n  variables = \"index\",\n  features_to_extract = [\"month\",\"week\",\"day_of_week\",\n                         \"day_of_month\",\"hour\",\"weekend\"]\n                         )\n\nprocessed_df = date_time_feat.fit_transform(raw_df.copy())\n\n\nprocessed_df.head()\n\n                     CO_sensor    RH  month  ...  day_of_month  hour  weekend\nDate_Time                                    ...                             \n2004-04-04 00:00:00     1224.0  56.5      4  ...             4     0        1\n2004-04-04 01:00:00     1215.0  59.2      4  ...             4     1        1\n2004-04-04 02:00:00     1115.0  62.4      4  ...             4     2        1\n2004-04-04 03:00:00     1124.0  65.0      4  ...             4     3        1\n2004-04-04 04:00:00     1028.0  65.3      4  ...             4     4        1\n\n[5 rows x 8 columns]\n\n\nIn this step, we are extracting date and time features from the index, such as the month, day of the week, and hour of the day. This helps us leverage the temporal structure of the data in subsequent modeling steps. For example, the day of the week or the hour could influence air quality, so extracting such features allows us to include this information in the model."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#lag-features",
    "href": "topics/forecasting/forecasting_pipeline.html#lag-features",
    "title": "Forecasting Pipeline",
    "section": "Lag features",
    "text": "Lag features\n\nlag_feat = LagFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  freq = [\"1h\",\"24h\"],\n  missing_values = \"ignore\"\n)\n\nprocessed_df = lag_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"lag\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_lag_1h  RH_lag_1h  CO_sensor_lag_24h  RH_lag_24h\nDate_Time                                                                      \n2004-04-04 00:00:00               NaN        NaN                NaN         NaN\n2004-04-04 01:00:00            1224.0       56.5                NaN         NaN\n2004-04-04 02:00:00            1215.0       59.2                NaN         NaN\n2004-04-04 03:00:00            1115.0       62.4                NaN         NaN\n2004-04-04 04:00:00            1124.0       65.0                NaN         NaN\n\n\nHere, we create lag features for the CO_sensor and RH (Relative Humidity) variables. Lagging is a powerful technique in time series forecasting as it allows us to capture information from previous time steps. In this case, we are creating two types of lags: one for 1 hour prior and another for 24 hours prior, which will help the model learn patterns that evolve over both short and longer time scales."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#window-features",
    "href": "topics/forecasting/forecasting_pipeline.html#window-features",
    "title": "Forecasting Pipeline",
    "section": "Window features",
    "text": "Window features\n\nwindow_feat = WindowFeatures(\n  variables = [\"CO_sensor\",\"RH\"],\n  window = \"3h\",\n  freq = \"1h\",\n  missing_values = \"ignore\"\n)\n\nprocessed_df = window_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"win\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor_window_3h_mean  RH_window_3h_mean\nDate_Time                                                       \n2004-04-04 00:00:00                       NaN                NaN\n2004-04-04 01:00:00               1224.000000          56.500000\n2004-04-04 02:00:00               1219.500000          57.850000\n2004-04-04 03:00:00               1184.666667          59.366667\n2004-04-04 04:00:00               1151.333333          62.200000\n\n\nIn this step, we generate window features. These features capture rolling window statistics over a 3-hour window for the CO_sensor and RH variables, calculated at 1-hour intervals. This provides insight into the short-term trends or fluctuations in the data, as moving averages or other summary statistics over the window can help smooth out noisy data and emphasize underlying patterns."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#cyclical-features",
    "href": "topics/forecasting/forecasting_pipeline.html#cyclical-features",
    "title": "Forecasting Pipeline",
    "section": "Cyclical features",
    "text": "Cyclical features\n\ncyclical_feat = CyclicalFeatures(\n  variables = [\"month\",\"hour\"],\n  drop_original = False\n)\n\nprocessed_df = cyclical_feat.fit_transform(processed_df.copy())\n\nnames_list = [name for name in processed_df.columns if \"month\" or \"hour\" in name]\n\nprocessed_df[names_list].head()\n\n                     CO_sensor    RH  month  ...  month_cos  hour_sin  hour_cos\nDate_Time                                    ...                               \n2004-04-04 00:00:00     1224.0  56.5      4  ...       -0.5  0.000000  1.000000\n2004-04-04 01:00:00     1215.0  59.2      4  ...       -0.5  0.269797  0.962917\n2004-04-04 02:00:00     1115.0  62.4      4  ...       -0.5  0.519584  0.854419\n2004-04-04 03:00:00     1124.0  65.0      4  ...       -0.5  0.730836  0.682553\n2004-04-04 04:00:00     1028.0  65.3      4  ...       -0.5  0.887885  0.460065\n\n[5 rows x 18 columns]\n\n\nCertain features, like month and hour, exhibit cyclical behavior (e.g., after December comes January, and after 23:00 comes 00:00). By converting these features into cyclical (sin and cos) representations, we ensure that the model properly understands these cyclic relationships. This prevents the model from interpreting consecutive values as linearly distant when they are, in fact, close (e.g., December and January)."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#missing-values-and-data-leakage-treatment",
    "href": "topics/forecasting/forecasting_pipeline.html#missing-values-and-data-leakage-treatment",
    "title": "Forecasting Pipeline",
    "section": "Missing values and data leakage treatment",
    "text": "Missing values and data leakage treatment\n\n\nna_drop = DropMissingData()\n\nprocessed_df = na_drop.fit_transform(processed_df.copy())\n\nAfter feature engineering, we may have introduced missing values, especially with techniques like lagging and windowing, which require previous data points. Therefore, we use DropMissingData to remove rows with missing values, ensuring a clean dataset for subsequent modeling.\nAt this stage, we have created a data frame of explanatory variables (X_mat). It is crucial to avoid data leakage (or “look-ahead bias”) in time series forecasting. This occurs when information from the future is unintentionally used to predict past events. To prevent this, we need to remove the original features (like CO_sensor and RH) after extracting all the required information through our feature engineering steps.\n\ndrop_feat = DropFeatures(features_to_drop = [\"CO_sensor\",\"RH\"])\n\nprocessed_df = drop_feat.fit_transform(processed_df.copy())\n\nprocessed_df.head()\n\n                     month  week  day_of_week  ...  month_cos  hour_sin  hour_cos\nDate_Time                                      ...                               \n2004-04-05 00:00:00      4    15            0  ...       -0.5  0.000000  1.000000\n2004-04-05 01:00:00      4    15            0  ...       -0.5  0.269797  0.962917\n2004-04-05 02:00:00      4    15            0  ...       -0.5  0.519584  0.854419\n2004-04-05 03:00:00      4    15            0  ...       -0.5  0.730836  0.682553\n2004-04-05 04:00:00      4    15            0  ...       -0.5  0.887885  0.460065\n\n[5 rows x 16 columns]\n\n\nFinally, we drop the original features (CO_sensor and RH) from the dataset. These original columns have already contributed their information through lagged, windowed, and cyclical features, so retaining them would lead to redundancy or potential data leakage."
  },
  {
    "objectID": "topics/forecasting/forecasting_pipeline.html#pipeline",
    "href": "topics/forecasting/forecasting_pipeline.html#pipeline",
    "title": "Forecasting Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\nWe now pack all the steps into a single pipeline. Pipelines allow us to apply a sequence of transformations to the data in a well-structured and reproducible way. This makes the data preparation process more efficient and less error-prone, especially when scaling up or iterating over different models or datasets.\n\ntrans_pipe = Pipeline(\n  [(\"date_time_features\",date_time_feat),\n   (\"lag_features\",lag_feat),\n   (\"window_features\",window_feat),\n   (\"periodic_features\",cyclical_feat),\n   (\"drop_missing_values\",na_drop),\n   (\"drop_original_features\",drop_feat)\n  ]\n  \n)\n\npipe_processed_df = trans_pipe.fit_transform(raw_df.copy())\n\nprint(f\"The processed df is equal to pipe_processed_df : {pipe_processed_df.equals(processed_df)}\")\n\nThe processed df is equal to pipe_processed_df : True\n\n\nHere, we’ve consolidated the entire feature engineering process into a single Pipeline object, which includes:\n\nDate and time feature extraction\nLag features\nWindow features\nCyclical features\nDropping missing values\nRemoving original features\n\nThis pipeline can be applied to any new dataset that follows a similar structure, ensuring that the feature engineering process is both scalable and consistent across different time periods or datasets. Additionally, this approach enhances model reproducibility and ease of deployment. After fitting and transforming the raw dataset through the pipeline, we confirm that the output matches the manually processed DataFrame."
  },
  {
    "objectID": "topics/intro/eda.html",
    "href": "topics/intro/eda.html",
    "title": "Exploratory Data Analysis - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/intro/eda.html#introduction-to-exploratory-data-analysis-eda",
    "href": "topics/intro/eda.html#introduction-to-exploratory-data-analysis-eda",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Introduction to Exploratory Data Analysis (EDA)",
    "text": "Introduction to Exploratory Data Analysis (EDA)\nExploratory Data Analysis (EDA) is a crucial step in any data science or time series analysis process. It involves visually and statistically summarizing the key characteristics of a dataset to gain insights into its structure, underlying patterns, and potential issues such as missing data or outliers. In this short tutorial, we will focus on performing EDA on time series data. Specifically, we will identify and handle missing values and explore seasonality—one of the most common characteristics in time series data. Seasonality refers to patterns that repeat at regular intervals, such as daily, weekly, or yearly trends, and detecting it is essential for accurate forecasting."
  },
  {
    "objectID": "topics/intro/eda.html#data-loading",
    "href": "topics/intro/eda.html#data-loading",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/intro/eda.html#data-visualization",
    "href": "topics/intro/eda.html#data-visualization",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nfor temp_col in air_quality_df.columns.values:\n  air_quality_df[temp_col].plot(figsize = (20,6))\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we plot each of the columns in the dataset (CO_sensor and RH) to visually explore the time series data. These plots provide a general understanding of how pollutant levels (CO_sensor) and humidity (RH) fluctuate over time. Larger trends, spikes, or patterns such as seasonality may already be visible, and such visualizations are a useful first step before deeper analysis."
  },
  {
    "objectID": "topics/intro/eda.html#handling-missing-values",
    "href": "topics/intro/eda.html#handling-missing-values",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nIn time series analysis, it is common to encounter missing data due to sensor malfunctions or recording errors. Properly handling missing data is crucial because forecasting models often assume data points are spaced at regular intervals. To address this, we first ensure the data is uniformly spaced by converting the Date_Time index to an hourly frequency. This allows us to easily detect missing data points and fill the gaps accordingly.\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n\nIn this block, we address missing values by using forward-filling (ffill()), which propagates the last valid observation forward until a new non-missing value is encountered. This technique works well when the missing values are sparse or when we expect the data to remain stable over short intervals. We create new columns, such as CO_sensor_imputed and RH_imputed, to store the imputed values, allowing us to compare them with the original data and ensure that no critical information is lost."
  },
  {
    "objectID": "topics/intro/eda.html#visualizing-imputed-values",
    "href": "topics/intro/eda.html#visualizing-imputed-values",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Visualizing Imputed Values",
    "text": "Visualizing Imputed Values\nTo ensure that our imputation strategy has been applied correctly, we will visually compare the original data with the imputed values. This comparison will highlight where missing values were filled and help us verify that the imputation did not introduce any distortions into the time series.\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  \n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  \n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(\n    ax = ax,legend = False,marker = \".\", color = \"red\", linestyle='None')\n    \n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  \n  plt.tick_params(axis='both', which='major', labelsize=16)\n  \n  plt.xlabel('')  # Disable x-axis label\n  \n  plt.ylabel('')  # Disable y-axis label\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this step, we overlay the imputed values onto the original time series. The original data is represented by a continuous line, while missing values that were filled via forward-fill are marked with red dots. This side-by-side comparison helps us visually assess the areas where imputation occurred and check whether it was applied appropriately without affecting the overall trend of the data."
  },
  {
    "objectID": "topics/intro/eda.html#seasonality",
    "href": "topics/intro/eda.html#seasonality",
    "title": "Exploratory Data Analysis - EDA",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality is a recurring pattern in data that occurs at regular intervals, often influenced by natural or social processes. In the context of air quality, pollutant levels such as carbon monoxide may follow daily or weekly cycles due to human activities like traffic or industrial operations. To detect seasonality, we group the data by the time of day (i.e., by the hour) and calculate the average pollutant levels across all observations for each hour. This provides a clear picture of how pollutant concentrations vary throughout the day.\n\nhours_con = impute_df.groupby(impute_df.index.time)[\n  \"CO_sensor\"].mean().reset_index()\n\nhours_con.plot(x = 'index', y = 'CO_sensor', legend = False)\nplt.title(\"Pollutant concentration over day time\")\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this final step, we plot the average CO_sensor concentration over the course of a typical day to visualize the intra-day seasonality. This chart allows us to observe how carbon monoxide levels fluctuate during different times of the day, potentially reflecting periods of higher traffic or other factors that influence air quality. Understanding these seasonal patterns is key for making accurate predictions and taking appropriate action in environmental monitoring."
  },
  {
    "objectID": "topics/intro/forecaster_demo.html",
    "href": "topics/intro/forecaster_demo.html",
    "title": "Forecasting demonstration",
    "section": "",
    "text": "import pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom feature_engine.timeseries.forecasting import LagFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import root_mean_squared_error\nIn this section, we provide a brief demonstration of a forecasting problem using a simple Linear Regression model alongside a naive benchmark model. The naive model assumes that the forecast for the next time step will be the same as the last known value. The dataset used for this example consists of historical airline passenger numbers. Our goal is to demonstrate the forecasting process, including key steps such as feature engineering, model fitting, and forecast evaluation."
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#data-loading",
    "href": "topics/intro/forecaster_demo.html#data-loading",
    "title": "Forecasting demonstration",
    "section": "Data loading",
    "text": "Data loading\nThe dataset contains monthly data on the number of airline passengers from January 1949 to December 1960. This time series dataset is commonly used in forecasting examples due to its clear seasonal patterns and upward trend over time. The time index of the data represents the first day of each month, and the target variable is the number of passengers. The following code loads the dataset, ensures the date column is correctly parsed as a datetime object, and sets it as the index of the DataFrame.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#compare-models",
    "href": "topics/intro/forecaster_demo.html#compare-models",
    "title": "Forecasting demonstration",
    "section": "Compare models",
    "text": "Compare models\nNext, we split the dataset into training and testing sets. The training set contains data up to a specific cutoff date (2005-03-04), and the testing set contains data after that date. The target variable (the one we aim to predict) is CO_sensor, which measures the concentration of carbon monoxide detected by the sensor. The feature matrix X_mat contains all the other predictor variables.\n\n\ny_vec = air_quality_processed_df[\"CO_sensor\"]\n\nX_mat = air_quality_processed_df.drop(\"CO_sensor\", axis = 1)\n\nX_mat_train = X_mat.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\ny_vec_train = y_vec.loc[X_mat.index &lt;= pd.to_datetime(\"2005-03-04\")]\n\nX_mat_test = X_mat.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\ny_vec_test = y_vec.loc[X_mat.index &gt; pd.to_datetime(\"2005-03-04\")]\n\n\nNaive model\nThe naive model is often used as a simple benchmark in forecasting problems. It assumes that the best prediction for the next time step is simply the last observed value from the previous time step. In this case, we use the CO sensor data from the previous hour (CO_sensor_lag_1) as our naive forecast.\n\n\nnaive_forecast = air_quality_processed_df.loc[air_quality_processed_df.index &gt; pd.to_datetime(\"2005-03-04\")][\"CO_sensor_lag_1\"]\n\n\n\nLinear regression\nThe first machine learning model we use is Linear Regression. This model attempts to find a linear relationship between the predictor variables and the target variable. Once trained on the training dataset, we use it to make predictions on the test set.\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(X_mat_train, y_vec_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nlin_reg_forecast = lin_reg.predict(X_mat_test)\n\n\n\nRandom forest\nThe second model is a Random Forest Regressor, an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and avoid overfitting. In this case, we set the number of trees (n_estimators) to 50 and the maximum depth of each tree to 3.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrand_forest = RandomForestRegressor(\n    n_estimators=50,\n    max_depth=3,\n    random_state=0,\n)\n\nrand_forest.fit(X_mat_train, y_vec_train)\n\nRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=3, n_estimators=50, random_state=0) \n\n\nrand_forest_forecast = rand_forest.predict(X_mat_test)\n\n\n\nEvaluate models\nTo evaluate the performance of the models, we calculate the Root Mean Squared Error (RMSE) for each forecast. RMSE is a widely used metric in forecasting problems because it provides a measure of how well a model’s predictions match the actual values, with lower values indicating better performance.\n\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef root_mean_squared_error(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nprint(f\"Naive forecast error is {root_mean_squared_error(naive_forecast, y_vec_test)}\")\n\nNaive forecast error is 104.08288851736968\n\nprint(f\"Linear regression error is {root_mean_squared_error(lin_reg_forecast, y_vec_test)}\")\n\nLinear regression error is 86.89761494134571\n\nprint(f\"Random forest error is {root_mean_squared_error(rand_forest_forecast, y_vec_test)}\")\n\nRandom forest error is 101.07981790929384\n\n\nIn summary, this demonstration illustrates how different models can be applied to a time series forecasting problem. By comparing a naive model, a linear regression model, and a random forest model, we can observe the strengths and weaknesses of each approach in terms of their forecasting accuracy, as measured by RMSE."
  },
  {
    "objectID": "topics/lagged_features/auto_correlation.html",
    "href": "topics/lagged_features/auto_correlation.html",
    "title": "Autocorrelation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport os \n\nfrom statsmodels.tsa.stattools import acf, pacf, ccf\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfrom statsmodels.tsa.seasonal import STL\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid"
  },
  {
    "objectID": "topics/lagged_features/auto_correlation.html#real-world-data",
    "href": "topics/lagged_features/auto_correlation.html#real-world-data",
    "title": "Autocorrelation",
    "section": "Real world data",
    "text": "Real world data\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\npassengers_df = pd.read_csv(file_path,index_col = \"date\")\n\npassengers_df.index = pd.to_datetime(passengers_df.index)\n\nsales_df = raw_df.copy()\n\ndel file_path\n\n\ndf = pd.DataFrame(data = {\"passengers\": passengers_df[\"passengers\"].iloc[0:120],\n                          \"sales\": sales_df[\"sales\"].iloc[0:120]})"
  },
  {
    "objectID": "topics/lagged_features/lagged_features.html",
    "href": "topics/lagged_features/lagged_features.html",
    "title": "Lagged features",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom feature_engine.timeseries.forecasting import LagFeatures\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/lagged_features/lagged_features.html#lagged-features",
    "href": "topics/lagged_features/lagged_features.html#lagged-features",
    "title": "Lagged features",
    "section": "Lagged features",
    "text": "Lagged features\n\npandas implementation\n\n\nlag_df = raw_df.copy()\n\nfor temp_lag in [1,2,12]:\n  lag_df[f\"lag_{temp_lag}\"] = lag_df[\"sales\"].shift(freq = f\"{temp_lag}MS\")\n  \n\n\nplt.clf()\nlag_df.plot(alpha = 0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature_engine implementation\n\nlag_trans = LagFeatures(variables = [\"sales\"], freq = [\"1MS\",\"2MS\",\"12MS\"])\n\nlag_df_fe = lag_trans.fit_transform(raw_df.copy()) \n\nprint(lag_df_fe.head())\n\n             sales  sales_lag_1MS  sales_lag_2MS  sales_lag_12MS\ndate                                                            \n1992-01-01  146376            NaN            NaN             NaN\n1992-02-01  147079       146376.0            NaN             NaN\n1992-03-01  159336       147079.0       146376.0             NaN\n1992-04-01  163669       159336.0       147079.0             NaN\n1992-05-01  170068       163669.0       159336.0             NaN"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html",
    "href": "topics/na_imputation/na_imputation.html",
    "title": "Missing data imputation",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_missing.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\nna_idx = raw_df.isnull()\n\n# url = \"https://raw.githubusercontent.com/facebook/prophet/master/examples/example_retail_sales.csv\"\n# df = pd.read_csv(url)\n# \n# df = df.iloc[0:160].copy()\n# \n# df.columns = [\"date\",\"sales\"]\n# \n# df = df.set_index(\"date\").copy()\n# \n# \n# # Insert missing data into dataframe\n# df.iloc[10:11] = np.NaN\n# df.iloc[25:28] = np.NaN\n# df.iloc[40:45] = np.NaN\n# df.iloc[70:94] = np.NaN\n# \n# \n# df.to_csv(file_path)\nprint(f\"There are {raw_df['sales'].isnull().sum()} missing values, these are {np.round(raw_df['sales'].isnull().sum() / len(raw_df)* 100,3)} percent of the data\")\n\nThere are 33 missing values, these are 20.625 percent of the data\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#forward-fill",
    "href": "topics/na_imputation/na_imputation.html#forward-fill",
    "title": "Missing data imputation",
    "section": "Forward fill",
    "text": "Forward fill\n\nffill_df = raw_df.ffill()\n\nplt.clf()\n\nax = ffill_df.plot(linestyle=\"-\", marker=\".\")\n\nffill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#backward-fill",
    "href": "topics/na_imputation/na_imputation.html#backward-fill",
    "title": "Missing data imputation",
    "section": "Backward fill",
    "text": "Backward fill\n++ explain that backward filling can introduce “data leakage” because we are carrying to the past information from the future.\n\nbfill_df = raw_df.bfill()\n\nplt.clf()\n\nax = bfill_df.plot(linestyle=\"-\", marker=\".\")\n\nbfill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "href": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "title": "Missing data imputation",
    "section": "Linear interpolation",
    "text": "Linear interpolation\n\nlin_inter = raw_df.interpolate(method = \"time\")\n\nplt.clf()\n\nax = lin_inter.plot(linestyle=\"-\", marker=\".\")\n\nlin_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "href": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "title": "Missing data imputation",
    "section": "Spline interpolation",
    "text": "Spline interpolation\n\nspline_inter = raw_df.interpolate(method = \"spline\", order = 3)\n\nplt.clf()\n\nax = spline_inter.plot(linestyle=\"-\", marker=\".\")\n\nspline_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "href": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "title": "Missing data imputation",
    "section": "Seasonal decomposition and interpolation",
    "text": "Seasonal decomposition and interpolation\n++ explain that we first use linear interpolation because STL can not handle missing data\n\n\nstl_inter = STL(raw_df.interpolate(method = \"time\"), seasonal = 31).fit()\n\nseasonal_component = stl_inter.seasonal\n\ndeaseasonlised_df = raw_df[\"sales\"] - seasonal_component\n\ndf_inter = deaseasonlised_df.interpolate(method = \"time\")\n\ndf_final = df_inter + seasonal_component\n\ndf_final = df_final.to_frame().rename(columns = {0:\"sales\"})\n\n\nplt.clf()\n\nax = df_final.plot(linestyle=\"-\", marker=\".\")\n\ndf_final[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html",
    "title": "Forecasting demonstration - feature engineering",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#data-loading",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#data-loading",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis."
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#time-related-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#time-related-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Time related features",
    "text": "Time related features\nIn this section, we extract time-related features that are essential for improving the performance of our forecasting model. These features include temporal information such as the month, day, and hour of each observation. These are known as “future-known” features, meaning that their values for future timestamps are already known at the time of making a forecast. For example, we always know in advance what the month or hour will be for any given future date. These time-related features provide useful context that can help the model better understand seasonal patterns, daily fluctuations, or other time-dependent behaviors in the data.\n\ncalendar_df = pd.DataFrame(index = air_quality_df.index)\n\ncalendar_df[\"Month\"] = air_quality_df.index.month\n\ncalendar_df[\"Day\"] = air_quality_df.index.day\n\ncalendar_df[\"Hour\"] = air_quality_df.index.hour\n\ncalendar_df.head()\n\n                     Month  Day  Hour\nDate_Time                            \n2004-04-04 00:00:00      4    4     0\n2004-04-04 01:00:00      4    4     1\n2004-04-04 02:00:00      4    4     2\n2004-04-04 03:00:00      4    4     3\n2004-04-04 04:00:00      4    4     4"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#lag-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#lag-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Lag features",
    "text": "Lag features\nLag features capture values from previous time points and can be particularly useful in forecasting. For instance, the concentration of CO at the current hour could be related to the concentration of CO from one or 24 hours ago. By introducing lagged versions of the original features, the model gains insight into how past values may influence future outcomes.\nIn the code, we generate lag features for each variable using a set of lag intervals (1 hour and 24 hours). This is done by shifting the values in the dataset by the specified lag periods. However, this process creates missing values for the initial time steps where the lagged data is not available (e.g., if we’re using a 24-hour lag, the first 24 hours will have missing values). These missing values will need to be handled later by either imputing them or dropping the corresponding rows.\nAdditionally, note the use of parentheses to continue the statement across lines in the loop. This enhances code readability and makes it easier to follow the logic.\n\nlag_features_df = pd.DataFrame(index = air_quality_df.index)\n\nlags = [1, 24]\n\nfor temp_col in air_quality_df.columns:\n  for temp_lag in lags:\n    lag_features_df[temp_col + \"_lag_\" + str(temp_lag)] = (\n      air_quality_df[temp_col].shift(freq = str(temp_lag) + \"h\")\n      )\n\nlag_features_df.head(25)\n\n                     CO_sensor_lag_1  CO_sensor_lag_24  RH_lag_1  RH_lag_24\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN               NaN       NaN        NaN\n2004-04-04 01:00:00           1224.0               NaN      56.5        NaN\n2004-04-04 02:00:00           1215.0               NaN      59.2        NaN\n2004-04-04 03:00:00           1115.0               NaN      62.4        NaN\n2004-04-04 04:00:00           1124.0               NaN      65.0        NaN\n2004-04-04 05:00:00           1028.0               NaN      65.3        NaN\n2004-04-04 06:00:00           1010.0               NaN      66.5        NaN\n2004-04-04 07:00:00           1074.0               NaN      69.1        NaN\n2004-04-04 08:00:00           1034.0               NaN      64.8        NaN\n2004-04-04 09:00:00           1130.0               NaN      59.0        NaN\n2004-04-04 10:00:00           1275.0               NaN      49.8        NaN\n2004-04-04 11:00:00           1324.0               NaN      40.7        NaN\n2004-04-04 12:00:00           1268.0               NaN      37.1        NaN\n2004-04-04 13:00:00           1272.0               NaN      33.8        NaN\n2004-04-04 14:00:00           1160.0               NaN      32.1        NaN\n2004-04-04 15:00:00           1136.0               NaN      31.1        NaN\n2004-04-04 16:00:00           1296.0               NaN      30.8        NaN\n2004-04-04 17:00:00           1345.0               NaN      36.0        NaN\n2004-04-04 18:00:00           1296.0               NaN      36.2        NaN\n2004-04-04 19:00:00           1258.0               NaN      39.3        NaN\n2004-04-04 20:00:00           1420.0               NaN      44.6        NaN\n2004-04-04 21:00:00           1366.0               NaN      48.9        NaN\n2004-04-04 22:00:00           1113.0               NaN      56.1        NaN\n2004-04-04 23:00:00           1196.0               NaN      58.8        NaN\n2004-04-05 00:00:00           1188.0            1224.0      60.8       56.5"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#window-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#window-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Window features",
    "text": "Window features\nWindow features represent rolling statistics (such as averages) calculated over a fixed window of previous time points. These are useful in capturing short-term trends in the data. For example, the mean CO concentration over the past three or seven hours might provide valuable information for predicting future values.\nIn the code, we generate window features by computing the rolling mean over windows of 3 and 7 hours. The .shift() function is applied to ensure that the calculated window statistics are available only for past observations (i.e., the mean is based on past data up to the current time point). This ensures that the model respects the forecasting principle of only using information that would have been available at the time of prediction.\n\nwindow_features_df = pd.DataFrame(index = air_quality_df.index)\n\nwindows = [3, 7]\n\nfor temp_col in air_quality_df.columns:\n  for temp_win in windows:\n    window_features_df[temp_col + \"_win_\" + str(temp_win)] = (\n      air_quality_df[temp_col]\n      .rolling(window = temp_win).mean()\n      .shift(freq = \"1h\")\n      )\n\nwindow_features_df.head(8)\n\n                     CO_sensor_win_3  CO_sensor_win_7   RH_win_3   RH_win_7\nDate_Time                                                                  \n2004-04-04 00:00:00              NaN              NaN        NaN        NaN\n2004-04-04 01:00:00              NaN              NaN        NaN        NaN\n2004-04-04 02:00:00              NaN              NaN        NaN        NaN\n2004-04-04 03:00:00      1184.666667              NaN  59.366667        NaN\n2004-04-04 04:00:00      1151.333333              NaN  62.200000        NaN\n2004-04-04 05:00:00      1089.000000              NaN  64.233333        NaN\n2004-04-04 06:00:00      1054.000000              NaN  65.600000        NaN\n2004-04-04 07:00:00      1037.333333      1112.857143  66.966667  63.428571\n\n\nLet’s verify the calculation of the 3-hour window feature manually. We want to compute the mean CO concentration for the hours leading up to “2004-04-04 03:00:00” (i.e., using data from “2004-04-04 00:00:00” to “2004-04-04 02:00:00”).\n\nexpected_value = air_quality_df.loc[\n  (air_quality_df.index &gt;= pd.Timestamp(\"2004-04-04 00:00:00\")) &\n  (air_quality_df.index &lt;= pd.Timestamp(\"2004-04-04 02:00:00\"))\n  ][\"CO_sensor\"].mean()\n\nexpected_value = round(expected_value,3)\n\ncalculated_value = window_features_df.loc[\nwindow_features_df.index == pd.Timestamp(\"2004-04-04 03:00:00\")\n][\"CO_sensor_win_3\"].iloc[0]\n\ncalculated_value = round(calculated_value,3)\n\nif (expected_value == calculated_value):\n  print(f'''\n  the expected value is {expected_value}, the calculated value is {calculated_value}. \n  We're good!\n  ''')\n\n\n  the expected value is 1184.667, the calculated value is 1184.667. \n  We're good!"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#periodic-features",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#periodic-features",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Periodic features",
    "text": "Periodic features\nCertain time-related features, such as the month or hour, follow a cyclical pattern. For instance, December (month 12) is closer to January (month 1) than it is to April (month 4), even though 12 is numerically farther from 1 than from 4. To capture this cyclical nature, we can transform these features using periodic functions such as sine and cosine.\nBy converting numerical time features into cyclical features, we help the model learn seasonal patterns more effectively. For this purpose, we use the feature_engine library to create these cyclical features for our dataset.\n\nfrom feature_engine.creation import CyclicalFeatures\n\ncyclical = CyclicalFeatures(\n  drop_original = True,\n)\n\ncyclical_df = cyclical.fit_transform(calendar_df)\n\ncyclical_df.head()\n\n                     Month_sin  Month_cos  ...  Hour_sin  Hour_cos\nDate_Time                                  ...                    \n2004-04-04 00:00:00   0.866025       -0.5  ...  0.000000  1.000000\n2004-04-04 01:00:00   0.866025       -0.5  ...  0.269797  0.962917\n2004-04-04 02:00:00   0.866025       -0.5  ...  0.519584  0.854419\n2004-04-04 03:00:00   0.866025       -0.5  ...  0.730836  0.682553\n2004-04-04 04:00:00   0.866025       -0.5  ...  0.887885  0.460065\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "topics/tabularizing_time_series/demo_feature_egineering.html#save-processed-data",
    "href": "topics/tabularizing_time_series/demo_feature_egineering.html#save-processed-data",
    "title": "Forecasting demonstration - feature engineering",
    "section": "Save processed data",
    "text": "Save processed data\nIn the final step of preprocessing, we must address the missing values that have been introduced during feature engineering, particularly in the creation of lag and window features. These missing values arise because, for example, a 24-hour lag feature requires data from 24 hours prior, which is unavailable for the first 24 observations. Similarly, window features, such as rolling averages, rely on past data over a specified period, leading to NA values at the start of the series where insufficient prior data exists.\nThere are generally two approaches to handle these missing values: imputation or deletion. Imputation involves replacing missing values with substitutes, such as the mean or median of the available data. However, given that the number of missing values is relatively small in this case, and since dropping rows with missing data simplifies the process, we will opt to drop the rows containing missing values. This ensures that the final dataset is complete and ready for modeling without introducing any potential bias from imputation.\nOnce the missing values are handled, the processed dataset, which includes the original data along with the engineered features (such as lag, window, and cyclical features), is saved to a CSV file for further use.\n\n\nprocessed_df = pd.concat([air_quality_df, calendar_df,lag_features_df, cyclical_df], axis = 1)\n\nprocessed_df.dropna(inplace = True)\n\nprocessed_df.to_csv(os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\" + \"\\\\data\\\\air_quality_processed_df.csv\")"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html",
    "href": "topics/tabularizing_time_series/forecasting_demo.html",
    "title": "Forecasting demonstration - EDA",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#data-loading",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#data-loading",
    "title": "Forecasting demonstration - EDA",
    "section": "Data loading",
    "text": "Data loading\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_quality.csv\"\n\nair_quality_df = pd.read_csv(file_path,\n                             index_col = \"Date_Time\")\n\nair_quality_df.index = pd.to_datetime(air_quality_df.index)\n\nThe dataset air_quality_df is loaded from a CSV file and indexed by a column named Date_Time, which represents timestamps of the air quality readings. This timestamp index is essential for time series analysis, allowing us to analyze the data in chronological order. The dataset includes sensor readings such as CO_sensor, which captures the concentration of carbon monoxide (CO) in the air, and RH, which records relative humidity (RH) levels. These two variables provide environmental and pollutant measurements that will be key in our analysis.\n\nfor temp_col in air_quality_df.columns.values:\n  air_quality_df[temp_col].plot(figsize = (20,6))\n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  plt.tick_params(axis='both', which='major', labelsize=16)\n  plt.xlabel('')  # Disable x-axis label\n  plt.ylabel('')  # Disable y-axis label\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we plot each of the columns in the dataset (CO_sensor and RH) to visually explore the time series data. These plots provide a general understanding of how pollutant levels (CO_sensor) and humidity (RH) fluctuate over time. Larger trends, spikes, or patterns such as seasonality may already be visible, and such visualizations are a useful first step before deeper analysis."
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#missing-values",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#missing-values",
    "title": "Forecasting demonstration - EDA",
    "section": "Missing values",
    "text": "Missing values\nTo handle missing data in a time series, we first ensure that our data is uniformly spaced by converting the Date_Time index to an hourly frequency using the .asfreq(\"1h\") method. This step introduces explicit gaps for any missing data points, making them easier to detect. This is important in time series analysis because irregular time steps can mislead algorithms, and most forecasting models require data to be at regular intervals.\n\n\nimpute_df = air_quality_df.asfreq(\"1h\").copy()\n\nfor temp_col in impute_df.columns:\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col]\n  impute_df[temp_col + \"_imputed\"] = impute_df[temp_col + \"_imputed\"].ffill()\n\nIn this block, we handle missing values by forward-filling the gaps (.ffill()), which propagates the last observed value forward until a new valid observation is encountered. This is a common imputation technique for time series data, especially in scenarios where missing values are sparse or values don’t drastically change within short time frames.\nWe create new columns (e.g., CO_sensor_imputed, RH_imputed) that store the imputed data, while retaining the original columns for comparison and visualization of the missing values.\nNext, we will overlay the imputed values onto the original time series to visually highlight where data was missing and how it was filled.\n\nfor temp_col in [\"CO_sensor\",\"RH\"]:\n  \n  ax = impute_df[temp_col].plot(figsize = (20,6))\n  \n  impute_df[impute_df[temp_col].isnull()][temp_col + \"_imputed\"].plot(\n    ax = ax,legend = False,marker = \".\", color = \"red\", linestyle='None')\n    \n  plt.title(temp_col, fontsize=20)  # Increase the title font size\n  \n  plt.tick_params(axis='both', which='major', labelsize=16)\n  \n  plt.xlabel('')  # Disable x-axis label\n  \n  plt.ylabel('')  # Disable y-axis label\n  \n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe purpose of this code block is to visualize the missing data points and the corresponding imputed values. The original data is plotted as a line, and any missing data points that were imputed are marked with red dots. This method of visual comparison allows us to easily inspect where the forward fill occurred and ensures that the imputation technique was applied correctly without distorting the overall time series pattern."
  },
  {
    "objectID": "topics/tabularizing_time_series/forecasting_demo.html#seasonality",
    "href": "topics/tabularizing_time_series/forecasting_demo.html#seasonality",
    "title": "Forecasting demonstration - EDA",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to recurring patterns or cycles in data that occur at regular intervals, such as daily or yearly trends. By grouping the data by time of day (i.e., the hour) and averaging across all observations for that time, we can observe how pollutant concentrations (like CO_sensor) vary over the course of a typical day. This allows us to detect intra-day seasonality, which is particularly relevant for environmental and pollution data that may exhibit daily cycles due to human activity or environmental factors.\n\nhours_con = impute_df.groupby(impute_df.index.time)[\n  \"CO_sensor\"].mean().reset_index()\n\nhours_con.plot(x = 'index', y = 'CO_sensor', legend = False)\nplt.title(\"Pollutant concentration over day time\")\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()"
  },
  {
    "objectID": "topics/tabularizing_time_series/ts_to_table.html",
    "href": "topics/tabularizing_time_series/ts_to_table.html",
    "title": "Tabularizing time series data",
    "section": "",
    "text": "import pandas as pd\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nimport os \n\n\n\nmulti_items_df = pd.read_csv(os.path.expanduser(\"~/Documents\") + \n\"\\\\DS_advanced_website\\\\data\\\\multi_item_sales.csv\")\n\nmulti_items_df[\"date\"] = pd.to_datetime(multi_items_df[\"date\"],\n                                        format = \"%d/%m/%Y\")\n\n\n\n# Auxiliary functions\n\ndef plot_series(data,**kwargs):\n    # Create a plot with two line plots\n    ax = sns.lineplot(x='date', y='sales', data=data)\n    \n    return ax\n\ndef plot_panel(long_df):\n  \n  panel_grid = sns.FacetGrid(long_df, col=\"item_category\",\n                             col_wrap=1,height = 2, aspect = 3)\n                             \n  panel_grid.map_dataframe(plot_series)\n  \n  plt.show()\n\n\nIntro to timeseries\n\nDefinition and example\nUnivariate and multivariate time series\nMultiple time series\nRegular vs irregular time series\nStationary vs non-stationary\n\nTime series data format has (at least) two mandatory components: 1. The data - a column of values 2. The time index - an additional column or (more commonly) as index of the pandas Series or Data Frame\n++ explain wide format (in this example a date column and 4 columns one for sales of each item: ‘beverages’, ‘bakery’, ‘cleaning’, ‘dairy’ )\n\nmulti_items_df.head()\n\n        date  beverages   bakery  cleaning  dairy\n0 2015-01-02        794  285.628       501    426\n1 2015-01-03        938  289.563       470    568\n2 2015-01-04        574  151.744       312    362\n3 2015-01-05       1299  457.543      1047    814\n4 2015-01-06       1028  405.280       831    679\n\n# Create a figure and axis\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(12, 8), sharex=True)\n\n# Plot each series against the date\nfor temp_ind in range(1,multi_items_df.shape[1]):\n  \n  temp_col = multi_items_df.columns.values[temp_ind]\n  \n  multi_items_df.plot(x='date', y=temp_col,ax=axes[temp_ind-1],\n                      title=temp_col.title(), legend = False)\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n++ explain long format (in this example a date column, an item category column and a sales values column)\n\nlong_format = multi_items_df.melt(id_vars = \"date\",\n                                  value_vars = ['beverages', 'bakery',\n                                                'cleaning', 'dairy'],\n                                  var_name = \"item_category\",\n                                  value_name = \"sales\").copy()\n\nlong_format.head()\n\n        date item_category   sales\n0 2015-01-02     beverages   794.0\n1 2015-01-03     beverages   938.0\n2 2015-01-04     beverages   574.0\n3 2015-01-05     beverages  1299.0\n4 2015-01-06     beverages  1028.0\n\n\n\nplot_panel(long_format)"
  },
  {
    "objectID": "topics/trend_features/piecewise_regression.html",
    "href": "topics/trend_features/piecewise_regression.html",
    "title": "Piecewise linear regression",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn import set_config\nfrom sklearn.base import clone\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom sktime.transformations.series.summarize import WindowSummarizer\nfrom sktime.transformations.series.time_since import TimeSince\n\nset_config(transform_output=\"pandas\")\n\nfrom sklearn.metrics import root_mean_squared_error\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nplt.clf()\n\nfig, ax = plt.subplots(figsize=[10, 5])\n\nraw_df.plot(y=\"sales\", marker=\".\", figsize=[10, 5], legend=None, ax=ax)\n\n# ax.set_xlabel(\"Time\")\n# \n# ax.set_ylabel(\"Retail Sales\")\n# \n# ax.set_title(\"Retail Sales\")\n\nax.vlines(\n    [\"2008-01-01\", \"2009-04-01\"],\n    ymin=raw_df[\"sales\"].min(),\n    ymax=raw_df[\"sales\"].max(),\n    color=\"r\",\n    alpha=0.5,\n)\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsplit_point = pd.to_datetime(\"2013-01-01\")\n\ntrain_set = raw_df.loc[raw_df.index &lt;= split_point]\n\ntest_set = raw_df.loc[raw_df.index &gt; split_point]\n\n\n\ntarget = [\"sales\"]\n\nchangepoints = [\n  \"2008-01-01\", # first changepoint\n  \"2009-04-01\"  # second changepoint\n  ]\n  \n# Polynomial time features\ntrend_trans = make_pipeline(\n    TimeSince(), PolynomialFeatures(degree=1, include_bias=False)\n)\n\n\n\nchangepoints_trans = TimeSince(changepoints, positive_only=True)\n\nlag_window_trans = WindowSummarizer(\n    lag_feature={\"lag\": 2},\n    target_cols=target,\n    truncate=\"bfill\",\n    n_jobs = 1\n)\n                               \n# Create features derived independent of one another\ntrans_pipe = make_union(trend_trans, changepoints_trans,lag_window_trans)\n\n# Apply min-max scaling to all the features\ntrans_pipe = make_pipeline(trans_pipe, MinMaxScaler())\n\n\nX_train_processed = trans_pipe.fit_transform(train_set.copy())\n\nC:\\Users\\Home\\AppData\\Local\\Programs\\Python\\PYTHON~1\\Lib\\site-packages\\sktime\\transformations\\series\\summarize.py:299: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  func_dict.loc[:, \"window\"] = func_dict[\"window\"].astype(\"object\")\n\n\n\n\n\ndef make_recursive_forecast(forecast_horizon,X_train, Y_train,\n                            model, preprocess_pipe):\n    \n    # Preprocess data and fit model\n    \n    feature_source_data, model_spec,preprocess_pipe_spec = \\\n    preprocess_data_and_fit_model(X_mat = X_train.copy(),\n                    Y_mat = Y_train.copy(),\n                    model_spec = model,\n                    preprocess_pipe_spec = preprocess_pipe)\n                    \n    \n     \n                    \n    # Make next prediction\n    \n    next_pred = predict_next_point(feature_source_data = feature_source_data,\n                              model_spec = model_spec,\n                              preprocess_pipe_spec = preprocess_pipe_spec)\n                              \n    predictions_df = next_pred.copy()                          \n    \n    # Update feature_source_data and iterate\n    \n    for step in range(1, forecast_horizon):\n    \n      feature_source_data = pd.concat([feature_source_data.copy(),\n                                       next_pred], axis = 0)\n                                       \n      next_pred = predict_next_point(feature_source_data = feature_source_data,\n                                model_spec = model_spec,\n                                preprocess_pipe_spec = preprocess_pipe_spec)\n                                \n      predictions_df = pd.concat([predictions_df.copy(), next_pred],\n                                  axis = 0)\n                                  \n    \n    return predictions_df\n    \n    \n \ndef preprocess_data_and_fit_model(X_mat, Y_mat,preprocess_pipe_spec, model_spec):\n  \n  preprocess_pipe_spec.fit(X_mat.copy())\n  \n  X_mat_processed = preprocess_pipe_spec.transform(X_mat.copy())\n\n  Y_mat_processed = Y_mat.loc[X_mat_processed.index].copy()\n  \n  model_spec.fit(X_mat_processed,Y_mat_processed)\n  \n  feature_source_data = X_mat.copy()\n  \n  return([feature_source_data, model_spec,preprocess_pipe_spec])\n\n\ndef predict_next_point(feature_source_data,model_spec,preprocess_pipe_spec):\n  \n  forecast_index = feature_source_data.index.max() + pd.DateOffset(months = 1)\n  \n  forecast_index_row = pd.DataFrame(data = np.nan,\n                              index = [forecast_index],\n                              columns = [\"sales\"])\n                              \n  feature_source_data = pd.concat([feature_source_data.copy(),\n                                forecast_index_row], axis = 0)\n                                \n  feature_vec = preprocess_pipe_spec.transform(feature_source_data)\n  \n  feature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n  \n  predictions = model_spec.predict(feature_vec)\n  \n  predictions_df = pd.DataFrame(data = predictions,\n                              index = [forecast_index],\n                              columns = [\"sales\"])\n  \n  return predictions_df\n\n\npred = make_recursive_forecast(forecast_horizon = 40,\n                        X_train = train_set.copy(),\n                        Y_train = train_set.copy(),\n                        model = LinearRegression(),\n                        preprocess_pipe = clone(trans_pipe))\n\nC:\\Users\\Home\\AppData\\Local\\Programs\\Python\\PYTHON~1\\Lib\\site-packages\\sktime\\transformations\\series\\summarize.py:299: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  func_dict.loc[:, \"window\"] = func_dict[\"window\"].astype(\"object\")\n\n\n\ncomparison_df = test_set.copy().join(pred.copy(), how = \"inner\",\n                         lsuffix='_actual', rsuffix='_pred')\n\n\ncomparison_df.plot()\n\nplt.show()"
  },
  {
    "objectID": "topics/trend_features/trend_trees.html",
    "href": "topics/trend_features/trend_trees.html",
    "title": "Tree models in trends",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn import set_config\n\nfrom sklearn.base import clone\n\nfrom sklearn.pipeline import make_pipeline, make_union\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sktime.forecasting.trend import PolynomialTrendForecaster\n\nfrom sktime.transformations.series.boxcox import LogTransformer\n\nfrom sktime.transformations.series.detrend import Detrender\n\nfrom sktime.transformations.series.summarize import WindowSummarizer\n\nfrom sktime.transformations.series.time_since import TimeSince\n\nset_config(transform_output=\"pandas\")\n\nfrom sklearn.metrics import root_mean_squared_error\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nplt.clf()\n\nraw_df.plot()\n\nplt.show()"
  },
  {
    "objectID": "topics/trend_features/trend_trees.html#recursive-forecast",
    "href": "topics/trend_features/trend_trees.html#recursive-forecast",
    "title": "Tree models in trends",
    "section": "Recursive forecast",
    "text": "Recursive forecast\n\n\n\ndef make_recursive_forecast(forecast_horizon,X_train, y_train,\n                            model, preprocess_pipe):\n    \n    # Preprocess data and fit model\n    \n    feature_source_data, model_spec,preprocess_pipe_spec = \\\n    preprocess_data_and_fit_model(X_mat = X_train.copy(),\n                    Y_mat = y_train.copy(),\n                    model_spec = model,\n                    preprocess_pipe_spec = preprocess_pipe)\n                    \n    \n     \n                    \n    # Make next prediction\n    \n    next_pred = predict_next_point(feature_source_data = feature_source_data,\n                              model_spec = model_spec,\n                              preprocess_pipe_spec = preprocess_pipe_spec)\n                              \n    predictions_df = next_pred.copy()                          \n    \n    # Update feature_source_data and iterate\n    \n    for step in range(1, forecast_horizon):\n    \n      feature_source_data = pd.concat([feature_source_data.copy(),\n                                       next_pred], axis = 0)\n                                       \n      next_pred = predict_next_point(feature_source_data = feature_source_data,\n                                model_spec = model_spec,\n                                preprocess_pipe_spec = preprocess_pipe_spec)\n                                \n      predictions_df = pd.concat([predictions_df.copy(), next_pred],\n                                  axis = 0)\n                                  \n    \n    return predictions_df\n    \n    \n \ndef preprocess_data_and_fit_model(X_mat, Y_mat,preprocess_pipe_spec, model_spec):\n  \n  preprocess_pipe_spec.fit(X_mat.copy())\n  \n  X_mat_processed = preprocess_pipe_spec.transform(X_mat.copy())\n\n  Y_mat_processed = Y_mat.loc[X_mat_processed.index].copy()\n  \n  model_spec.fit(X_mat_processed,Y_mat_processed)\n  \n  feature_source_data = X_mat.copy()\n  \n  return([feature_source_data, model_spec,preprocess_pipe_spec])\n\n\ndef predict_next_point(feature_source_data,model_spec,preprocess_pipe_spec):\n  \n  forecast_index = feature_source_data.index.max() + pd.DateOffset(months = 1)\n  \n  forecast_index_row = pd.DataFrame(data = np.nan,\n                              index = [forecast_index],\n                              columns = [\"passengers\"])\n                              \n  feature_source_data = pd.concat([feature_source_data.copy(),\n                                forecast_index_row], axis = 0)\n                                \n  feature_vec = preprocess_pipe_spec.transform(feature_source_data)\n  \n  feature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n  \n  predictions = model_spec.predict(feature_vec)\n  \n  predictions_df = pd.DataFrame(data = predictions,\n                              index = [forecast_index],\n                              columns = [\"passengers\"])\n  \n  return predictions_df\n\n\n\n\npred = make_recursive_forecast(forecast_horizon = 35,\n                               X_train = X_train_detrended.copy(),\n                               y_train = X_train_detrended.copy(),\n                               model = DecisionTreeRegressor(),\n                               preprocess_pipe = clone(feat_eng_pipe))\n                               \n\npred_restored = detrend_pipe.inverse_transform(pred.copy())\n\n\ncomparison_df = test_set.copy().join(pred_restored.copy(), how = \"inner\",\n                         lsuffix='_actual', rsuffix='_pred')\n\n\ncomparison_df.plot()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html",
    "href": "topics/ts_decomposition/lowess_decomp.html",
    "title": "LOWESS Decomposition",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nfrom sklearn.metrics import root_mean_squared_error\n\nfrom sklearn.model_selection import KFold\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "href": "topics/ts_decomposition/lowess_decomp.html#lowess-parametres",
    "title": "LOWESS Decomposition",
    "section": "LOWESS parametres",
    "text": "LOWESS parametres\n++ explain the parameters in python implementations: frac - fraction of data for the window size (determines the smoothness) it - number of iterations for robust regression. Also explain the endog and exog parameters\n\n\ny = raw_df[\"sales\"]\n\nx = np.arange(0, len(y))\n\nts_decomp = lowess(endog = y, exog = x, frac = 0.1, it = 3)\n\nraw_df[\"trend_lowess\"] = ts_decomp[:,1]\n\n\nCross validation to select appropriate fraction parameter\n\ndef get_rmse_for_df(X,y, frac_param):\n  KFold_obj = KFold(n_splits = 5, shuffle = True, random_state = 0)\n  rmse_list = []\n  for train_index, test_index in KFold_obj.split(X,y):\n    X_train = X[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X[test_index]\n    y_test = y.iloc[test_index]\n    y_pred = lowess(endog = y_train, exog = X_train, frac = frac_param, xvals = X_test)\n    rmse = root_mean_squared_error(y_pred, y_test)\n    rmse_list.append(rmse)\n  return rmse_list\n\n\nresults = []\n\nfor temp_frac in [0.05,0.1,0.5,1]:\n  rmse_list = get_rmse_for_df(X = x,y = y,frac_param = temp_frac)\n  rmse_df = pd.DataFrame(data = rmse_list, columns = [\"rmse\"])\n  rmse_df[\"frac\"] = temp_frac\n  results.append(rmse_df)\n\n  \npd.concat(results, axis = 0)\n\n           rmse  frac\n0  22981.706955  0.05\n1  22624.297346  0.05\n2  20582.387871  0.05\n3  28572.985707  0.05\n4  21376.838789  0.05\n0  23018.284152  0.10\n1  21215.656841  0.10\n2  20279.923647  0.10\n3  25214.068880  0.10\n4  20387.775112  0.10\n0  26184.512853  0.50\n1  22354.596572  0.50\n2  23964.726098  0.50\n3  25516.588727  0.50\n4  20683.629095  0.50\n0  26343.368462  1.00\n1  23258.024086  1.00\n2  24924.462787  1.00\n3  26254.158157  1.00\n4  20870.150531  1.00"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html",
    "href": "topics/ts_decomposition/transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\nraw_df[\"passengers\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#log-transformation",
    "href": "topics/ts_decomposition/transformations.html#log-transformation",
    "title": "Transformations",
    "section": "Log transformation",
    "text": "Log transformation\n++ explain that sometimes we want to transform a features, for example in order to stabilize the variance\n\nraw_df[\"passengers_log\"] = np.log(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_log\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "href": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "title": "Transformations",
    "section": "Box Cox transformation",
    "text": "Box Cox transformation\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nlambdas_vec = [-1,-0.5,0,0.5,1,2]\n\nplt.clf()\n\nfig,ax = plt.subplots(ncols = 2, nrows = 3,figsize=[25, 15], sharex = True)\n\nax = ax.flatten()\n\nfor ix, temp_lambda in enumerate(lambdas_vec):\n  print(temp_lambda)\n  \n  bc_trans = BoxCoxTransformer(lambda_fixed = temp_lambda)\n  \n  raw_df[\"temp_box_cox\"] = bc_trans.fit_transform(raw_df[\"passengers\"])\n  \n  raw_df.plot(y = \"temp_box_cox\",ax  = ax[ix], label = f\"lambda = {temp_lambda}\")\n  \n  ax[ix].legend()\n  \n  ax[ix].set_xlabel(\"\")\n  \n\nplt.tight_layout()\n  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Guerrero methond\n++ explain briefly the Guerrero method. Emphasize that it’s advantage is the explicit focus on stabilization of variance.\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nbc_guerrero = BoxCoxTransformer(method = \"guerrero\", sp = 12)\n\nraw_df[\"passengers_bc_guerrero\"] = bc_guerrero.fit_transform(raw_df[\"passengers\"])\n\nplt.clf()\n\nraw_df[\"passengers_bc_guerrero\"].plot()\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#moving-averages",
    "href": "topics/ts_decomposition/transformations.html#moving-averages",
    "title": "Transformations",
    "section": "Moving averages",
    "text": "Moving averages\n++ explain the odd moving average window is symmetric (there are equal amount of point on each side of the center). The even size moving average does not have a natural center but we can change the weights to achieve a symmetric window. The weights will not be the same, on the edges the weights will be smaller. Actually in order to make an even size window symmetric we need to apply additional moving average of window 2 so the edge’s weights will be half the other (inside) weights.\n\n\nma_file_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nma_raw_df = pd.read_csv(ma_file_path,index_col = \"date\")\n\nma_raw_df.index = pd.to_datetime(ma_raw_df.index)\n\ndel ma_file_path\n\n\n\nma_df = ma_raw_df.copy()\n\nma_df[\"ma_3\"] = ma_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\nma_df[\"ma_3\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\neven_window_size = 12\n\nma_df[\"ma_2_12\"] = ma_df[\"sales\"].rolling(window = even_window_size).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].rolling(window = 2, center = True).mean()\n\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].shift(- even_window_size // 2)\n\n\nplt.clf()\n\nma_df[\"ma_2_12\"].plot(color = \"steelblue\")\n\nma_df[\"sales\"].plot(color = \"grey\", alpha = 0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne7329f12e3f9c1d7266bdc4614b17630418a7520"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#performance-evaluation",
    "href": "topics/intro/forecaster_demo.html#performance-evaluation",
    "title": "Forecasting demonstration",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nprint(f\"Train set RMSE is {np.round(root_mean_squared_error(y_train, y_train_pred),2)}\")\n\nTrain set RMSE is 23.22\n\nprint(f\"Test set RMSE is {np.round(root_mean_squared_error(y_test, y_test_pred),2)}\")\n\nTest set RMSE is 49.64"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#summary",
    "href": "topics/intro/forecaster_demo.html#summary",
    "title": "Forecasting demonstration",
    "section": "Summary",
    "text": "Summary\nIn this demonstration, we explored the basic steps of a time series forecasting task using airline passenger data. We processed the data by creating lagged features, trained a simple Linear Regression model, and evaluated the model’s performance using RMSE. This step-by-step approach shows how to handle feature engineering, model training, and performance evaluation in a forecasting scenario, providing a foundation for more advanced time series techniques."
  }
]