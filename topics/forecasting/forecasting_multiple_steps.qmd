---
title: "Forecasting multiple periods (steps) ahead"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python}

import numpy as np

import pandas as pd

from feature_engine.creation import CyclicalFeatures

from feature_engine.datetime import DatetimeFeatures

from feature_engine.imputation import DropMissingData

from feature_engine.selection import DropFeatures 

from feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)

from sklearn.linear_model import Lasso

from sklearn.multioutput import MultiOutputRegressor

from sklearn.metrics import root_mean_squared_error

from sklearn.pipeline import Pipeline

import matplotlib.pyplot as plt

import os 

```

Here, we want to demonstrate how to forecast multiple period (step) using a pipeline approach. 

```{python import_data}

file_path = os.path.expanduser("~/Documents") + "\\DS_advanced_website\\data\\example_air_quality.csv"

raw_df = pd.read_csv(file_path,
                             index_col = "Date_Time")

raw_df.index = pd.to_datetime(raw_df.index)

```

## Pipeline

The following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.

```{python define_preprocessing}

date_time_feat = DatetimeFeatures(
  variables = "index",
  features_to_extract = ["month","week","day_of_week",
                         "day_of_month","hour","weekend"]
                         )

lag_feat = LagFeatures(
  variables = ["CO_sensor","RH"],
  freq = ["1h","24h"],
  missing_values = "ignore"
)


window_feat = WindowFeatures(
  variables = ["CO_sensor","RH"],
  window = "3h",
  freq = "1h",
  missing_values = "ignore"
)


cyclical_feat = CyclicalFeatures(
  variables = ["month","hour"],
  drop_original = False
)


na_drop = DropMissingData()

drop_feat = DropFeatures(features_to_drop = ["CO_sensor","RH"])


trans_pipe = Pipeline(
  [("date_time_features",date_time_feat),
   ("lag_features",lag_feat),
   ("window_features",window_feat),
   ("periodic_features",cyclical_feat),
   ("drop_missing_values",na_drop),
   ("drop_original_features",drop_feat)
  ]
  
)


```

The pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.

```{python pipeline}

trans_pipe = Pipeline(
  [("date_time_features",date_time_feat),
   ("lag_features",lag_feat),
   ("window_features",window_feat),
   ("periodic_features",cyclical_feat),
   ("drop_missing_values",na_drop),
   ("drop_original_features",drop_feat)
  ]
  
)

```

## Train and test split

In time series forecasting, itâ€™s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is "2005-03-04". If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.

```{python }

split_point = pd.Timestamp("2005-03-04")

X_train = raw_df.loc[raw_df.index < split_point]

X_test = raw_df.loc[raw_df.index >= split_point - pd.offsets.Hour(24)]

y_train = raw_df.loc[raw_df.index < split_point,"CO_sensor"]

y_test = raw_df.loc[raw_df.index >= split_point - pd.offsets.Hour(24),"CO_sensor"]


```


Since we want to make predictions for multiple periods now instead of one series (vector) of target feature we will have multiple series (matrix of vectors), one for each forecast horizon. This will resolve some missing data in the target matrix because for each forecasting time point we need the next (future) 24 time points. For our last data point we don't have future values at all, for the one before the last we only have one and so on. We handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the target matrix but remain in the train matrix. To resolve this, we need to realign the target vector with the processed features by using `.loc` to filter both the on the updated index.


```{python construct_Y_matrix}

forecast_horizon = 24

Y_train = pd.DataFrame(index = X_train.index)

Y_test = pd.DataFrame(index = X_test.index)

for temp_h in range(forecast_horizon):
  Y_train[f"h_{temp_h}"] = X_train["CO_sensor"].shift(-temp_h, freq = "h")
  Y_test[f"h_{temp_h}"] = X_test["CO_sensor"].shift(-temp_h, freq = "h")


print(Y_train.iloc[0:5,0:5])



```




```{python align_X_Y_matrix}

Y_train = Y_train.dropna().copy()

Y_test = Y_test.dropna().copy()

X_train = X_train.loc[Y_train.index]

X_test = X_test.loc[Y_test.index]

```


## Preprocess data

```{python preprocess_data}

X_train_processed = trans_pipe.fit_transform(X_train.copy())

X_test_processed = trans_pipe.fit_transform(X_test.copy())

Y_train = Y_train.loc[X_train_processed.index]

Y_test = Y_test.loc[X_test_processed.index]

```


After preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data.

## Prediction

```{python }

lasso_model = MultiOutputRegressor(Lasso())

lasso_model.fit(X_train_processed, Y_train)

```

After fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.

```{python predict}

predictions_mat = lasso_model.predict(X_test_processed)

predictions_mat = pd.DataFrame(predictions_mat)

```

## Evaluation

Finally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.

```{python evaluation}

rmse_df = []

for temp_hor in range(Y_test.shape[1]):
  y_test = Y_test.iloc[:,temp_hor]
  pred_vec = predictions_mat.iloc[:,temp_hor]
  rmse = np.round(root_mean_squared_error(y_test, pred_vec), 4)
  rmse_df.append(pd.DataFrame(columns = ["horizon","rmse"],
                              data = [[temp_hor,rmse]]))

rmse_df = pd.concat(rmse_df, axis = 0)

```


```{python plot_rmse}

rmse_df.plot(x = "horizon", y = "rmse")

plt.show()

```

