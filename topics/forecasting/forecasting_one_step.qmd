---
title: "Forecasting one period (step) ahead"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python}

import numpy as np

import pandas as pd

from feature_engine.creation import CyclicalFeatures

from feature_engine.datetime import DatetimeFeatures

from feature_engine.imputation import DropMissingData

from feature_engine.selection import DropFeatures 

from feature_engine.timeseries.forecasting import (LagFeatures, WindowFeatures)

from sklearn.linear_model import Lasso

from sklearn.metrics import root_mean_squared_error

from sklearn.pipeline import Pipeline

import matplotlib.pyplot as plt

import os 

```

Here, we want to demonstrate how to forecast the next period (step) using a pipeline approach. The pipeline encapsulates all preprocessing operations such as feature engineering, imputation, and feature selection into a single streamlined process. This allows for consistency and efficiency when handling time series data. It is crucial to ensure that the explanatory features (the X matrix) and the target feature (the y vector) are properly aligned in time. Misalignment can lead to look-ahead bias, where future information is inappropriately used in the model training phase, resulting in overoptimistic performance estimates. Careful attention is also required to avoid data leakage, ensuring that the model does not have access to information from the future when forecasting.

```{python import_data}

file_path = os.path.expanduser("~/Documents") + "\\DS_advanced_website\\data\\example_air_quality.csv"

raw_df = pd.read_csv(file_path,
                             index_col = "Date_Time")

raw_df.index = pd.to_datetime(raw_df.index)

```

## Pipeline

The following steps involve extracting essential features from the datetime index, creating lag and window-based features, and transforming cyclical features like month and hour into sinusoidal form to capture seasonality. Additionally, any missing data is handled and specific features are dropped before fitting the model. The pipeline approach is utilized to bundle these operations into a single object for convenience and reusability.

```{python define_preprocessing}

date_time_feat = DatetimeFeatures(
  variables = "index",
  features_to_extract = ["month","week","day_of_week",
                         "day_of_month","hour","weekend"]
                         )

lag_feat = LagFeatures(
  variables = ["CO_sensor","RH"],
  freq = ["1h","24h"],
  missing_values = "ignore"
)


window_feat = WindowFeatures(
  variables = ["CO_sensor","RH"],
  window = "3h",
  freq = "1h",
  missing_values = "ignore"
)


cyclical_feat = CyclicalFeatures(
  variables = ["month","hour"],
  drop_original = False
)


na_drop = DropMissingData()

drop_feat = DropFeatures(features_to_drop = ["CO_sensor","RH"])


trans_pipe = Pipeline(
  [("date_time_features",date_time_feat),
   ("lag_features",lag_feat),
   ("window_features",window_feat),
   ("periodic_features",cyclical_feat),
   ("drop_missing_values",na_drop),
   ("drop_original_features",drop_feat)
  ]
  
)


```

The pipeline defined here combines feature engineering tasks such as creating lag features, window statistics, and cyclical features, along with handling missing data and dropping unnecessary columns. This ensures that all transformations are consistently applied to both the training and testing sets, preventing leakage of future information.

```{python pipeline}

trans_pipe = Pipeline(
  [("date_time_features",date_time_feat),
   ("lag_features",lag_feat),
   ("window_features",window_feat),
   ("periodic_features",cyclical_feat),
   ("drop_missing_values",na_drop),
   ("drop_original_features",drop_feat)
  ]
  
)

```

## Train and test split

In time series forecasting, itâ€™s important to account for the lagged features when splitting the data into train and test sets. The test set should contain enough prior data to compute the lagged and window-based features accurately. In this case, the longest lag is 24 hours, so we need to ensure that the test set includes the first forecasting point and at least 24 hours before it. We will split the data so that the last month is allocated to the test set. The chosen split point is "2005-03-04". If we have enough data in order to be on the safe side we can completely eliminate any overlap between the train and the test set by limiting the train set to data before split point shifted by the offset range.

```{python }

split_point = pd.Timestamp("2005-03-04")

X_train = raw_df.loc[raw_df.index < split_point]

X_test = raw_df.loc[raw_df.index >= split_point - pd.offsets.Hour(24)]

y_train = raw_df.loc[raw_df.index < split_point,"CO_sensor"]

y_test = raw_df.loc[raw_df.index >= split_point - pd.offsets.Hour(24),"CO_sensor"]


```

## Preprocess data

```{python preprocess_data}

X_train_processed = trans_pipe.fit_transform(X_train.copy())

X_test_processed = trans_pipe.fit_transform(X_test.copy())

```

During preprocessing, we apply transformations that handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the training features but remain in the target vector. To resolve this, we need to realign the target vector with the processed features by using `.loc` to filter both the training and test target vectors based on the updated index of the processed feature sets.

```{python }

print(y_train.shape)

y_train = y_train.loc[X_train_processed.index]

print(y_train.shape)

y_test = y_test.loc[X_test_processed.index]

```


After preprocessing and ensuring that the features and target are properly aligned, we can train the forecasting model. Here, we use Lasso regression, a linear model that performs both variable selection and regularization to prevent overfitting. This model is suitable for time series forecasting with many features, especially when we want to avoid overly complex models that may not generalize well to unseen data.

## Prediction

```{python }

lasso_model = Lasso()

lasso_model.fit(X_train_processed, y_train)

```

After fitting the Lasso model, we can generate predictions for the test set. The model uses the processed test features to forecast the target variable (CO sensor readings) for the next period.

```{python predict}

predictions_vec = lasso_model.predict(X_test_processed)

```

## Evaluation

Finally, we evaluate the performance of our model using Root Mean Squared Error (RMSE). RMSE is a widely used metric for regression tasks, as it gives us an indication of how well the predicted values match the actual values. Lower RMSE values indicate better performance.

```{python evaluation}

rmse = np.round(root_mean_squared_error(y_test, predictions_vec), 4)

print(f"The root mean squared error on the test set is {rmse}")

```