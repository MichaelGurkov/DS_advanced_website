---
title: "LOWESS to Extract the Trend"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python}

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import os 

from statsmodels.nonparametric.smoothers_lowess import lowess

from sklearn.metrics import mean_squared_error

from sklearn.model_selection import KFold

```

```{python import_data}

file_path = os.path.expanduser("~/Documents") + "\\DS_advanced_website\\data\\example_retail_sales.csv"

raw_df = pd.read_csv(file_path,index_col = "date")

raw_df.index = pd.to_datetime(raw_df.index)

```

## Introduction

In time series analysis, data often contains inherent noise that can obscure trends and patterns, making it challenging to analyze or forecast accurately. Smoothing techniques are valuable for filtering out this noise, and one popular method is Locally Weighted Scatterplot Smoothing (LOWESS). LOWESS is a non-parametric regression method that applies localized, weighted linear regressions to generate a smoothed trend line over the data. This tutorial provides an in-depth guide to using LOWESS for time series decomposition, including an explanation of its parameters and a demonstration of cross-validation to select the optimal smoothing factor. By the end, you’ll understand how to tune LOWESS to achieve an effective decomposition of your data and isolate its trend component.

We will use the retail sales dataset.

```{python plot_data}

raw_df["sales"].plot()

plt.show()

```

## LOWESS Parameters

LOWESS, or Locally Weighted Scatterplot Smoothing, is a method used to smooth data points in a time series or other types of datasets with a lot of noise. It does so by fitting multiple linear regressions over subsets of data, or "windows." In the Python implementation of LOWESS from the `statsmodels` library, several key parameters influence the outcome of this smoothing. These include `frac`, `it`, `endog`, and `exog`.

- **frac**: This parameter controls the fraction of data points used to compute each local regression. It effectively determines the window size in the smoothing process. A smaller `frac` value means that fewer points are used in each regression, leading to a curve that follows the data more closely (less smoothing). A larger `frac` value, in contrast, results in greater smoothing as more points are included in each local regression.
  
- **it**: The `it` parameter specifies the number of iterations in the robust regression process. Robust regression helps handle outliers in the data by down-weighting points that deviate significantly from the trend. Increasing `it` results in more iterations to identify and adjust for outliers, potentially improving the robustness of the trend line.

- **endog** and **exog**: These parameters define the endogenous (dependent) and exogenous (independent) variables for the LOWESS fitting process. In this case, `endog` (or `y`) represents the variable we wish to smooth (e.g., "sales" in a time series), while `exog` (or `x`) is the independent variable, typically the index of the dataset or time variable. Specifying `endog` and `exog` allows LOWESS to map out the trend of `endog` against `exog`.

The following code snippet applies LOWESS to our dataset, using the specified parameters to calculate a smoothed trend line for a time series of sales data.

A too-high `frac` value can result in over-smoothing, where essential details in the data are averaged out and the trend appears excessively flat. Conversely, a too-low `frac` value may under-smooth the data, capturing noise and failing to reveal the true trend. Here, we intentionally set a higher `frac` value to demonstrate some degree of over-smoothing; in the next section, we will determine an optimal `frac` value using cross-validation.

```{python lowess}

y = raw_df["sales"]

x = np.arange(0, len(y))

ts_decomp = lowess(endog=y, exog=x, frac=0.5, it=3)

raw_df["trend_lowess"] = ts_decomp[:,1]

```

```{python plot_data_and_lowess_trend}

plt.clf()

raw_df["sales"].plot(color="steelblue")

raw_df["trend_lowess"].plot(color="orange")

plt.show()

```

### Cross-Validation to Select the Appropriate Fraction Parameter

To ensure that the LOWESS smoothing is optimally tuned for our data, we can use cross-validation to select the best `frac` parameter. By testing multiple `frac` values, we aim to find a balance between smoothness and accuracy, ultimately minimizing the root mean squared error (RMSE) between observed and predicted values in a test set. The `KFold` cross-validation approach is used here to split the dataset into training and test sets multiple times, thereby enhancing the reliability of our RMSE estimates.

In the function `get_rmse_for_df`, we implement this cross-validation procedure for each candidate `frac` value. The function uses the KFold technique, splitting the data into five folds and iterating over them to calculate the RMSE of predictions for each fold. By doing this, we measure how well each `frac` parameter fits the data, and select the one with the lowest average RMSE, which indicates the optimal balance of fit and smoothness.

In the code snippet below, we calculate the RMSE for various values of `frac` and collect the results. This allows us to compare different `frac` settings and choose the one that minimizes error.

```{python cross_validation}

def get_rmse_for_df(X, y, frac_param):
  KFold_obj = KFold(n_splits=5, shuffle=True, random_state=0)
  rmse_list = []
  for train_index, test_index in KFold_obj.split(X, y):
    X_train = X[train_index]
    y_train = y.iloc[train_index]
    X_test = X[test_index]
    y_test = y.iloc[test_index]
    y_pred = lowess(endog=y_train, exog=X_train, frac=frac_param, xvals=X_test)
    rmse = np.sqrt(mean_squared_error(y_pred, y_test))
    rmse_list.append(rmse)
  return rmse_list


results = []

for temp_frac in np.arange(0.05, 1.02, 0.05):
  rmse_list = get_rmse_for_df(X=x, y=y, frac_param=temp_frac)
  rmse_df = pd.DataFrame(data=rmse_list, columns=["rmse"])
  rmse_df["frac"] = temp_frac
  results.append(rmse_df)

  
results = pd.concat(results, axis=0)
  
```

```{python plot_cross_validation}

cv_plot_df = results.groupby("frac")["rmse"].agg(["mean", "std"]).reset_index()

plt.figure(figsize=(8, 5))

plt.errorbar(cv_plot_df['frac'],
             cv_plot_df['mean'],
             yerr=cv_plot_df['std'],
             fmt='o',
             capsize=5,
             capthick=1,
             elinewidth=1)

plt.title('Mean RMSE with Standard Deviation Error Bars')

plt.show()

```

The RMSE values have a considerable spread, as indicated by the length of the error bars. A large spread (standard deviation) suggests that there is significant variability in RMSE across the cross-validation folds. Despite this variability, we will proceed with the minimal `frac` value identified, as it represents an improvement over the higher value used in the previous section. This will allow us to demonstrate how the optimal value enhances the balance between smoothness and trend accuracy.

```{python best_frac}

best_frac = cv_plot_df.sort_values(["mean"]).iloc[0,0]

print(f"The best frac value is {np.round(best_frac,2)}")

```

```{python best}

ts_decomp = lowess(endog=y, exog=x, frac=best_frac, it=3)

raw_df["trend_lowess_best"] = ts_decomp[:,1]


plt.clf()

raw_df["sales"].plot(color="steelblue")

raw_df["trend_lowess_best"].plot(color="orange")

plt.show()

```

## Summary

In this tutorial, we explored how to use LOWESS for time series decomposition and trend extraction. By understanding and adjusting the parameters of `frac`, `it`, `endog`, and `exog`, we can fine-tune the smoothing process to better reveal underlying trends in our data. We also implemented a cross-validation technique to help identify the optimal `frac` parameter, balancing accuracy and smoothness. This approach is especially useful in time series analysis, where accurate trend estimation plays a key role in forecasting and insights. Through LOWESS and parameter tuning, we can effectively isolate and analyze the trends within complex datasets, enhancing our understanding of the data’s behavior over time.