---
title: "Forecasting demonstration"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python}

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import os 

from feature_engine.timeseries.forecasting import LagFeatures

from feature_engine.imputation import DropMissingData

from feature_engine.selection import DropFeatures 

from sklearn.pipeline import Pipeline

from sklearn.linear_model import LinearRegression

from sklearn.metrics import root_mean_squared_error

```

In this section, we provide a brief demonstration of a forecasting problem using a simple Linear Regression model. The dataset used for this example consists of historical airline passenger numbers. Our goal is to demonstrate the forecasting process, including key steps such as feature engineering, model fitting, and forecast evaluation.

## Data loading

The dataset contains monthly data on the number of airline passengers from January 1949 to December 1960. This time series dataset is commonly used in forecasting examples due to its clear seasonal patterns and upward trend over time. The time index of the data represents the first day of each month, and the target variable is the number of passengers. The following code loads the dataset, ensures the date column is correctly parsed as a datetime object, and sets it as the index of the DataFrame.

```{python import_data}

file_path = os.path.expanduser("~/Documents") + "\\DS_advanced_website\\data\\example_air_passengers.csv"

raw_df = pd.read_csv(file_path,index_col = "date")

raw_df.index = pd.to_datetime(raw_df.index)

```

# Split the data into training and testing sets

In this step, we divide the dataset into training and testing sets. The training set includes data up until April 1957, which allows the model to learn patterns from the historical data. The test set contains data from May 1957 onward, which will be used to evaluate how well the model generalizes to unseen data. Splitting the dataset this way helps assess the model's performance on future data points.

```{python split_data}

split_date = pd.to_datetime("1957-04-01")

train_set = raw_df.iloc[raw_df.index <= split_date]

test_set = raw_df.iloc[raw_df.index > split_date]

```

# Feature engineering

In forecasting, the target variable is the one we want to predict. In this case, the target is the `passengers` column. We will use the number of passengers from the previous month (`passengers_lag_1`) as a feature. This approach is called lagging, and it helps capture the temporal dependencies in the data. The naive approach assumes that the forecast for the next time step will be the same as the last known value, we will follow this approach by looking at first lag.

The following code sets up a pipeline that performs feature engineering. Specifically, it generates a lagged version of the `passengers` variable, drops the original target variable from the feature set, and removes any rows with missing values (which occur due to lagging). This pipeline will be applied to both the training and testing sets.

```{python feature_engineer}

target = ["passengers"]

lag_trans = LagFeatures(variables = target,
                        freq = ["1MS"])
                        
target_drop_trans = DropFeatures(features_to_drop = target)

na_drop_trans = DropMissingData()

feature_engine_pipe = Pipeline([("get_first_lag", lag_trans),
                                ("remove_target", target_drop_trans),
                                ("drop_missing_values", na_drop_trans)])

X_train_processed = feature_engine_pipe.fit_transform(train_set.copy())

# Aligning target feature with the predictors in the train set
# (because of NA's removal)

y_train = train_set[target].loc[X_train_processed.index]


X_test_processed = feature_engine_pipe.transform(test_set.copy())

y_test = test_set[target].loc[X_test_processed.index]

```

# Model fitting

Now that we have our features prepared, we can proceed to model fitting. In this demonstration, we use a basic Linear Regression model to predict the number of passengers based on the lagged feature (the number of passengers from the previous month). Although Linear Regression is a simple model, it is often used as a baseline for forecasting tasks. This step illustrates how the processed features are used to train the model.

```{python fit_model}

lin_reg = LinearRegression()

lin_reg.fit(X_train_processed, y_train)

```

# Making predictions

After fitting the model on the training data, we can generate predictions for both the training set and the test set. The predictions from the test set will allow us to assess the model's performance on unseen data, helping us understand how well the model generalizes to future values.

```{python predictions}

y_train_pred = lin_reg.predict(X_train_processed)

y_test_pred = lin_reg.predict(X_test_processed)

```

## Performance evaluation

To evaluate the model's performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.

```{python calculate_rmse}


print(f"Train set RMSE is {np.round(root_mean_squared_error(y_train, y_train_pred),2)}")



print(f"Test set RMSE is {np.round(root_mean_squared_error(y_test, y_test_pred),2)}")


```

## Summary

In this demonstration, we explored the basic steps of a time series forecasting task using airline passenger data. We processed the data by creating lagged features, trained a simple Linear Regression model, and evaluated the model's performance using RMSE. This step-by-step approach shows how to handle feature engineering, model training, and performance evaluation in a forecasting scenario, providing a foundation for more advanced time series techniques.