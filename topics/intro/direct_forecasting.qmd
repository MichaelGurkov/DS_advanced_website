---
title: "Forecasting interval (multiple periods ahead) - direct approach"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python import_libraries}

import numpy as np

import pandas as pd

from feature_engine.imputation import DropMissingData

from feature_engine.selection import DropFeatures 

from feature_engine.timeseries.forecasting import LagFeatures

from sklearn.linear_model import LinearRegression

from sklearn.multioutput import MultiOutputRegressor

from sklearn.metrics import root_mean_squared_error

from sklearn.pipeline import Pipeline

import matplotlib.pyplot as plt

import os 

```

++ explain that we want to demonstrate how to forecast multiple period (step) using a pipeline approach. we will use a direct approach - we'll make a forecast for each time point in the forecast interval that mean making simultaneous forecast for several horizons. For demonstration we'll use a forecast interval of three periods ahead (so we need to make a forecast for the next value, the one after the next and the one after him.)

```{python import_data}

file_path = os.path.expanduser("~/Documents") + "\\DS_advanced_website\\data\\example_air_passengers.csv"

raw_df = pd.read_csv(file_path,index_col = "date")

raw_df.index = pd.to_datetime(raw_df.index)

```

# Split the data into training and testing sets

In this step, we divide the dataset into training and testing sets. The training set includes data up until April 1957, which allows the model to learn patterns from the historical data. The test set contains data from May 1957 onward, which will be used to evaluate how well the model generalizes to unseen data. Splitting the dataset this way helps assess the model's performance on future data points.

```{python split_data}

split_date = pd.to_datetime("1957-04-01")

train_set = raw_df.iloc[raw_df.index <= split_date]

test_set = raw_df.iloc[raw_df.index > split_date]

```

# Feature engineering

In forecasting, the target variable is the one we want to predict. In this case, the target is the `passengers` column. We will use the number of passengers from the previous month (`passengers_lag_1`) as a feature. This approach is called lagging, and it helps capture the temporal dependencies in the data. The naive approach assumes that the forecast for the next time step will be the same as the last known value, we will follow this approach by looking at first lag.

The following code sets up a pipeline that performs feature engineering. Specifically, it generates a lagged version of the `passengers` variable, drops the original target variable from the feature set, and removes any rows with missing values (which occur due to lagging). This pipeline will be applied to both the training and testing sets.

```{python feature_engineer}

target = ["passengers"]

lag_trans = LagFeatures(variables = target,
                        freq = ["1MS"])
                        
target_drop_trans = DropFeatures(features_to_drop = target)

na_drop_trans = DropMissingData()

feature_engine_pipe = Pipeline([("get_first_lag", lag_trans),
                                ("remove_target", target_drop_trans),
                                ("drop_missing_values", na_drop_trans)])

X_train_processed = feature_engine_pipe.fit_transform(train_set.copy())


X_test_processed = feature_engine_pipe.transform(test_set.copy())

```


++ Since we want to make predictions for multiple periods we will have multiple target series , one for each forecast horizon. This will resolve some missing data in the target matrix because for each forecasting time point we need the next (future) 3 time points. For our last data point we don't have future values at all, for the one before the last we only have one and so on. We handle missing data by dropping rows with missing values. This can result in a misalignment between the processed features and the target variable, as some time points are removed from the target matrix but remain in the train matrix. To resolve this, we will filter the data to common dates using .loc


```{python construct_Y_matrix}

Y_train_processed = train_set[target].copy()

Y_test_processed = test_set[target].copy()


forecast_horizon = 3


for temp_h in range(forecast_horizon):
  
  Y_train_processed[f"h_{temp_h}"] = (Y_train_processed[target]
                                      .shift(-temp_h, freq = "MS")
                                      .copy()
                                      )
  
  Y_test_processed[f"h_{temp_h}"] = (Y_test_processed[target]
                                      .shift(-temp_h, freq = "MS")
                                      .copy()
                                      )


Y_train_processed = Y_train_processed.dropna().copy()

Y_train_processed = Y_train_processed.drop(columns = target).copy()

Y_test_processed = Y_test_processed.dropna().copy()

Y_test_processed = Y_test_processed.drop(columns = target).copy()

```


```{python filter_data_to_common_dates}

train_index = X_train_processed.index.intersection(Y_train_processed.index)

test_index = X_test_processed.index.intersection(Y_test_processed.index)

# Filter train data to common dates

X_train_processed = X_train_processed.loc[train_index].copy()

Y_train_processed = Y_train_processed.loc[train_index].copy()

# Filter test data to common dates

X_test_processed = X_test_processed.loc[test_index].copy()

Y_test_processed = Y_test_processed.loc[test_index].copy()



```


# Model fitting

Now that we have our features prepared, we can proceed to model fitting. In this demonstration, we use a basic Linear Regression model to predict the number of passengers based on the lagged feature (the number of passengers from the previous month). Although Linear Regression is a simple model, it is often used as a baseline for forecasting tasks. This step illustrates how the processed features are used to train the model.


```{python fit_model}

lin_reg = MultiOutputRegressor(LinearRegression())

lin_reg.fit(X_train_processed, Y_train_processed)

```


# Making predictions

After fitting the model on the training data, we can generate predictions for both the training set and the test set. The predictions from the test set will allow us to assess the model's performance on unseen data, helping us understand how well the model generalizes to future values.

```{python predictions}

Y_train_pred = lin_reg.predict(X_train_processed)

Y_train_pred = pd.DataFrame(data = Y_train_pred.copy(),
                            index = train_index,
                            columns = [f"pred_{i}" for i in range(forecast_horizon)])


Y_train_pred = Y_train_pred.copy().join(Y_train_processed.copy())


Y_test_pred = lin_reg.predict(X_test_processed)

Y_test_pred = pd.DataFrame(data = Y_test_pred.copy(),
                            index = test_index,
                            columns = [f"pred_{i}" for i in range(forecast_horizon)])
                            
Y_test_pred = Y_test_pred.copy().join(Y_test_processed.copy())



```


## Performance evaluation

To evaluate the model's performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.

```{python calculate_rmse}


for temp_h in range(forecast_horizon):
  
  y_pred_train = Y_train_pred[[f"pred_{temp_h}"]]
  
  y_actual_train = Y_train_processed[f"h_{temp_h}"]
  
  train_rmse = root_mean_squared_error(y_pred_train, y_actual_train)
  
  y_pred_test = Y_test_pred[[f"pred_{temp_h}"]]
  
  y_actual_test = Y_test_processed[f"h_{temp_h}"]
  
  test_rmse = root_mean_squared_error(y_pred_test, y_actual_test)
  
  print(f"Train RMSE for horizon {temp_h} is {np.round(train_rmse,2)},Test RMSE is {np.round(test_rmse,2)}")


```
