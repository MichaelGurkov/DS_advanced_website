[
  {
    "objectID": "topics/ts_decomposition/ts_decomposition_page.html",
    "href": "topics/ts_decomposition/ts_decomposition_page.html",
    "title": "Time Series decomposition methods",
    "section": "",
    "text": "Classical Decomposition\n\n\n\n\n\n\nLowess Decomposition\n\n\n\n\n\n\nSTL Decomposition"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html",
    "href": "topics/ts_decomposition/stl_decomp.html",
    "title": "STL Decomposition",
    "section": "",
    "text": "STL decomposition (Seasonal and Trend decomposition using Loess) is a powerful tool in time series analysis that breaks down a time series into three distinct components: trend, seasonality, and remainder (noise or residual). This decomposition helps in understanding the underlying patterns in the data, isolating cyclical variations, and detecting anomalies. In this tutorial, we will demonstrate how to implement STL decomposition using Python’s statsmodels library, and provide a detailed explanation of key parameters, as well as steps to visualize the results.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#introduction",
    "href": "topics/ts_decomposition/stl_decomp.html#introduction",
    "title": "STL Decomposition",
    "section": "",
    "text": "STL decomposition (Seasonal and Trend decomposition using Loess) is a powerful tool in time series analysis that breaks down a time series into three distinct components: trend, seasonality, and remainder (noise or residual). This decomposition helps in understanding the underlying patterns in the data, isolating cyclical variations, and detecting anomalies. In this tutorial, we will demonstrate how to implement STL decomposition using Python’s statsmodels library, and provide a detailed explanation of key parameters, as well as steps to visualize the results.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#trend",
    "href": "topics/ts_decomposition/stl_decomp.html#trend",
    "title": "STL Decomposition",
    "section": "Trend",
    "text": "Trend\nThe STL implementation in statsmodels.tsa.seasonal offers several parameters that allow customization of the decomposition process. Below are the key parameters we used:\n\nperiod: This parameter specifies the length of one complete cycle of the seasonal component. In this example, we set period=12, assuming the data exhibits monthly seasonality over a year.\nseasonal: This controls the degree of smoothing for the seasonal component. A higher value results in more flexible seasonal components. Here, seasonal=7 indicates moderate smoothing.\nrobust: When set to True, the decomposition becomes robust to outliers. This ensures that the trend and seasonal components are not unduly influenced by extreme values.\n\nOther parameters of the STL class, such as trend or low_pass, are left at their default values, which are typically sufficient for most use cases.\n\n\ndecompostion_df = raw_df.copy()\n\nstl_decomp = STL(endog = decompostion_df[\"sales\"], period = 12, seasonal = 7,\n                 robust = True).fit()\n                 \ndecompostion_df[\"trend\"] = stl_decomp.trend\n\ndecompostion_df[\"seasonality\"] = stl_decomp.seasonal\n\ndecompostion_df[\"remainder\"] = stl_decomp.resid"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#visualization",
    "href": "topics/ts_decomposition/stl_decomp.html#visualization",
    "title": "STL Decomposition",
    "section": "Visualization",
    "text": "Visualization\nAfter performing the decomposition, it’s crucial to visualize the components to better interpret the data. The following code generates individual plots for the trend, seasonality, and remainder components, allowing us to inspect each part separately.\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\ndecompostion_df[\"trend\"].plot(ax = axes[0], title = \"trend\")\ndecompostion_df[\"seasonality\"].plot(ax = axes[1], title = \"seasonality\")\ndecompostion_df[\"remainder\"].plot(ax = axes[2], title = \"remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/stl_decomp.html#summary",
    "href": "topics/ts_decomposition/stl_decomp.html#summary",
    "title": "STL Decomposition",
    "section": "Summary",
    "text": "Summary\nSTL decomposition is an invaluable technique for time series analysis, offering insights into the underlying patterns by breaking the data into trend, seasonal, and remainder components. This tutorial demonstrated the implementation of STL decomposition using Python’s statsmodels library, explaining key parameters such as period, seasonal, and robust. Finally, we visualized the components to provide a clear understanding of how each contributes to the overall time series. With these tools, you can perform detailed analyses and make data-driven decisions based on temporal patterns."
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html",
    "href": "topics/ts_decomposition/class_decomp.html",
    "title": "Classical Decomposition",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#introduction",
    "href": "topics/ts_decomposition/class_decomp.html#introduction",
    "title": "Classical Decomposition",
    "section": "Introduction",
    "text": "Introduction\nIn time series analysis, breaking down a time series into its constituent components can provide valuable insights into the data’s underlying structure. The classical decomposition method is a popular approach that isolates three key components: Trend, Seasonality, and Remainder (also called residuals or noise). The Trend component represents the long-term progression or direction of the data, while Seasonality captures repetitive, cyclical patterns that occur at regular intervals. The Remainder component, meanwhile, includes random variations or noise not accounted for by the trend or seasonality. In the classical decomposition approach, the trend is typically extracted using moving averages, which smooth out short-term fluctuations to reveal the underlying trend. This tutorial demonstrates how to apply classical decomposition to a retail sales dataset to isolate and examine each component, using a moving average to estimate the trend and group-based means to identify seasonality."
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#trend",
    "href": "topics/ts_decomposition/class_decomp.html#trend",
    "title": "Classical Decomposition",
    "section": "Trend",
    "text": "Trend\nIn time series analysis, identifying the trend is crucial to understanding the long-term movement in data, as it shows the general direction or tendency over time. To estimate the trend effectively, we can use a moving average, which smooths out fluctuations, making the trend clearer by reducing the impact of random noise or seasonality.\nThe choice of window size for the moving average is significant: typically, it is set to match the frequency of the seasonality if it is known. For instance, with monthly data exhibiting annual seasonality, setting the window to 12 months captures the yearly cycle. By choosing a window size that encapsulates an entire seasonality cycle, we can “isolate” and average out the seasonality’s effects. If the seasonality frequency is unknown, testing different window sizes and visually evaluating the resulting trend line can help. Choosing a too-small window leads to under-smoothing, which results in a trend line with excessive fluctuations. Conversely, an overly large window causes over-smoothing, where the trend line may appear flat and miss important trends in the data.\nIn the following code, we create both under-smoothed and over-smoothed trend lines to demonstrate these effects.\n\n\ntrend_df = raw_df.copy()\n\neven_win_len = 84\n\ntrend_df[\"over_smoothing\"] = trend_df[\"sales\"].rolling(window = even_win_len).mean()\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].rolling(window = 2, center = True).mean()\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].shift(- even_win_len // 2)\n\ntrend_df[\"under_smoothing\"] = trend_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\ntrend_df[\"sales\"].plot(color = \"grey\", alpha = 0.5)\ntrend_df[\"under_smoothing\"].plot(color = \"steelblue\")\ntrend_df[\"over_smoothing\"].plot(color = \"orange\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nCross-validation for Trend Estimation\nTo ensure that we select an optimal window size for the trend component, cross-validation can be used as a more rigorous method. By partitioning the data into training and test sets and evaluating the trend’s predictive accuracy on unseen data, we can refine our choice of window size based on empirical evidence rather than visual inspection alone. This approach can help balance under-smoothing and over-smoothing, leading to a trend line that effectively represents the underlying pattern in the data."
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#seasonality",
    "href": "topics/ts_decomposition/class_decomp.html#seasonality",
    "title": "Classical Decomposition",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to periodic fluctuations in data that occur at regular intervals due to factors like seasons, quarters, or months. To extract seasonality from a time series, the trend component must first be isolated and removed. This is done by detrending the data: subtracting the trend if the seasonality is additive or dividing by the trend if it is multiplicative. Once detrended, the data can reveal the repetitive seasonal pattern, allowing us to capture the recurring variations.\nIn the following code, we apply a rolling mean to determine the trend, then detrend the data by subtracting this trend from the original sales data. After detrending, we group the data by month to calculate average seasonal effects, which enables us to isolate and visualize the seasonality. The resulting seasonal component shows the monthly effect after excluding the trend.\n\n\nseason_df = raw_df.copy()\n\nseason_df[\"trend\"] = season_df[\"sales\"].rolling(window = 12).mean()\nseason_df[\"trend\"] = season_df[\"trend\"].rolling(window = 2, center = True).mean()\nseason_df[\"trend\"] = season_df[\"trend\"].shift(- 12 // 2)\n\nseason_df[\"detrended_data\"] = season_df[\"sales\"] - season_df[\"trend\"]\n\nseason_df[\"month\"] = season_df.index.month\nseasonality = season_df.groupby(\"month\").mean()[\"detrended_data\"].reset_index()\n\nseasonality.columns = [\"month\",\"seasonality\"]\nseason_df = pd.merge(season_df.copy(), seasonality, on = \"month\", how = \"left\")\n\nseason_df[\"remainder\"] = season_df[\"detrended_data\"] - season_df[\"seasonality\"]\n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\nseason_df[\"trend\"].plot(ax = axes[0], title = \"Trend\")\nseason_df[\"seasonality\"].plot(ax = axes[1], title = \"Seasonality\")\nseason_df[\"remainder\"].plot(ax = axes[2], title = \"Remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#summary",
    "href": "topics/ts_decomposition/class_decomp.html#summary",
    "title": "Classical Decomposition",
    "section": "Summary",
    "text": "Summary\nThrough the classical decomposition method, we successfully isolated the trend, seasonality, and remainder components of a time series, offering a clearer view of each component’s influence on the data. By using moving averages, we smoothed out short-term noise to identify the trend and then detrended the data to reveal seasonality. This process allowed us to break down the time series into interpretable parts, providing insights into underlying patterns and variability. Such decomposition is essential in time series analysis, as it enhances the forecasting accuracy and interpretability of models by focusing on distinct data patterns. This approach is valuable in various domains, from retail sales to finance, where understanding the interplay between trend and seasonal effects is critical for decision-making."
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html",
    "href": "topics/na_imputation/na_imputation.html",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "Handling missing data is a critical step in data preprocessing, as the presence of missing values can disrupt analysis and model performance. This tutorial explores various methods for imputing missing data in time series datasets. These methods range from simple approaches, such as forward and backward filling, to more advanced techniques like spline interpolation and seasonal decomposition with interpolation. The goal is to provide a comprehensive guide to imputation, highlighting the strengths and potential pitfalls of each method while maintaining the integrity of the time series data.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\n\n\n\nThe dataset is loaded and indexed by date. The presence of missing values is identified and summarized to understand the scope of the issue.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_missing.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\nna_idx = raw_df.isnull()\n\n\nprint(f\"There are {raw_df['sales'].isnull().sum()} missing values, these are {np.round(raw_df['sales'].isnull().sum() / len(raw_df)* 100,3)} percent of the data\")\n\nThere are 33 missing values, these are 20.625 percent of the data\n\n\n\n\n\nA plot of the data highlights where the missing values occur, providing a visual understanding of their distribution over time.\n\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nForward filling imputes missing values using the last available value. This method is straightforward but may not reflect actual trends if the missing period spans significant variations in the data.\n\nffill_df = raw_df.ffill()\n\nplt.clf()\n\nax = ffill_df.plot(linestyle=\"-\", marker=\".\")\n\nffill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackward filling replaces missing values with the next available value. This approach can lead to data leakage, as it introduces future information into past time points. Care should be taken when using this method, especially in predictive modeling contexts.\n\nbfill_df = raw_df.bfill()\n\nplt.clf()\n\nax = bfill_df.plot(linestyle=\"-\", marker=\".\")\n\nbfill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear interpolation fills missing values by assuming a linear relationship between neighboring data points. This method is suitable for time series where trends between points are approximately linear.\n\nlin_inter = raw_df.interpolate(method = \"time\")\n\nplt.clf()\n\nax = lin_inter.plot(linestyle=\"-\", marker=\".\")\n\nlin_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpline interpolation uses cubic splines to create a smooth curve through the data points, providing a more sophisticated imputation than linear interpolation. It works well when the data has non-linear patterns, but it may overfit in some cases.\n\nspline_inter = raw_df.interpolate(method = \"spline\", order = 3)\n\nplt.clf()\n\nax = spline_inter.plot(linestyle=\"-\", marker=\".\")\n\nspline_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTL (Seasonal and Trend decomposition using Loess) is used to separate the seasonal, trend, and residual components of the time series. Linear interpolation is applied first because STL cannot handle missing data directly. After seasonal decomposition, imputation is performed on the deseasonalized series, and the seasonal component is added back to reconstruct the series.\n\n\nstl_inter = STL(raw_df.interpolate(method = \"time\"), seasonal = 31).fit()\n\nseasonal_component = stl_inter.seasonal\n\ndeaseasonlised_df = raw_df[\"sales\"] - seasonal_component\n\ndf_inter = deaseasonlised_df.interpolate(method = \"time\")\n\ndf_final = df_inter + seasonal_component\n\ndf_final = df_final.to_frame().rename(columns = {0:\"sales\"})\n\n\nplt.clf()\n\nax = df_final.plot(linestyle=\"-\", marker=\".\")\n\ndf_final[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#importing-the-data",
    "href": "topics/na_imputation/na_imputation.html#importing-the-data",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "The dataset is loaded and indexed by date. The presence of missing values is identified and summarized to understand the scope of the issue.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_missing.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\nna_idx = raw_df.isnull()\n\n\nprint(f\"There are {raw_df['sales'].isnull().sum()} missing values, these are {np.round(raw_df['sales'].isnull().sum() / len(raw_df)* 100,3)} percent of the data\")\n\nThere are 33 missing values, these are 20.625 percent of the data"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#visualizing-missing-data",
    "href": "topics/na_imputation/na_imputation.html#visualizing-missing-data",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "A plot of the data highlights where the missing values occur, providing a visual understanding of their distribution over time.\n\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#forward-fill",
    "href": "topics/na_imputation/na_imputation.html#forward-fill",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "Forward filling imputes missing values using the last available value. This method is straightforward but may not reflect actual trends if the missing period spans significant variations in the data.\n\nffill_df = raw_df.ffill()\n\nplt.clf()\n\nax = ffill_df.plot(linestyle=\"-\", marker=\".\")\n\nffill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#backward-fill",
    "href": "topics/na_imputation/na_imputation.html#backward-fill",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "Backward filling replaces missing values with the next available value. This approach can lead to data leakage, as it introduces future information into past time points. Care should be taken when using this method, especially in predictive modeling contexts.\n\nbfill_df = raw_df.bfill()\n\nplt.clf()\n\nax = bfill_df.plot(linestyle=\"-\", marker=\".\")\n\nbfill_df[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "href": "topics/na_imputation/na_imputation.html#linear-interpolation",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "Linear interpolation fills missing values by assuming a linear relationship between neighboring data points. This method is suitable for time series where trends between points are approximately linear.\n\nlin_inter = raw_df.interpolate(method = \"time\")\n\nplt.clf()\n\nax = lin_inter.plot(linestyle=\"-\", marker=\".\")\n\nlin_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "href": "topics/na_imputation/na_imputation.html#spline-interpolation",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "Spline interpolation uses cubic splines to create a smooth curve through the data points, providing a more sophisticated imputation than linear interpolation. It works well when the data has non-linear patterns, but it may overfit in some cases.\n\nspline_inter = raw_df.interpolate(method = \"spline\", order = 3)\n\nplt.clf()\n\nax = spline_inter.plot(linestyle=\"-\", marker=\".\")\n\nspline_inter[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "href": "topics/na_imputation/na_imputation.html#seasonal-decomposition-and-interpolation",
    "title": "Missing Data Imputation",
    "section": "",
    "text": "STL (Seasonal and Trend decomposition using Loess) is used to separate the seasonal, trend, and residual components of the time series. Linear interpolation is applied first because STL cannot handle missing data directly. After seasonal decomposition, imputation is performed on the deseasonalized series, and the seasonal component is added back to reconstruct the series.\n\n\nstl_inter = STL(raw_df.interpolate(method = \"time\"), seasonal = 31).fit()\n\nseasonal_component = stl_inter.seasonal\n\ndeaseasonlised_df = raw_df[\"sales\"] - seasonal_component\n\ndf_inter = deaseasonlised_df.interpolate(method = \"time\")\n\ndf_final = df_inter + seasonal_component\n\ndf_final = df_final.to_frame().rename(columns = {0:\"sales\"})\n\n\nplt.clf()\n\nax = df_final.plot(linestyle=\"-\", marker=\".\")\n\ndf_final[na_idx].plot(ax=ax, legend=None, marker=\".\", color=\"r\")\n\nplt.show()"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html",
    "href": "topics/intro/forecaster_demo.html",
    "title": "Forecasting demonstration",
    "section": "",
    "text": "import pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom feature_engine.timeseries.forecasting import LagFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import root_mean_squared_error\nIn this section, we provide a brief demonstration of a forecasting problem using a simple Linear Regression model. The dataset used for this example consists of historical airline passenger numbers. Our goal is to demonstrate the forecasting process, including key steps such as feature engineering, model fitting, and forecast evaluation."
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#data-loading",
    "href": "topics/intro/forecaster_demo.html#data-loading",
    "title": "Forecasting demonstration",
    "section": "Data loading",
    "text": "Data loading\nThe dataset contains monthly data on the number of airline passengers from January 1949 to December 1960. This time series dataset is commonly used in forecasting examples due to its clear seasonal patterns and upward trend over time. The time index of the data represents the first day of each month, and the target variable is the number of passengers. The following code loads the dataset, ensures the date column is correctly parsed as a datetime object, and sets it as the index of the DataFrame.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#performance-evaluation",
    "href": "topics/intro/forecaster_demo.html#performance-evaluation",
    "title": "Forecasting demonstration",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nprint(f\"Train set RMSE is {np.round(root_mean_squared_error(y_train, y_train_pred),2)}\")\n\nTrain set RMSE is 23.22\n\nprint(f\"Test set RMSE is {np.round(root_mean_squared_error(y_test, y_test_pred),2)}\")\n\nTest set RMSE is 49.64"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#summary",
    "href": "topics/intro/forecaster_demo.html#summary",
    "title": "Forecasting demonstration",
    "section": "Summary",
    "text": "Summary\nIn this demonstration, we explored the basic steps of a time series forecasting task using airline passenger data. We processed the data by creating lagged features, trained a simple Linear Regression model, and evaluated the model’s performance using RMSE. This step-by-step approach shows how to handle feature engineering, model training, and performance evaluation in a forecasting scenario, providing a foundation for more advanced time series techniques."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Forecast Demo\n\n\n\n\n \n\nDirect forecast\n\n\n\n\n \n\nRecursive forecast\n\n\n\n\n \n\nTransformations\n\n\n\n\n \n\nTime SeriesDecomposition\n\n\n\n\n \n\nMissing valuesimputation\n\n\n\n\n \n\nOutliers identification"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "topics/intro/direct_forecasting.html",
    "href": "topics/intro/direct_forecasting.html",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Direct Approach",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.selection import DropFeatures \nfrom feature_engine.timeseries.forecasting import LagFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport os\nIn this tutorial, we will demonstrate a method for forecasting multiple periods ahead using a pipeline-based approach in Python. Specifically, we will use a direct approach for multi-step forecasting, where each future period within the forecast horizon is predicted independently. The direct approach allows us to generate a forecast for each individual time point in a given interval, which is helpful for applications that need predictions for multiple consecutive periods. For our demonstration, we’ll use a forecast interval of three periods, predicting the next, the following, and the third subsequent time point. This approach involves handling the challenges of preparing lagged data and synchronizing multiple targets across different forecast horizons.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/direct_forecasting.html#performance-evaluation",
    "href": "topics/intro/direct_forecasting.html#performance-evaluation",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Direct Approach",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nfor temp_h in range(forecast_horizon):\n  \n  y_pred_train = Y_train_pred[[f\"pred_{temp_h}\"]]\n  \n  y_actual_train = Y_train_processed[f\"h_{temp_h}\"]\n  \n  train_rmse = root_mean_squared_error(y_pred_train, y_actual_train)\n  \n  y_pred_test = Y_test_pred[[f\"pred_{temp_h}\"]]\n  \n  y_actual_test = Y_test_processed[f\"h_{temp_h}\"]\n  \n  test_rmse = root_mean_squared_error(y_pred_test, y_actual_test)\n  \n  print(f\"Train RMSE for horizon {temp_h} is {np.round(train_rmse,2)}, Test RMSE is {np.round(test_rmse,2)}\")\n\nTrain RMSE for horizon 0 is 22.73, Test RMSE is 49.69\nTrain RMSE for horizon 1 is 35.5, Test RMSE is 81.48\nTrain RMSE for horizon 2 is 42.48, Test RMSE is 99.33"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html",
    "href": "topics/intro/recursive_forecasting.html",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "",
    "text": "In this tutorial, we demonstrate how to forecast multiple periods (or steps) ahead using a recursive approach, which is commonly applied in time series forecasting. The recursive approach involves using the model’s previous predictions as inputs for generating subsequent predictions. This iterative process is essential for making forecasts that extend over multiple time steps, especially in cases where future values depend on previous predictions. We’ll use this approach to build a forecasting model and evaluate its performance, covering essential preprocessing steps, model fitting, and accuracy measurement.\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.base import clone\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom feature_engine.timeseries.forecasting import LagFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom sklearn.metrics import root_mean_squared_error\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html#first-point",
    "href": "topics/intro/recursive_forecasting.html#first-point",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "First Point",
    "text": "First Point\nTo forecast the first point in the test set, we’ll use the last known value from the training set. This point will serve as the basis for predicting the next time step, and the prediction for this time step will then be used as an input to forecast subsequent steps.\n\n\nfeature_source_data = train_set.copy()\n\nfirst_forecast_date = train_set.index.max() + pd.DateOffset(months = 1)\n\nfeature_source_data.loc[first_forecast_date] = 0\n\nfeature_vec = feature_engine_pipe.transform(feature_source_data.copy())\n\nfeature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n\nfirst_pred = lin_reg.predict(feature_vec)\n\nmanual_preds = pd.DataFrame(index = [first_forecast_date],\n                            data = first_pred)"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html#second-point",
    "href": "topics/intro/recursive_forecasting.html#second-point",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "Second Point",
    "text": "Second Point\nFor the second prediction point, we take the forecasted value from the first point and incorporate it as the next input, following the recursive approach.\n\nfeature_source_data.loc[first_forecast_date] = first_pred\n\n&lt;string&gt;:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[344.3149789]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\n\nsecond_forecast_date = first_forecast_date + pd.DateOffset(months = 1)\n\nfeature_source_data.loc[second_forecast_date] = 0\n\nfeature_vec = feature_engine_pipe.transform(feature_source_data.copy())\n\nfeature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n\nsecond_pred = lin_reg.predict(feature_vec)\n\nmanual_preds = pd.concat([manual_preds,pd.DataFrame(index = [second_forecast_date],\n                            data = second_pred)], axis = 0).copy()\n\nmanual_preds.columns = [\"passengers\"]"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html#performance-evaluation",
    "href": "topics/intro/recursive_forecasting.html#performance-evaluation",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nfor temp_h in range(forecast_horizon):\n  \n  y_pred_train = Y_train_pred[[f\"pred_{temp_h}\"]]\n\n  y_actual_train = Y_train_pred[f\"h_{temp_h}\"]\n\n  train_rmse = root_mean_squared_error(y_pred_train, y_actual_train)\n  \n  y_pred_test = Y_test_pred[[f\"pred_{temp_h}\"]]\n  \n  y_actual_test = Y_test_pred[f\"h_{temp_h}\"]\n  \n  test_rmse = root_mean_squared_error(y_pred_test, y_actual_test)\n  \n  print(f\"Train RMSE for horizon {temp_h} is {np.round(train_rmse,2)},Test RMSE is {np.round(test_rmse,2)}\")\n\nTrain RMSE for horizon 0 is 22.75,Test RMSE is 49.35\nTrain RMSE for horizon 1 is 35.58,Test RMSE is 80.89\nTrain RMSE for horizon 2 is 42.59,Test RMSE is 98.79\n\n\nIn this tutorial, we successfully implemented a recursive forecasting approach, generating predictions that consider previously predicted values as inputs for future time steps. This method is particularly useful for extending forecasts over multiple periods, as each forecast builds upon the last. The evaluation metrics, especially RMSE, help us assess the model’s accuracy, demonstrating its predictive strength and ability to generalize to new data."
  },
  {
    "objectID": "topics/outliers/outliers.html",
    "href": "topics/outliers/outliers.html",
    "title": "Outliers",
    "section": "",
    "text": "Handling missing data and outliers is a critical step in data preprocessing. Missing or anomalous values can distort statistical analysis and predictive modeling, leading to unreliable results. In this tutorial, we focus on identifying and imputing outliers in a time series dataset, which involves deseasonalizing the data, detecting anomalies using statistical methods, and imputing these anomalies with appropriate techniques. This guide demonstrates how to achieve these steps using Python, providing both theoretical explanations and practical code implementation.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_outliers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#introduction",
    "href": "topics/outliers/outliers.html#introduction",
    "title": "Outliers",
    "section": "",
    "text": "Handling missing data and outliers is a critical step in data preprocessing. Missing or anomalous values can distort statistical analysis and predictive modeling, leading to unreliable results. In this tutorial, we focus on identifying and imputing outliers in a time series dataset, which involves deseasonalizing the data, detecting anomalies using statistical methods, and imputing these anomalies with appropriate techniques. This guide demonstrates how to achieve these steps using Python, providing both theoretical explanations and practical code implementation.\n\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.tsa.seasonal import STL\n\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales_outliers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nplt.clf()\n\nraw_df[\"sales\"].plot(marker = \".\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#deseasonalize-data",
    "href": "topics/outliers/outliers.html#deseasonalize-data",
    "title": "Outliers",
    "section": "Deseasonalize Data",
    "text": "Deseasonalize Data\nSeasonal patterns in time series data can obscure anomalies. To effectively identify outliers, we first deseasonalize the data. This process involves decomposing the time series into seasonal, trend, and residual components and removing the seasonal component. This ensures that the seasonality does not interfere with our analysis of deviations from expected patterns.\n\n\nstl_decomp = STL(raw_df[\"sales\"], robust = True).fit()\n\nseasonal_component = stl_decomp.seasonal\n\nraw_df[\"sales_deseasonalised\"] = raw_df[\"sales\"] - seasonal_component\n\n\nplt.clf()\n\nraw_df[\"sales_deseasonalised\"].plot(marker = \".\")\n\nplt.title(\"Deseasonlalized data with outliers\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#identify-outliers",
    "href": "topics/outliers/outliers.html#identify-outliers",
    "title": "Outliers",
    "section": "Identify Outliers",
    "text": "Identify Outliers\nOutlier detection involves identifying data points that deviate significantly from the expected range. Here, the expected value is estimated using rolling statistics, and the margin is defined by the variability around this expectation. Two approaches are used: rolling mean and standard deviation, and rolling median with median absolute deviation (MAD).\n\nRolling Mean and Standard Deviation\nRolling mean and standard deviation provide a straightforward way to calculate the expected value and variability. However, they are sensitive to outliers, as extreme values can skew these measures. To identify outliers, we calculate the rolling mean and standard deviation over a sliding window and define thresholds for outlier detection.\n\n\nraw_df[[\"roll_mean\",\"roll_std\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_mean\":\"mean\", \"roll_std\":\"std\"})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper\"] = raw_df[\"roll_mean\"] + margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"lower\"] = raw_df[\"roll_mean\"] - margin_factor * raw_df[\"roll_std\"]\n\nraw_df[\"is_outlier\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRolling Median and Median Absolute Deviation\nMedian and MAD are robust to outliers, making them suitable for datasets with extreme values. The rolling median serves as the expected value, and MAD provides a robust measure of variability. This method ensures that the detection of outliers is not overly influenced by extreme data points.\n\n\ndef mad(x):\n  return np.median(np.abs(x - np.median(x)))\n\nraw_df[[\"roll_median\",\"roll_mad\"]] = (\n                \n                raw_df[\"sales_deseasonalised\"]\n                .rolling(window = 13,center = True, min_periods = 1)\n                .agg({\"roll_median\":\"median\", \"roll_mad\":mad})\n  \n)\n\n\n\nmargin_factor = 3\n\nraw_df[\"upper_2\"] = raw_df[\"roll_median\"] + margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"lower_2\"] = raw_df[\"roll_median\"] - margin_factor * raw_df[\"roll_mad\"]\n\nraw_df[\"is_outlier_2\"] = np.abs((raw_df[\"sales_deseasonalised\"] &lt;= raw_df[\"lower_2\"]) |\n                              (raw_df[\"sales_deseasonalised\"] &gt;= raw_df[\"upper_2\"]))\n\n\nplt.clf()\n\nax = raw_df[\"sales_deseasonalised\"].plot()\n\nraw_df[\"upper_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df[\"lower_2\"].plot(ax = ax, color = \"black\", linestyle = \"dashed\")\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_deseasonalised\"].plot(ax = ax,\n                                                             color = \"red\",\n                                                             marker = \"o\",\n                                                             linestyle = \"none\")\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#impute-outliers",
    "href": "topics/outliers/outliers.html#impute-outliers",
    "title": "Outliers",
    "section": "Impute Outliers",
    "text": "Impute Outliers\nTo handle outliers, we replace them with imputed values. For demonstration purposes, linear interpolation is used. This technique fills missing or anomalous values by estimating intermediate points using a linear function based on surrounding data.\n\n\nraw_df[\"sales_na\"] = raw_df[\"sales_deseasonalised\"]\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_na\"] = np.nan\n\nraw_df[\"sales_imputed\"] = raw_df[\"sales_na\"].interpolate(method = \"time\").copy()\n\n\nplt.clf()\n\nax = raw_df[\"sales_imputed\"].plot()\n\nraw_df.loc[raw_df[\"is_outlier_2\"],\"sales_imputed\"].plot(ax = ax,\n                                                      alpha = 0.5,\n                                                      color = \"red\",\n                                                      marker = \"o\",\n                                                      linestyle = \"none\")\n\nplt.title(\"Imputed outliers\")\n\n\nplt.show()"
  },
  {
    "objectID": "topics/outliers/outliers.html#summary",
    "href": "topics/outliers/outliers.html#summary",
    "title": "Outliers",
    "section": "Summary",
    "text": "Summary\nThis tutorial demonstrated a systematic approach to handling outliers in a time series dataset. Starting with deseasonalizing the data, we explored two methods for outlier detection: rolling mean and standard deviation, and rolling median with MAD. Finally, we showed how to impute outliers using linear interpolation. This workflow ensures a cleaner dataset for further analysis or modeling, enhancing the accuracy and reliability of the results."
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html",
    "href": "topics/ts_decomposition/lowess_decomp.html",
    "title": "LOWESS to Extract the Trend",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import KFold\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#introduction",
    "href": "topics/ts_decomposition/lowess_decomp.html#introduction",
    "title": "LOWESS to Extract the Trend",
    "section": "Introduction",
    "text": "Introduction\nIn time series analysis, data often contains inherent noise that can obscure trends and patterns, making it challenging to analyze or forecast accurately. Smoothing techniques are valuable for filtering out this noise, and one popular method is Locally Weighted Scatterplot Smoothing (LOWESS). LOWESS is a non-parametric regression method that applies localized, weighted linear regressions to generate a smoothed trend line over the data. This tutorial provides an in-depth guide to using LOWESS for time series decomposition, including an explanation of its parameters and a demonstration of cross-validation to select the optimal smoothing factor. By the end, you’ll understand how to tune LOWESS to achieve an effective decomposition of your data and isolate its trend component.\nWe will use the retail sales dataset.\n\nraw_df[\"sales\"].plot()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#lowess-parameters",
    "href": "topics/ts_decomposition/lowess_decomp.html#lowess-parameters",
    "title": "LOWESS to Extract the Trend",
    "section": "LOWESS Parameters",
    "text": "LOWESS Parameters\nLOWESS, or Locally Weighted Scatterplot Smoothing, is a method used to smooth data points in a time series or other types of datasets with a lot of noise. It does so by fitting multiple linear regressions over subsets of data, or “windows.” In the Python implementation of LOWESS from the statsmodels library, several key parameters influence the outcome of this smoothing. These include frac, it, endog, and exog.\n\nfrac: This parameter controls the fraction of data points used to compute each local regression. It effectively determines the window size in the smoothing process. A smaller frac value means that fewer points are used in each regression, leading to a curve that follows the data more closely (less smoothing). A larger frac value, in contrast, results in greater smoothing as more points are included in each local regression.\nit: The it parameter specifies the number of iterations in the robust regression process. Robust regression helps handle outliers in the data by down-weighting points that deviate significantly from the trend. Increasing it results in more iterations to identify and adjust for outliers, potentially improving the robustness of the trend line.\nendog and exog: These parameters define the endogenous (dependent) and exogenous (independent) variables for the LOWESS fitting process. In this case, endog (or y) represents the variable we wish to smooth (e.g., “sales” in a time series), while exog (or x) is the independent variable, typically the index of the dataset or time variable. Specifying endog and exog allows LOWESS to map out the trend of endog against exog.\n\nThe following code snippet applies LOWESS to our dataset, using the specified parameters to calculate a smoothed trend line for a time series of sales data.\nA too-high frac value can result in over-smoothing, where essential details in the data are averaged out and the trend appears excessively flat. Conversely, a too-low frac value may under-smooth the data, capturing noise and failing to reveal the true trend. Here, we intentionally set a higher frac value to demonstrate some degree of over-smoothing; in the next section, we will determine an optimal frac value using cross-validation.\n\n\ny = raw_df[\"sales\"]\n\nx = np.arange(0, len(y))\n\nts_decomp = lowess(endog=y, exog=x, frac=0.5, it=3)\n\nraw_df[\"trend_lowess\"] = ts_decomp[:,1]\n\n\nplt.clf()\n\nraw_df[\"sales\"].plot(color=\"steelblue\")\n\nraw_df[\"trend_lowess\"].plot(color=\"orange\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCross-Validation to Select the Appropriate Fraction Parameter\nTo ensure that the LOWESS smoothing is optimally tuned for our data, we can use cross-validation to select the best frac parameter. By testing multiple frac values, we aim to find a balance between smoothness and accuracy, ultimately minimizing the root mean squared error (RMSE) between observed and predicted values in a test set. The KFold cross-validation approach is used here to split the dataset into training and test sets multiple times, thereby enhancing the reliability of our RMSE estimates.\nIn the function get_rmse_for_df, we implement this cross-validation procedure for each candidate frac value. The function uses the KFold technique, splitting the data into five folds and iterating over them to calculate the RMSE of predictions for each fold. By doing this, we measure how well each frac parameter fits the data, and select the one with the lowest average RMSE, which indicates the optimal balance of fit and smoothness.\nIn the code snippet below, we calculate the RMSE for various values of frac and collect the results. This allows us to compare different frac settings and choose the one that minimizes error.\n\n\ndef get_rmse_for_df(X, y, frac_param):\n  KFold_obj = KFold(n_splits=5, shuffle=True, random_state=0)\n  rmse_list = []\n  for train_index, test_index in KFold_obj.split(X, y):\n    X_train = X[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X[test_index]\n    y_test = y.iloc[test_index]\n    y_pred = lowess(endog=y_train, exog=X_train, frac=frac_param, xvals=X_test)\n    rmse = np.sqrt(mean_squared_error(y_pred, y_test))\n    rmse_list.append(rmse)\n  return rmse_list\n\n\nresults = []\n\nfor temp_frac in np.arange(0.05, 1.02, 0.05):\n  rmse_list = get_rmse_for_df(X=x, y=y, frac_param=temp_frac)\n  rmse_df = pd.DataFrame(data=rmse_list, columns=[\"rmse\"])\n  rmse_df[\"frac\"] = temp_frac\n  results.append(rmse_df)\n\n  \nresults = pd.concat(results, axis=0)\n  \n\n\ncv_plot_df = results.groupby(\"frac\")[\"rmse\"].agg([\"mean\", \"std\"]).reset_index()\n\nplt.figure(figsize=(8, 5))\n\nplt.errorbar(cv_plot_df['frac'],\n             cv_plot_df['mean'],\n             yerr=cv_plot_df['std'],\n             fmt='o',\n             capsize=5,\n             capthick=1,\n             elinewidth=1)\n\nplt.title('Mean RMSE with Standard Deviation Error Bars')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe RMSE values have a considerable spread, as indicated by the length of the error bars. A large spread (standard deviation) suggests that there is significant variability in RMSE across the cross-validation folds. Despite this variability, we will proceed with the minimal frac value identified, as it represents an improvement over the higher value used in the previous section. This will allow us to demonstrate how the optimal value enhances the balance between smoothness and trend accuracy.\n\nbest_frac = cv_plot_df.sort_values([\"mean\"]).iloc[0,0]\n\nprint(f\"The best frac value is {np.round(best_frac,2)}\")\n\nThe best frac value is 0.15\n\n\n\nts_decomp = lowess(endog=y, exog=x, frac=best_frac, it=3)\n\nraw_df[\"trend_lowess_best\"] = ts_decomp[:,1]\n\n\nplt.clf()\n\nraw_df[\"sales\"].plot(color=\"steelblue\")\n\nraw_df[\"trend_lowess_best\"].plot(color=\"orange\")\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#summary",
    "href": "topics/ts_decomposition/lowess_decomp.html#summary",
    "title": "LOWESS to Extract the Trend",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we explored how to use LOWESS for time series decomposition and trend extraction. By understanding and adjusting the parameters of frac, it, endog, and exog, we can fine-tune the smoothing process to better reveal underlying trends in our data. We also implemented a cross-validation technique to help identify the optimal frac parameter, balancing accuracy and smoothness. This approach is especially useful in time series analysis, where accurate trend estimation plays a key role in forecasting and insights. Through LOWESS and parameter tuning, we can effectively isolate and analyze the trends within complex datasets, enhancing our understanding of the data’s behavior over time."
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html",
    "href": "topics/ts_decomposition/transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os \n\nfrom scipy.stats import boxcox"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#introduction",
    "href": "topics/ts_decomposition/transformations.html#introduction",
    "title": "Transformations",
    "section": "Introduction",
    "text": "Introduction\nData transformation is an essential preprocessing step in data analysis and machine learning, particularly when dealing with time series or data that exhibits non-linear patterns. Transformations, such as log, Box-Cox, and moving averages, are used to adjust data to make it more amenable to analysis by stabilizing variance, enhancing linearity, or smoothing out noise. These adjustments make it easier to uncover trends, detect seasonality, and improve model performance. This tutorial will explore three key transformations—log transformation, Box-Cox transformation (including the Guerrero method), and moving averages—and demonstrate how each can be applied to real-world time series data to improve its suitability for analysis and forecasting.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\nraw_df = pd.read_csv(file_path,index_col = \"date\")\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nraw_df[\"passengers\"].plot()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#log-transformation",
    "href": "topics/ts_decomposition/transformations.html#log-transformation",
    "title": "Transformations",
    "section": "Log Transformation",
    "text": "Log Transformation\nTo make data more suitable for certain analyses, we may need to transform it. One common reason is to stabilize the variance, especially when dealing with data that shows exponential growth or high variability. Log transformation is an effective method for this. By taking the logarithm of each value in the data, we can compress large values and expand smaller ones, resulting in a dataset where variability across different ranges is reduced. This transformation can make trends more visible and prepare the data for further processing or modeling steps.\n\nraw_df[\"passengers_log\"] = np.log(raw_df[\"passengers\"])\nplt.clf()\nraw_df[\"passengers_log\"].plot()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "href": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "title": "Transformations",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nThe Box-Cox transformation is a versatile method used to stabilize variance across a dataset. Unlike the log transformation, Box-Cox can adapt to various data distributions by tuning the transformation parameter, \\(\\lambda\\). Different values of \\(\\lambda\\) change the shape of the transformation, allowing for either compression or expansion of data values in a customized way. By testing various \\(\\lambda\\) values, we can achieve the best variance stabilization and approximate a normal distribution, which is helpful for many statistical analyses.\nIn this example, we test a series of \\(\\lambda\\) values to demonstrate how the shape of the transformed data changes, which can help identify an optimal transformation for stabilizing variance in our dataset.\n\nlambdas_vec = [-1,-0.5,0,0.5,1,2]\nplt.clf()\nfig, ax = plt.subplots(ncols=2, nrows=3, figsize=[15, 15], sharex=True)\nax = ax.flatten()\n\nfor ix, temp_lambda in enumerate(lambdas_vec):\n    raw_df[\"temp_box_cox\"] = boxcox(raw_df[\"passengers\"], temp_lambda)\n    raw_df.plot(y=\"temp_box_cox\", ax=ax[ix], label=f\"lambda = {temp_lambda}\")\n    ax[ix].legend(fontsize=18)\n    ax[ix].set_xlabel(\"\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Guerrero Method\nThe Guerrero method for the Box-Cox transformation offers a targeted approach to variance stabilization by automatically selecting an optimal \\(\\lambda\\) based on the data’s seasonal patterns. This method evaluates the seasonal periodicity and adjusts \\(\\lambda\\) to achieve minimal variance over time. The advantage of using Guerrero’s method is that it provides a systematic way to stabilize variance in seasonal data without manually testing multiple \\(\\lambda\\) values. This automatic tuning is particularly useful when working with time series data with complex seasonal patterns.\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nbc_guerrero = BoxCoxTransformer(method=\"guerrero\", sp=12)\nraw_df[\"passengers_bc_guerrero\"] = bc_guerrero.fit_transform(raw_df[\"passengers\"])\n\nplt.clf()\nraw_df[\"passengers_bc_guerrero\"].plot()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#moving-averages",
    "href": "topics/ts_decomposition/transformations.html#moving-averages",
    "title": "Transformations",
    "section": "Moving Averages",
    "text": "Moving Averages\nMoving averages are a common technique for smoothing time series data, allowing for a clearer view of the underlying trend by reducing noise. With an odd-sized window, the moving average is symmetric, meaning it has an equal number of data points on either side of the center, which simplifies calculation and interpretation.\nWhen using an even-sized window, achieving symmetry is more challenging because there isn’t a natural center point. To create a balanced, symmetric effect, we can adjust the weights, assigning smaller weights to the edges while retaining higher weights for the central values. Another approach to achieve symmetry with an even window is to apply a secondary moving average of window size 2. This additional step smooths the edge values, effectively balancing the window weights.\n\n\nma_file_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\nma_raw_df = pd.read_csv(ma_file_path, index_col=\"date\")\nma_raw_df.index = pd.to_datetime(ma_raw_df.index)\ndel ma_file_path\n\n\n3-Point Moving Average\nHere, we apply a 3-point moving average, which is symmetric by nature. This approach smooths out the data while retaining the central tendency, making it easier to observe general trends without the distraction of short-term fluctuations.\n\n\nma_df = ma_raw_df.copy()\nma_df[\"ma_3\"] = ma_df[\"sales\"].rolling(window=3, center=True).mean()\n\n\nplt.clf()\nma_df[\"ma_3\"].plot(color=\"steelblue\")\nma_df[\"sales\"].plot(color=\"grey\", alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n12-Point Moving Average with Centering\nTo create a symmetric 12-point moving average, we apply an additional 2-point moving average to our initial 12-point average. This second step reduces the impact of edge weights, creating a balanced smoothing effect. After this adjustment, we shift the result by half the window size to ensure alignment with the original data, resulting in a centered moving average.\n\n\neven_window_size = 12\nma_df[\"ma_2_12\"] = ma_df[\"sales\"].rolling(window=even_window_size).mean()\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].rolling(window=2, center=True).mean()\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].shift(-even_window_size // 2)\n\n\nplt.clf()\nma_df[\"ma_2_12\"].plot(color=\"steelblue\")\nma_df[\"sales\"].plot(color=\"grey\", alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#summary",
    "href": "topics/ts_decomposition/transformations.html#summary",
    "title": "Transformations",
    "section": "Summary",
    "text": "Summary\nThis tutorial has demonstrated the use of log transformations, Box-Cox transformations, and moving averages to prepare time series data for analysis. Each transformation serves a specific purpose: log transformations reduce skewness and stabilize variance, Box-Cox transformations provide flexibility through parameter tuning, and moving averages smooth data for trend analysis. By understanding and applying these techniques, we can better handle data variability, enhance interpretability, and ultimately improve forecasting accuracy. The choice of transformation should align with the data characteristics and analytical goals, making transformations a valuable tool in the data scientist’s arsenal for managing diverse time series data."
  }
]