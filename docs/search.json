[
  {
    "objectID": "topics/ts_decomposition/ts_decomposition_page.html",
    "href": "topics/ts_decomposition/ts_decomposition_page.html",
    "title": "Time Series decomposition methods",
    "section": "",
    "text": "Classical Decomposition\n\n\n\n\n\n\nLowess Decomposition\n\n\n\n\n\n\nSTL Decomposition"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html",
    "href": "topics/ts_decomposition/lowess_decomp.html",
    "title": "LOWESS to Extract the Trend",
    "section": "",
    "text": "import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import KFold\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#introduction",
    "href": "topics/ts_decomposition/lowess_decomp.html#introduction",
    "title": "LOWESS to Extract the Trend",
    "section": "Introduction",
    "text": "Introduction\nIn time series analysis, data often contains inherent noise that can obscure trends and patterns, making it challenging to analyze or forecast accurately. Smoothing techniques are valuable for filtering out this noise, and one popular method is Locally Weighted Scatterplot Smoothing (LOWESS). LOWESS is a non-parametric regression method that applies localized, weighted linear regressions to generate a smoothed trend line over the data. This tutorial provides an in-depth guide to using LOWESS for time series decomposition, including an explanation of its parameters and a demonstration of cross-validation to select the optimal smoothing factor. By the end, you’ll understand how to tune LOWESS to achieve an effective decomposition of your data and isolate its trend component.\nWe will use the retail sales dataset.\n\nraw_df[\"sales\"].plot()\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#lowess-parameters",
    "href": "topics/ts_decomposition/lowess_decomp.html#lowess-parameters",
    "title": "LOWESS to Extract the Trend",
    "section": "LOWESS Parameters",
    "text": "LOWESS Parameters\nLOWESS, or Locally Weighted Scatterplot Smoothing, is a method used to smooth data points in a time series or other types of datasets with a lot of noise. It does so by fitting multiple linear regressions over subsets of data, or “windows.” In the Python implementation of LOWESS from the statsmodels library, several key parameters influence the outcome of this smoothing. These include frac, it, endog, and exog.\n\nfrac: This parameter controls the fraction of data points used to compute each local regression. It effectively determines the window size in the smoothing process. A smaller frac value means that fewer points are used in each regression, leading to a curve that follows the data more closely (less smoothing). A larger frac value, in contrast, results in greater smoothing as more points are included in each local regression.\nit: The it parameter specifies the number of iterations in the robust regression process. Robust regression helps handle outliers in the data by down-weighting points that deviate significantly from the trend. Increasing it results in more iterations to identify and adjust for outliers, potentially improving the robustness of the trend line.\nendog and exog: These parameters define the endogenous (dependent) and exogenous (independent) variables for the LOWESS fitting process. In this case, endog (or y) represents the variable we wish to smooth (e.g., “sales” in a time series), while exog (or x) is the independent variable, typically the index of the dataset or time variable. Specifying endog and exog allows LOWESS to map out the trend of endog against exog.\n\nThe following code snippet applies LOWESS to our dataset, using the specified parameters to calculate a smoothed trend line for a time series of sales data.\nA too-high frac value can result in over-smoothing, where essential details in the data are averaged out and the trend appears excessively flat. Conversely, a too-low frac value may under-smooth the data, capturing noise and failing to reveal the true trend. Here, we intentionally set a higher frac value to demonstrate some degree of over-smoothing; in the next section, we will determine an optimal frac value using cross-validation.\n\n\ny = raw_df[\"sales\"]\n\nx = np.arange(0, len(y))\n\nts_decomp = lowess(endog=y, exog=x, frac=0.5, it=3)\n\nraw_df[\"trend_lowess\"] = ts_decomp[:,1]\n\n\nplt.clf()\n\nraw_df[\"sales\"].plot(color=\"steelblue\")\n\nraw_df[\"trend_lowess\"].plot(color=\"orange\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCross-Validation to Select the Appropriate Fraction Parameter\nTo ensure that the LOWESS smoothing is optimally tuned for our data, we can use cross-validation to select the best frac parameter. By testing multiple frac values, we aim to find a balance between smoothness and accuracy, ultimately minimizing the root mean squared error (RMSE) between observed and predicted values in a test set. The KFold cross-validation approach is used here to split the dataset into training and test sets multiple times, thereby enhancing the reliability of our RMSE estimates.\nIn the function get_rmse_for_df, we implement this cross-validation procedure for each candidate frac value. The function uses the KFold technique, splitting the data into five folds and iterating over them to calculate the RMSE of predictions for each fold. By doing this, we measure how well each frac parameter fits the data, and select the one with the lowest average RMSE, which indicates the optimal balance of fit and smoothness.\nIn the code snippet below, we calculate the RMSE for various values of frac and collect the results. This allows us to compare different frac settings and choose the one that minimizes error.\n\n\ndef get_rmse_for_df(X, y, frac_param):\n  KFold_obj = KFold(n_splits=5, shuffle=True, random_state=0)\n  rmse_list = []\n  for train_index, test_index in KFold_obj.split(X, y):\n    X_train = X[train_index]\n    y_train = y.iloc[train_index]\n    X_test = X[test_index]\n    y_test = y.iloc[test_index]\n    y_pred = lowess(endog=y_train, exog=X_train, frac=frac_param, xvals=X_test)\n    rmse = np.sqrt(mean_squared_error(y_pred, y_test))\n    rmse_list.append(rmse)\n  return rmse_list\n\n\nresults = []\n\nfor temp_frac in np.arange(0.05, 1.02, 0.05):\n  rmse_list = get_rmse_for_df(X=x, y=y, frac_param=temp_frac)\n  rmse_df = pd.DataFrame(data=rmse_list, columns=[\"rmse\"])\n  rmse_df[\"frac\"] = temp_frac\n  results.append(rmse_df)\n\n  \nresults = pd.concat(results, axis=0)\n  \n\n\ncv_plot_df = results.groupby(\"frac\")[\"rmse\"].agg([\"mean\", \"std\"]).reset_index()\n\nplt.figure(figsize=(8, 5))\n\nplt.errorbar(cv_plot_df['frac'],\n             cv_plot_df['mean'],\n             yerr=cv_plot_df['std'],\n             fmt='o',\n             capsize=5,\n             capthick=1,\n             elinewidth=1)\n\nplt.title('Mean RMSE with Standard Deviation Error Bars')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe RMSE values have a considerable spread, as indicated by the length of the error bars. A large spread (standard deviation) suggests that there is significant variability in RMSE across the cross-validation folds. Despite this variability, we will proceed with the minimal frac value identified, as it represents an improvement over the higher value used in the previous section. This will allow us to demonstrate how the optimal value enhances the balance between smoothness and trend accuracy.\n\nbest_frac = cv_plot_df.sort_values([\"mean\"]).iloc[0,0]\n\nprint(f\"The best frac value is {np.round(best_frac,2)}\")\n\nThe best frac value is 0.15\n\n\n\nts_decomp = lowess(endog=y, exog=x, frac=best_frac, it=3)\n\nraw_df[\"trend_lowess_best\"] = ts_decomp[:,1]\n\n\nplt.clf()\n\nraw_df[\"sales\"].plot(color=\"steelblue\")\n\nraw_df[\"trend_lowess_best\"].plot(color=\"orange\")\n\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/lowess_decomp.html#summary",
    "href": "topics/ts_decomposition/lowess_decomp.html#summary",
    "title": "LOWESS to Extract the Trend",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we explored how to use LOWESS for time series decomposition and trend extraction. By understanding and adjusting the parameters of frac, it, endog, and exog, we can fine-tune the smoothing process to better reveal underlying trends in our data. We also implemented a cross-validation technique to help identify the optimal frac parameter, balancing accuracy and smoothness. This approach is especially useful in time series analysis, where accurate trend estimation plays a key role in forecasting and insights. Through LOWESS and parameter tuning, we can effectively isolate and analyze the trends within complex datasets, enhancing our understanding of the data’s behavior over time."
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html",
    "href": "topics/intro/recursive_forecasting.html",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "",
    "text": "In this tutorial, we demonstrate how to forecast multiple periods (or steps) ahead using a recursive approach, which is commonly applied in time series forecasting. The recursive approach involves using the model’s previous predictions as inputs for generating subsequent predictions. This iterative process is essential for making forecasts that extend over multiple time steps, especially in cases where future values depend on previous predictions. We’ll use this approach to build a forecasting model and evaluate its performance, covering essential preprocessing steps, model fitting, and accuracy measurement.\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport os \n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.base import clone\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom feature_engine.timeseries.forecasting import LagFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom sklearn.metrics import root_mean_squared_error\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html#first-point",
    "href": "topics/intro/recursive_forecasting.html#first-point",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "First Point",
    "text": "First Point\nTo forecast the first point in the test set, we’ll use the last known value from the training set. This point will serve as the basis for predicting the next time step, and the prediction for this time step will then be used as an input to forecast subsequent steps.\n\n\nfeature_source_data = train_set.copy()\n\nfirst_forecast_date = train_set.index.max() + pd.DateOffset(months = 1)\n\nfeature_source_data.loc[first_forecast_date] = 0\n\nfeature_vec = feature_engine_pipe.transform(feature_source_data.copy())\n\nfeature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n\nfirst_pred = lin_reg.predict(feature_vec)\n\nmanual_preds = pd.DataFrame(index = [first_forecast_date],\n                            data = first_pred)"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html#second-point",
    "href": "topics/intro/recursive_forecasting.html#second-point",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "Second Point",
    "text": "Second Point\nFor the second prediction point, we take the forecasted value from the first point and incorporate it as the next input, following the recursive approach.\n\nfeature_source_data.loc[first_forecast_date] = first_pred\n\n&lt;string&gt;:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[344.3149789]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\n\nsecond_forecast_date = first_forecast_date + pd.DateOffset(months = 1)\n\nfeature_source_data.loc[second_forecast_date] = 0\n\nfeature_vec = feature_engine_pipe.transform(feature_source_data.copy())\n\nfeature_vec = feature_vec.iloc[[len(feature_vec) - 1]].copy()\n\nsecond_pred = lin_reg.predict(feature_vec)\n\nmanual_preds = pd.concat([manual_preds,pd.DataFrame(index = [second_forecast_date],\n                            data = second_pred)], axis = 0).copy()\n\nmanual_preds.columns = [\"passengers\"]"
  },
  {
    "objectID": "topics/intro/recursive_forecasting.html#performance-evaluation",
    "href": "topics/intro/recursive_forecasting.html#performance-evaluation",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Recursive Approach",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nfor temp_h in range(forecast_horizon):\n  \n  y_pred_train = Y_train_pred[[f\"pred_{temp_h}\"]]\n\n  y_actual_train = Y_train_pred[f\"h_{temp_h}\"]\n\n  train_rmse = root_mean_squared_error(y_pred_train, y_actual_train)\n  \n  y_pred_test = Y_test_pred[[f\"pred_{temp_h}\"]]\n  \n  y_actual_test = Y_test_pred[f\"h_{temp_h}\"]\n  \n  test_rmse = root_mean_squared_error(y_pred_test, y_actual_test)\n  \n  print(f\"Train RMSE for horizon {temp_h} is {np.round(train_rmse,2)},Test RMSE is {np.round(test_rmse,2)}\")\n\nTrain RMSE for horizon 0 is 22.75,Test RMSE is 49.35\nTrain RMSE for horizon 1 is 35.58,Test RMSE is 80.89\nTrain RMSE for horizon 2 is 42.59,Test RMSE is 98.79\n\n\nIn this tutorial, we successfully implemented a recursive forecasting approach, generating predictions that consider previously predicted values as inputs for future time steps. This method is particularly useful for extending forecasts over multiple periods, as each forecast builds upon the last. The evaluation metrics, especially RMSE, help us assess the model’s accuracy, demonstrating its predictive strength and ability to generalize to new data."
  },
  {
    "objectID": "topics/intro/direct_forecasting.html",
    "href": "topics/intro/direct_forecasting.html",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Direct Approach",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom feature_engine.imputation import DropMissingData\nfrom feature_engine.selection import DropFeatures \nfrom feature_engine.timeseries.forecasting import LagFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport os\nIn this tutorial, we will demonstrate a method for forecasting multiple periods ahead using a pipeline-based approach in Python. Specifically, we will use a direct approach for multi-step forecasting, where each future period within the forecast horizon is predicted independently. The direct approach allows us to generate a forecast for each individual time point in a given interval, which is helpful for applications that need predictions for multiple consecutive periods. For our demonstration, we’ll use a forecast interval of three periods, predicting the next, the following, and the third subsequent time point. This approach involves handling the challenges of preparing lagged data and synchronizing multiple targets across different forecast horizons.\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/direct_forecasting.html#performance-evaluation",
    "href": "topics/intro/direct_forecasting.html#performance-evaluation",
    "title": "Forecasting Interval (Multiple Periods Ahead) - Direct Approach",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nfor temp_h in range(forecast_horizon):\n  \n  y_pred_train = Y_train_pred[[f\"pred_{temp_h}\"]]\n  \n  y_actual_train = Y_train_processed[f\"h_{temp_h}\"]\n  \n  train_rmse = root_mean_squared_error(y_pred_train, y_actual_train)\n  \n  y_pred_test = Y_test_pred[[f\"pred_{temp_h}\"]]\n  \n  y_actual_test = Y_test_processed[f\"h_{temp_h}\"]\n  \n  test_rmse = root_mean_squared_error(y_pred_test, y_actual_test)\n  \n  print(f\"Train RMSE for horizon {temp_h} is {np.round(train_rmse,2)}, Test RMSE is {np.round(test_rmse,2)}\")\n\nTrain RMSE for horizon 0 is 22.73, Test RMSE is 49.69\nTrain RMSE for horizon 1 is 35.5, Test RMSE is 81.48\nTrain RMSE for horizon 2 is 42.48, Test RMSE is 99.33"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Forecast Demo\n\n\n\n\n \n\nDirect forecast\n\n\n\n\n \n\nRecursive forecast\n\n\n\n\n \n\nTransformations\n\n\n\n\n \n\nTime SeriesDecomposition"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html",
    "href": "topics/intro/forecaster_demo.html",
    "title": "Forecasting demonstration",
    "section": "",
    "text": "import pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport os \n\nfrom feature_engine.timeseries.forecasting import LagFeatures\n\nfrom feature_engine.imputation import DropMissingData\n\nfrom feature_engine.selection import DropFeatures \n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import root_mean_squared_error\nIn this section, we provide a brief demonstration of a forecasting problem using a simple Linear Regression model. The dataset used for this example consists of historical airline passenger numbers. Our goal is to demonstrate the forecasting process, including key steps such as feature engineering, model fitting, and forecast evaluation."
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#data-loading",
    "href": "topics/intro/forecaster_demo.html#data-loading",
    "title": "Forecasting demonstration",
    "section": "Data loading",
    "text": "Data loading\nThe dataset contains monthly data on the number of airline passengers from January 1949 to December 1960. This time series dataset is commonly used in forecasting examples due to its clear seasonal patterns and upward trend over time. The time index of the data represents the first day of each month, and the target variable is the number of passengers. The following code loads the dataset, ensures the date column is correctly parsed as a datetime object, and sets it as the index of the DataFrame.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\n\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#performance-evaluation",
    "href": "topics/intro/forecaster_demo.html#performance-evaluation",
    "title": "Forecasting demonstration",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nTo evaluate the model’s performance, we calculate the Root Mean Squared Error (RMSE), a commonly used metric in forecasting. RMSE measures the average magnitude of the prediction errors, with lower values indicating a better fit between the predicted and actual values. By comparing the RMSE for both the training and test sets, we can gauge how well the model performs and whether it generalizes effectively to new data.\n\nprint(f\"Train set RMSE is {np.round(root_mean_squared_error(y_train, y_train_pred),2)}\")\n\nTrain set RMSE is 23.22\n\nprint(f\"Test set RMSE is {np.round(root_mean_squared_error(y_test, y_test_pred),2)}\")\n\nTest set RMSE is 49.64"
  },
  {
    "objectID": "topics/intro/forecaster_demo.html#summary",
    "href": "topics/intro/forecaster_demo.html#summary",
    "title": "Forecasting demonstration",
    "section": "Summary",
    "text": "Summary\nIn this demonstration, we explored the basic steps of a time series forecasting task using airline passenger data. We processed the data by creating lagged features, trained a simple Linear Regression model, and evaluated the model’s performance using RMSE. This step-by-step approach shows how to handle feature engineering, model training, and performance evaluation in a forecasting scenario, providing a foundation for more advanced time series techniques."
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html",
    "href": "topics/ts_decomposition/class_decomp.html",
    "title": "Classical Decomposition",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\n\nraw_df = pd.read_csv(file_path,index_col = \"date\")\nraw_df.index = pd.to_datetime(raw_df.index)"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#introduction",
    "href": "topics/ts_decomposition/class_decomp.html#introduction",
    "title": "Classical Decomposition",
    "section": "Introduction",
    "text": "Introduction\nIn time series analysis, breaking down a time series into its constituent components can provide valuable insights into the data’s underlying structure. The classical decomposition method is a popular approach that isolates three key components: Trend, Seasonality, and Remainder (also called residuals or noise). The Trend component represents the long-term progression or direction of the data, while Seasonality captures repetitive, cyclical patterns that occur at regular intervals. The Remainder component, meanwhile, includes random variations or noise not accounted for by the trend or seasonality. In the classical decomposition approach, the trend is typically extracted using moving averages, which smooth out short-term fluctuations to reveal the underlying trend. This tutorial demonstrates how to apply classical decomposition to a retail sales dataset to isolate and examine each component, using a moving average to estimate the trend and group-based means to identify seasonality."
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#trend",
    "href": "topics/ts_decomposition/class_decomp.html#trend",
    "title": "Classical Decomposition",
    "section": "Trend",
    "text": "Trend\nIn time series analysis, identifying the trend is crucial to understanding the long-term movement in data, as it shows the general direction or tendency over time. To estimate the trend effectively, we can use a moving average, which smooths out fluctuations, making the trend clearer by reducing the impact of random noise or seasonality.\nThe choice of window size for the moving average is significant: typically, it is set to match the frequency of the seasonality if it is known. For instance, with monthly data exhibiting annual seasonality, setting the window to 12 months captures the yearly cycle. By choosing a window size that encapsulates an entire seasonality cycle, we can “isolate” and average out the seasonality’s effects. If the seasonality frequency is unknown, testing different window sizes and visually evaluating the resulting trend line can help. Choosing a too-small window leads to under-smoothing, which results in a trend line with excessive fluctuations. Conversely, an overly large window causes over-smoothing, where the trend line may appear flat and miss important trends in the data.\nIn the following code, we create both under-smoothed and over-smoothed trend lines to demonstrate these effects.\n\n\ntrend_df = raw_df.copy()\n\neven_win_len = 84\n\ntrend_df[\"over_smoothing\"] = trend_df[\"sales\"].rolling(window = even_win_len).mean()\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].rolling(window = 2, center = True).mean()\ntrend_df[\"over_smoothing\"] = trend_df[\"over_smoothing\"].shift(- even_win_len // 2)\n\ntrend_df[\"under_smoothing\"] = trend_df[\"sales\"].rolling(window = 3, center = True).mean()\n\n\nplt.clf()\n\ntrend_df[\"sales\"].plot(color = \"grey\", alpha = 0.5)\ntrend_df[\"under_smoothing\"].plot(color = \"steelblue\")\ntrend_df[\"over_smoothing\"].plot(color = \"orange\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nCross-validation for Trend Estimation\nTo ensure that we select an optimal window size for the trend component, cross-validation can be used as a more rigorous method. By partitioning the data into training and test sets and evaluating the trend’s predictive accuracy on unseen data, we can refine our choice of window size based on empirical evidence rather than visual inspection alone. This approach can help balance under-smoothing and over-smoothing, leading to a trend line that effectively represents the underlying pattern in the data."
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#seasonality",
    "href": "topics/ts_decomposition/class_decomp.html#seasonality",
    "title": "Classical Decomposition",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to periodic fluctuations in data that occur at regular intervals due to factors like seasons, quarters, or months. To extract seasonality from a time series, the trend component must first be isolated and removed. This is done by detrending the data: subtracting the trend if the seasonality is additive or dividing by the trend if it is multiplicative. Once detrended, the data can reveal the repetitive seasonal pattern, allowing us to capture the recurring variations.\nIn the following code, we apply a rolling mean to determine the trend, then detrend the data by subtracting this trend from the original sales data. After detrending, we group the data by month to calculate average seasonal effects, which enables us to isolate and visualize the seasonality. The resulting seasonal component shows the monthly effect after excluding the trend.\n\n\nseason_df = raw_df.copy()\n\nseason_df[\"trend\"] = season_df[\"sales\"].rolling(window = 12).mean()\nseason_df[\"trend\"] = season_df[\"trend\"].rolling(window = 2, center = True).mean()\nseason_df[\"trend\"] = season_df[\"trend\"].shift(- 12 // 2)\n\nseason_df[\"detrended_data\"] = season_df[\"sales\"] - season_df[\"trend\"]\n\nseason_df[\"month\"] = season_df.index.month\nseasonality = season_df.groupby(\"month\").mean()[\"detrended_data\"].reset_index()\n\nseasonality.columns = [\"month\",\"seasonality\"]\nseason_df = pd.merge(season_df.copy(), seasonality, on = \"month\", how = \"left\")\n\nseason_df[\"remainder\"] = season_df[\"detrended_data\"] - season_df[\"seasonality\"]\n\n\nplt.clf()\n\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n\nseason_df[\"trend\"].plot(ax = axes[0], title = \"Trend\")\nseason_df[\"seasonality\"].plot(ax = axes[1], title = \"Seasonality\")\nseason_df[\"remainder\"].plot(ax = axes[2], title = \"Remainder\")\n\n# Adjust layout to avoid overlap\nplt.tight_layout(pad = 3.0)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/class_decomp.html#summary",
    "href": "topics/ts_decomposition/class_decomp.html#summary",
    "title": "Classical Decomposition",
    "section": "Summary",
    "text": "Summary\nThrough the classical decomposition method, we successfully isolated the trend, seasonality, and remainder components of a time series, offering a clearer view of each component’s influence on the data. By using moving averages, we smoothed out short-term noise to identify the trend and then detrended the data to reveal seasonality. This process allowed us to break down the time series into interpretable parts, providing insights into underlying patterns and variability. Such decomposition is essential in time series analysis, as it enhances the forecasting accuracy and interpretability of models by focusing on distinct data patterns. This approach is valuable in various domains, from retail sales to finance, where understanding the interplay between trend and seasonal effects is critical for decision-making."
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html",
    "href": "topics/ts_decomposition/transformations.html",
    "title": "Transformations",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os \n\nfrom scipy.stats import boxcox"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#introduction",
    "href": "topics/ts_decomposition/transformations.html#introduction",
    "title": "Transformations",
    "section": "Introduction",
    "text": "Introduction\nData transformation is an essential preprocessing step in data analysis and machine learning, particularly when dealing with time series or data that exhibits non-linear patterns. Transformations, such as log, Box-Cox, and moving averages, are used to adjust data to make it more amenable to analysis by stabilizing variance, enhancing linearity, or smoothing out noise. These adjustments make it easier to uncover trends, detect seasonality, and improve model performance. This tutorial will explore three key transformations—log transformation, Box-Cox transformation (including the Guerrero method), and moving averages—and demonstrate how each can be applied to real-world time series data to improve its suitability for analysis and forecasting.\n\n\nfile_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_air_passengers.csv\"\nraw_df = pd.read_csv(file_path,index_col = \"date\")\nraw_df.index = pd.to_datetime(raw_df.index)\n\n\nraw_df[\"passengers\"].plot()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#log-transformation",
    "href": "topics/ts_decomposition/transformations.html#log-transformation",
    "title": "Transformations",
    "section": "Log Transformation",
    "text": "Log Transformation\nTo make data more suitable for certain analyses, we may need to transform it. One common reason is to stabilize the variance, especially when dealing with data that shows exponential growth or high variability. Log transformation is an effective method for this. By taking the logarithm of each value in the data, we can compress large values and expand smaller ones, resulting in a dataset where variability across different ranges is reduced. This transformation can make trends more visible and prepare the data for further processing or modeling steps.\n\nraw_df[\"passengers_log\"] = np.log(raw_df[\"passengers\"])\nplt.clf()\nraw_df[\"passengers_log\"].plot()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "href": "topics/ts_decomposition/transformations.html#box-cox-transformation",
    "title": "Transformations",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nThe Box-Cox transformation is a versatile method used to stabilize variance across a dataset. Unlike the log transformation, Box-Cox can adapt to various data distributions by tuning the transformation parameter, \\(\\lambda\\). Different values of \\(\\lambda\\) change the shape of the transformation, allowing for either compression or expansion of data values in a customized way. By testing various \\(\\lambda\\) values, we can achieve the best variance stabilization and approximate a normal distribution, which is helpful for many statistical analyses.\nIn this example, we test a series of \\(\\lambda\\) values to demonstrate how the shape of the transformed data changes, which can help identify an optimal transformation for stabilizing variance in our dataset.\n\nlambdas_vec = [-1,-0.5,0,0.5,1,2]\nplt.clf()\nfig, ax = plt.subplots(ncols=2, nrows=3, figsize=[15, 15], sharex=True)\nax = ax.flatten()\n\nfor ix, temp_lambda in enumerate(lambdas_vec):\n    raw_df[\"temp_box_cox\"] = boxcox(raw_df[\"passengers\"], temp_lambda)\n    raw_df.plot(y=\"temp_box_cox\", ax=ax[ix], label=f\"lambda = {temp_lambda}\")\n    ax[ix].legend(fontsize=18)\n    ax[ix].set_xlabel(\"\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Guerrero Method\nThe Guerrero method for the Box-Cox transformation offers a targeted approach to variance stabilization by automatically selecting an optimal \\(\\lambda\\) based on the data’s seasonal patterns. This method evaluates the seasonal periodicity and adjusts \\(\\lambda\\) to achieve minimal variance over time. The advantage of using Guerrero’s method is that it provides a systematic way to stabilize variance in seasonal data without manually testing multiple \\(\\lambda\\) values. This automatic tuning is particularly useful when working with time series data with complex seasonal patterns.\n\nfrom sktime.transformations.series.boxcox import BoxCoxTransformer\n\nbc_guerrero = BoxCoxTransformer(method=\"guerrero\", sp=12)\nraw_df[\"passengers_bc_guerrero\"] = bc_guerrero.fit_transform(raw_df[\"passengers\"])\n\nplt.clf()\nraw_df[\"passengers_bc_guerrero\"].plot()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#moving-averages",
    "href": "topics/ts_decomposition/transformations.html#moving-averages",
    "title": "Transformations",
    "section": "Moving Averages",
    "text": "Moving Averages\nMoving averages are a common technique for smoothing time series data, allowing for a clearer view of the underlying trend by reducing noise. With an odd-sized window, the moving average is symmetric, meaning it has an equal number of data points on either side of the center, which simplifies calculation and interpretation.\nWhen using an even-sized window, achieving symmetry is more challenging because there isn’t a natural center point. To create a balanced, symmetric effect, we can adjust the weights, assigning smaller weights to the edges while retaining higher weights for the central values. Another approach to achieve symmetry with an even window is to apply a secondary moving average of window size 2. This additional step smooths the edge values, effectively balancing the window weights.\n\n\nma_file_path = os.path.expanduser(\"~/Documents\") + \"\\\\DS_advanced_website\\\\data\\\\example_retail_sales.csv\"\nma_raw_df = pd.read_csv(ma_file_path, index_col=\"date\")\nma_raw_df.index = pd.to_datetime(ma_raw_df.index)\ndel ma_file_path\n\n\n3-Point Moving Average\nHere, we apply a 3-point moving average, which is symmetric by nature. This approach smooths out the data while retaining the central tendency, making it easier to observe general trends without the distraction of short-term fluctuations.\n\n\nma_df = ma_raw_df.copy()\nma_df[\"ma_3\"] = ma_df[\"sales\"].rolling(window=3, center=True).mean()\n\n\nplt.clf()\nma_df[\"ma_3\"].plot(color=\"steelblue\")\nma_df[\"sales\"].plot(color=\"grey\", alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n12-Point Moving Average with Centering\nTo create a symmetric 12-point moving average, we apply an additional 2-point moving average to our initial 12-point average. This second step reduces the impact of edge weights, creating a balanced smoothing effect. After this adjustment, we shift the result by half the window size to ensure alignment with the original data, resulting in a centered moving average.\n\n\neven_window_size = 12\nma_df[\"ma_2_12\"] = ma_df[\"sales\"].rolling(window=even_window_size).mean()\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].rolling(window=2, center=True).mean()\nma_df[\"ma_2_12\"] = ma_df[\"ma_2_12\"].shift(-even_window_size // 2)\n\n\nplt.clf()\nma_df[\"ma_2_12\"].plot(color=\"steelblue\")\nma_df[\"sales\"].plot(color=\"grey\", alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "topics/ts_decomposition/transformations.html#summary",
    "href": "topics/ts_decomposition/transformations.html#summary",
    "title": "Transformations",
    "section": "Summary",
    "text": "Summary\nThis tutorial has demonstrated the use of log transformations, Box-Cox transformations, and moving averages to prepare time series data for analysis. Each transformation serves a specific purpose: log transformations reduce skewness and stabilize variance, Box-Cox transformations provide flexibility through parameter tuning, and moving averages smooth data for trend analysis. By understanding and applying these techniques, we can better handle data variability, enhance interpretability, and ultimately improve forecasting accuracy. The choice of transformation should align with the data characteristics and analytical goals, making transformations a valuable tool in the data scientist’s arsenal for managing diverse time series data."
  }
]